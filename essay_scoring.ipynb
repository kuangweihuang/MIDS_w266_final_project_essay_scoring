{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W266 Project - Essay Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "\n",
    "The data is obtained from the Automated Student Assessment Prize (ASAP) AES dataset (https://www.kaggle.com/c/asap-aes/data), which contains essays written by students ranging from Grade 7 to Grade 10. The dataset consists of 8 essay sets, each with a different topic or prompt, with a total of 12,978 essays with scores.\n",
    "\n",
    "Each of the sets of essays was generated from a single prompt. Selected essays range from an average length of 150 to 550 words per response. Some of the essays are dependent upon source information and others are not. All responses were written by students ranging in grade levels from Grade 7 to Grade 10. All essays were hand graded and were double-scored. Each of the eight data sets has its own unique characteristics. The variability is intended to test the limits of your scoring engine's capabilities.\n",
    "\n",
    "The training data is provided in three formats: a tab-separated value (TSV) file, a Microsoft Excel 2010 spreadsheet, and a Microsoft Excel 2003 spreadsheet.  The current release of the training data contains essay sets 1-6.  Sets 7-8 will be released on February 10, 2012.  Each of these files contains 28 columns:\n",
    "\n",
    "    essay_id: A unique identifier for each individual student essay\n",
    "    essay_set: 1-8, an id for each set of essays\n",
    "    essay: The ascii text of a student's response\n",
    "    rater1_domain1: Rater 1's domain 1 score; all essays have this\n",
    "    rater2_domain1: Rater 2's domain 1 score; all essays have this\n",
    "    rater3_domain1: Rater 3's domain 1 score; only some essays in set 8 have this.\n",
    "    domain1_score: Resolved score between the raters; all essays have this\n",
    "    rater1_domain2: Rater 1's domain 2 score; only essays in set 2 have this\n",
    "    rater2_domain2: Rater 2's domain 2 score; only essays in set 2 have this\n",
    "    domain2_score: Resolved score between the raters; only essays in set 2 have this\n",
    "    rater1_trait1 score - rater3_trait6 score: trait scores for sets 7-8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up ML libraries\n",
    "\n",
    "Importing the relevant NLP and tensorflow libraries for our use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/smus78/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smus78/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import datetime\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Pandas and SKLearn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, patched_numpy_io\n",
    "#from w266_common import utils, vocabulary, tf_embed_viz\n",
    "\n",
    "# Your code\n",
    "#import rnnlm; reload(rnnlm)\n",
    "#import rnnlm_test; reload(rnnlm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the data\n",
    "\n",
    "Data from AES dataset is stored in the `/data/` folder.  We will begin by loading the training dataset `training_set_rel3.tsv` and partitioning it into train, test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of rows in full data set: 12978\n"
     ]
    }
   ],
   "source": [
    "training_set_rel3_df = pd.read_csv(\"data/training_set_rel3.csv\")\n",
    "#training_set_rel3_df.head()\n",
    "print(\"No. of rows in full data set:\", len(training_set_rel3_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0             4.0             4.0             NaN            8.0   \n",
       "1             5.0             4.0             NaN            9.0   \n",
       "2             4.0             3.0             NaN            7.0   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score      ...        \\\n",
       "0             NaN             NaN            NaN      ...         \n",
       "1             NaN             NaN            NaN      ...         \n",
       "2             NaN             NaN            NaN      ...         \n",
       "\n",
       "   rater2_trait3  rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  \\\n",
       "0            NaN            NaN            NaN            NaN            NaN   \n",
       "1            NaN            NaN            NaN            NaN            NaN   \n",
       "2            NaN            NaN            NaN            NaN            NaN   \n",
       "\n",
       "   rater3_trait2  rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n",
       "0            NaN            NaN            NaN            NaN            NaN  \n",
       "1            NaN            NaN            NaN            NaN            NaN  \n",
       "2            NaN            NaN            NaN            NaN            NaN  \n",
       "\n",
       "[3 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_rel3_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of rows in full data set: 12977\n"
     ]
    }
   ],
   "source": [
    "# Dropping data with no scores in \"domain1_score\"\n",
    "training_set_rel3_df.dropna(axis=0, subset=['domain1_score'], inplace=True)\n",
    "print(\"No. of rows in full data set:\", len(training_set_rel3_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the prompts\n",
    "prompt_dict = {1 : 'More and more people use computers, but not everyone agrees that this benefits society. Those who support advances in technology believe that computers have a positive effect on people. They teach hand-eye coordination, give people the ability to learn about faraway places and people, and even allow people to talk online with other people. Others have different ideas. Some experts are concerned that people are spending too much time on their computers and less time exercising, enjoying nature, and interacting with family and friends. Write a letter to your local newspaper in which you state your opinion on the effects computers have on people. Persuade the readers to agree with you.',\n",
    "               2 : 'Censorship in the Libraries. \"All of us can think of a book that we hope none of our children or any other children have taken off the shelf. But if I have the right to remove that book from the shelf -- that work I abhor -- then you also have exactly the same right and so does everyone else. And then we have no books left on the shelf for any of us.\" --Katherine Paterson, Author. Write a persuasive essay to a newspaper reflecting your views on censorship in libraries. Do you believe that certain materials, such as books, music, movies, magazines, etc., should be removed from the shelves if they are found offensive? Support your position with convincing arguments from your own experience, observations, and/or reading.',\n",
    "               3 : 'ROUGH ROAD AHEAD: Do Not Exceed Posted Speed Limit -- by Joe Kurmaskie. FORGET THAT OLD SAYING ABOUT NEVER taking candy from strangers. No, a better piece of advice for the solo cyclist would be, “Never accept travel advice from a collection of old-timers who haven’t left the confines of their porches since Carter was in office.” It’s not that a group of old guys doesn’t know the terrain. With age comes wisdom and all that, but the world is a fluid place. Things change. At a reservoir campground outside of Lodi, California, I enjoyed the serenity of an early-summer evening and some lively conversation with these old codgers. What I shouldn’t have done was let them have a peek at my map. Like a foolish youth, the next morning I followed their advice and launched out at first light along a “shortcut” that was to slice away hours from my ride to Yosemite National Park. They’d sounded so sure of themselves when pointing out landmarks and spouting off towns I would come to along this breezy jaunt. Things began well enough. I rode into the morning with strong legs and a smile on my face. About forty miles into the pedal, I arrived at the first “town.” This place might have been a thriving little spot at one time—say, before the last world war—but on that morning it fit the traditional definition of a ghost town. I chuckled, checked my water supply, and moved on. The sun was beginning to beat down, but I barely noticed it. The cool pines and rushing rivers of Yosemite had my name written all over them. Twenty miles up the road, I came to a fork of sorts. One ramshackle shed, several rusty pumps, and a corral that couldn’t hold in the lamest mule greeted me. This sight was troubling. I had been hitting my water bottles pretty regularly, and I was traveling through the high deserts of California in June. I got down on my hands and knees, working the handle of the rusted water pump with all my strength. A tarlike substance oozed out, followed by brackish water feeling somewhere in the neighborhood of two hundred degrees. I pumped that handle for several minutes, but the water wouldn’t cool down. It didn’t matter. When I tried a drop or two, it had the flavor of battery acid. The old guys had sworn the next town was only eighteen miles down the road. I could make that! I would conserve my water and go inward for an hour or so—a test of my inner spirit. Not two miles into this next section of the ride, I noticed the terrain changing. Flat road was replaced by short, rolling hills. After I had crested the first few of these, a large highway sign jumped out at me. It read: ROUGH ROAD AHEAD: DO NOT EXCEED POSTED SPEED LIMIT. The speed limit was 55 mph. I was doing a water-depleting 12 mph. Sometimes life can feel so cruel. I toiled on. At some point, tumbleweeds crossed my path and a ridiculously large snake—it really did look like a diamondback—blocked the majority of the pavement in front of me. I eased past, trying to keep my balance in my dehydrated state. The water bottles contained only a few tantalizing sips. Wide rings of dried sweat circled my shirt, and the growing realization that I could drop from heatstroke on a gorgeous day in June simply because I listened to some gentlemen who hadn’t been off their porch in decades, caused me to laugh. It was a sad, hopeless laugh, mind you, but at least I still had the energy to feel sorry for myself. There was no one in sight, not a building, car, or structure of any kind. I began breaking the ride down into distances I could see on the horizon, telling myself that if I could make it that far, I’d be fine. Over one long, crippling hill, a building came into view. I wiped the sweat from my eyes to make sure it wasn’t a mirage, and tried not to get too excited. With what I believed was my last burst of energy, I maneuvered down the hill. In an ironic twist that should please all sadists reading this, the building—abandoned years earlier, by the looks of it—had been a Welch’s Grape Juice factory and bottling plant. A sandblasted picture of a young boy pouring a refreshing glass of juice into his mouth could still be seen. I hung my head. That smoky blues tune “Summertime” rattled around in the dry honeycombs of my deteriorating brain. I got back on the bike, but not before I gathered up a few pebbles and stuck them in my mouth. I’d read once that sucking on stones helps take your mind off thirst by allowing what spit you have left to circulate. With any luck I’d hit a bump and lodge one in my throat. It didn’t really matter. I was going to die and the birds would pick me clean, leaving only some expensive outdoor gear and a diary with the last entry in praise of old men, their wisdom, and their keen sense of direction. I made a mental note to change that paragraph if it looked like I was going to lose consciousness for the last time. Somehow, I climbed away from the abandoned factory of juices and dreams, slowly gaining elevation while losing hope. Then, as easily as rounding a bend, my troubles, thirst, and fear were all behind me. GARY AND WILBER’S FISH CAMP—IF YOU WANT BAIT FOR THE BIG ONES, WE’RE YOUR BEST BET! “And the only bet,” I remember thinking. As I stumbled into a rather modern bathroom and drank deeply from the sink, I had an overwhelming urge to seek out Gary and Wilber, kiss them, and buy some bait—any bait, even though I didn’t own a rod or reel. An old guy sitting in a chair under some shade nodded in my direction. Cool water dripped from my head as I slumped against the wall beside him. “Where you headed in such a hurry?” “Yosemite,” I whispered. “Know the best way to get there?” I watched him from the corner of my eye for a long moment. He was even older than the group I’d listened to in Lodi. “Yes, sir! I own a very good map.” And I promised myself right then that I’d always stick to it in the future. “Rough Road Ahead” by Joe Kurmaskie, from Metal Cowboy, copyright © 1999 Joe Kurmaskie. Write a response that explains how the features of the setting affect the cyclist. In your response, include examples from the essay that support your conclusion.',\n",
    "               4 : 'Winter Hibiscus by Minfong Ho Saeng, a teenage girl, and her family have moved to the United States from Vietnam. As Saeng walks home after failing her driver’s test, she sees a familiar plant. Later, she goes to a florist shop to see if the plant can be purchased. It was like walking into another world. A hot, moist world exploding with greenery. Huge flat leaves, delicate wisps of tendrils, ferns and fronds and vines of all shades and shapes grew in seemingly random profusion. “Over there, in the corner, the hibiscus. Is that what you mean?” The florist pointed at a leafy potted plant by the corner. There, in a shaft of the wan afternoon sunlight, was a single blood-red blossom, its five petals splayed back to reveal a long stamen tipped with yellow pollen. Saeng felt a shock of recognition so intense, it was almost visceral. “Saebba,” Saeng whispered. A saebba hedge, tall and lush, had surrounded their garden, its lush green leaves dotted with vermilion flowers. And sometimes after a monsoon rain, a blossom or two would have blown into the well, so that when she drew the well water, she would find a red blossom floating in the bucket. Slowly, Saeng walked down the narrow aisle toward the hibiscus. Orchids, lanna bushes, oleanders, elephant ear begonias, and bougainvillea vines surrounded her. Plants that she had not even realized she had known but had forgotten drew her back into her childhood world. When she got to the hibiscus, she reached out and touched a petal gently. It felt smooth and cool, with a hint of velvet toward the center—just as she had known it would feel. And beside it was yet another old friend, a small shrub with waxy leaves and dainty flowers with purplish petals and white centers. “Madagascar periwinkle,” its tag announced. How strange to see it in a pot, Saeng thought. Back home it just grew wild, jutting out from the cracks in brick walls or between tiled roofs. And that rich, sweet scent—that was familiar, too. Saeng scanned the greenery around her and found a tall, gangly plant with exquisite little white blossoms on it.  “Dok Malik,” she said, savoring the feel of the word on her tongue, even as she silently noted the English name on its tag, “jasmine.” One of the blossoms had fallen off, and carefully Saeng picked it up and smelled it. She closed her eyes and breathed in, deeply. The familiar fragrance filled her lungs, and Saeng could almost feel the light strands of her grandmother’s long gray hair, freshly washed, as she combed it out with the fine-toothed buffalo-horn comb. And when the sun had dried it, Saeng would help the gnarled old fingers knot the hair into a bun, then slip a dok Malik bud into it. Saeng looked at the white bud in her hand now, small and fragile. Gently, she closed her palm around it and held it tight. That, at least, she could hold on to. But where was the fine-toothed comb? The hibiscus hedge? The well? Her gentle grandmother? A wave of loss so deep and strong that it stung Saeng’s eyes now swept over her. A blink, a channel switch, a boat ride into the night, and it was all gone. Irretrievably, irrevocably gone. And in the warm moist shelter of the greenhouse, Saeng broke down and wept. It was already dusk when Saeng reached home. The wind was blowing harder, tearing off the last remnants of green in the chicory weeds that were growing out of the cracks in the sidewalk. As if oblivious to the cold, her mother was still out in the vegetable garden, digging up the last of the onions with a rusty trowel. She did not see Saeng until the girl had quietly knelt down next to her. Her smile of welcome warmed Saeng. “Ghup ma laio le? You’re back?” she said cheerfully. “Goodness, it’s past five. What took you so long? How did it go? Did you—?” Then she noticed the potted plant that Saeng was holding, its leaves quivering in the wind. Mrs. Panouvong uttered a small cry of surprise and delight. “Dok faeng-noi!” she said. “Where did you get it?” “I bought it,” Saeng answered, dreading her mother’s next question. “How much?” For answer Saeng handed her mother some coins. “That’s all?” Mrs. Panouvong said, appalled, “Oh, but I forgot! You and the Lambert boy ate Bee-Maags . . . .” “No, we didn’t, Mother,” Saeng said. “Then what else—?” “Nothing else. I paid over nineteen dollars for it.” “You what?” Her mother stared at her incredulously. “But how could you? All the seeds for this vegetable garden didn’t cost that much! You know how much we—” She paused, as she noticed the tearstains on her daughter’s cheeks and her puffy eyes. “What happened?” she asked, more gently. “I—I failed the test,” Saeng said. For a long moment Mrs. Panouvong said nothing. Saeng did not dare look her mother in the eye. Instead, she stared at the hibiscus plant and nervously tore off a leaf, shredding it to bits. Her mother reached out and brushed the fragments of green off Saeng’s hands. “It’s a beautiful plant, this dok faeng-noi,” she finally said. “I’m glad you got it.” “It’s—it’s not a real one,” Saeng mumbled. “I mean, not like the kind we had at—at—” She found that she was still too shaky to say the words at home, lest she burst into tears again. “Not like the kind we had before,” she said. “I know,” her mother said quietly. “I’ve seen this kind blooming along the lake. Its flowers aren’t as pretty, but it’s strong enough to make it through the cold months here, this winter hibiscus. That’s what matters.” She tipped the pot and deftly eased the ball of soil out, balancing the rest of the plant in her other hand. “Look how root-bound it is, poor thing,” she said. “Let’s plant it, right now.” She went over to the corner of the vegetable patch and started to dig a hole in the ground. The soil was cold and hard, and she had trouble thrusting the shovel into it. Wisps of her gray hair trailed out in the breeze, and her slight frown deepened the wrinkles around her eyes. There was a frail, wiry beauty to her that touched Saeng deeply. “Here, let me help, Mother,” she offered, getting up and taking the shovel away from her. Mrs. Panouvong made no resistance. “I’ll bring in the hot peppers and bitter melons, then, and start dinner. How would you like an omelet with slices of the bitter melon?” “I’d love it,” Saeng said. Left alone in the garden, Saeng dug out a hole and carefully lowered the “winter hibiscus” into it. She could hear the sounds of cooking from the kitchen now, the beating of eggs against a bowl, the sizzle of hot oil in the pan. The pungent smell of bitter melon wafted out, and Saeng’s mouth watered. It was a cultivated taste, she had discovered—none of her classmates or friends, not even Mrs. Lambert, liked it—this sharp, bitter melon that left a golden aftertaste on the tongue. But she had grown up eating it and, she admitted to herself, much preferred it to a Big Mac. The “winter hibiscus” was in the ground now, and Saeng tamped down the soil around it. Overhead, a flock of Canada geese flew by, their faint honks clear and—yes—familiar to Saeng now. Almost reluctantly, she realized that many of the things that she had thought of as strange before had become, through the quiet repetition of season upon season, almost familiar to her now. Like the geese. She lifted her head and watched as their distinctive V was etched against the evening sky, slowly fading into the distance. When they come back, Saeng vowed silently to herself, in the spring, when the snows melt and the geese return and this hibiscus is budding, then I will take that test again. “Winter Hibiscus” by Minfong Ho, copyright © 1993 by Minfong Ho, from Join In, Multiethnic Short Stories, by Donald R. Gallo, ed. Read the last paragraph of the story. \"When they come back, Saeng vowed silently to herself, in the spring, when the snows melt and the geese return and this hibiscus is budding, then I will take that test again.\" Write a response that explains why the author concludes the story with this paragraph. In your response, include details and examples from the story that support your ideas.',\n",
    "               5 : 'Narciso Rodriguez from Home: The Blueprints of Our Lives My parents, originally from Cuba, arrived in the United States in 1956. After living for a year in a furnished one-room apartment, twenty-one-year-old Rawedia Maria and twenty-seven-year-old Narciso Rodriguez, Sr., could afford to move into a modest, three-room apartment I would soon call home. In 1961, I was born into this simple house, situated in a two-family, blond-brick building in the Ironbound section of Newark, New Jersey. Within its walls, my young parents created our traditional Cuban home, the very heart of which was the kitchen. My parents both shared cooking duties and unwittingly passed on to me their rich culinary skills and a love of cooking that is still with me today (and for which I am eternally grateful). Passionate Cuban music (which I adore to this day) filled the air, mixing with the aromas of the kitchen. Here, the innocence of childhood, the congregation of family and friends, and endless celebrations that encompassed both, formed the backdrop to life in our warm home. Growing up in this environment instilled in me a great sense that “family” had nothing to do with being a blood relative. Quite the contrary, our neighborhood was made up of mostly Spanish, Cuban, and Italian immigrants at a time when overt racism was the norm and segregation prevailed in the United States. In our neighborhood, despite customs elsewhere, all of these cultures came together in great solidarity and friendship. It was a close-knit community of honest, hardworking immigrants who extended a hand to people who, while not necessarily their own kind, were clearly in need. Our landlord and his daughter, Alegria (my babysitter and first friend), lived above us, and Alegria graced our kitchen table for meals more often than not. Also at the table were Sergio and Edelmira, my surrogate grandparents who lived in the basement apartment. (I would not know my “real” grandparents, Narciso the Elder and Consuelo, until 1970 when they were allowed to leave Cuba.) My aunts Bertha and Juanita and my cousins Arnold, Maria, and Rosemary also all lived nearby and regularly joined us at our table. Countless extended family members came and went — and there was often someone staying with us temporarily until they were able to get back on their feet. My parents always kept their arms and their door open to the many people we considered family, knowing that they would do the same for us. My mother and father had come to this country with such courage, without any knowledge of the language or the culture. They came selflessly, as many immigrants do, to give their children a better life, even though it meant leaving behind their families, friends, and careers in the country they loved. They struggled both personally and financially, braving the harsh northern winters while yearning for their native tropics and facing cultural hardships. The barriers to work were strong and high, and my parents both had to accept that they might not be able to find the kind of jobs they deserved. In Cuba, Narciso, Sr., had worked in a laboratory and Rawedia Maria had studied chemical engineering. In the United States, they had to start their lives over entirely, taking whatever work they could find. The faith that this struggle would lead them and their children to better times drove them to endure these hard times. I will always be grateful to my parents for their love and sacrifice. I’ve often told them that what they did was a much more courageous thing than I could have ever done. I’ve often told them of my admiration for their strength and perseverance, and I’ve thanked them repeatedly. But, in reality, there is no way to express my gratitude for the spirit of generosity impressed upon me at such an early age and the demonstration of how important family and friends are. These are two lessons that my parents did not just tell me. They showed me with their lives, and these teachings have been the basis of my life. It was in this simple house that my parents welcomed other refugees to celebrate their arrival to this country and where I celebrated my first birthdays. It was in the warmth of the kitchen in this humble house where a Cuban feast (albeit a frugal Cuban feast) always filled the air with not just scent and music but life and love. It was here where I learned the real definition of “family.” And for this, I will never forget that house or its gracious neighborhood or the many things I learned there about how to love. I will never forget how my parents turned this simple house into a home. — Narciso Rodriguez, Fashion designer. Hometown: Newark, New Jersey. “Narciso Rodriguez” by Narciso Rodriguez, from Home: The Blueprints of Our Lives. Copyright © 2006 by John Edwards. Describe the mood created by the author in the memoir. Support your answer with relevant and specific information from the memoir.',\n",
    "               6 : 'The Mooring Mast by Marcia Amidon Lüsted. When the Empire State Building was conceived, it was planned as the world’s tallest building, taller even than the new Chrysler Building that was being constructed at Forty-second Street and Lexington Avenue in New York. At seventy-seven stories, it was the tallest building before the Empire State began construction, and Al Smith was determined to outstrip it in height. The architect building the Chrysler Building, however, had a trick up his sleeve. He secretly constructed a 185-foot spire inside the building, and then shocked the public and the media by hoisting it up to the top of the Chrysler Building, bringing it to a height of 1,046 feet, 46 feet taller than the originally announced height of the Empire State Building. Al Smith realized that he was close to losing the title of world’s tallest building, and on December 11, 1929, he announced that the Empire State would now reach the height of 1,250 feet. He would add a top or a hat to the building that would be even more distinctive than any other building in the city. John Tauranac describes the plan: [The top of the Empire State Building] would be more than ornamental, more than a spire or dome or a pyramid put there to add a desired few feet to the height of the building or to mask something as mundane as a water tank. Their top, they said, would serve a higher calling. The Empire State Building would be equipped for an age of transportation that was then only the dream of aviation pioneers. This dream of the aviation pioneers was travel by dirigible, or zeppelin, and the Empire State Building was going to have a mooring mast at its top for docking these new airships, which would accommodate passengers on already existing transatlantic routes and new routes that were yet to come. The Age of Dirigibles. By the 1920s, dirigibles were being hailed as the transportation of the future. Also known today as blimps, dirigibles were actually enormous steel-framed balloons, with envelopes of cotton fabric filled with hydrogen and helium to make them lighter than air. Unlike a balloon, a dirigible could be maneuvered by the use of propellers and rudders, and passengers could ride in the gondola, or enclosed compartment, under the balloon.Dirigibles had a top speed of eighty miles per hour, and they could cruise at seventy miles per hour for thousands of miles without needing refueling. Some were as long as one thousand feet, the same length as four blocks in New York City. The one obstacle to their expanded use in New York City was the lack of a suitable landing area. Al Smith saw an opportunity for his Empire State Building: A mooring mast added to the top of the building would allow dirigibles to anchor there for several hours for refueling or service, and to let passengers off and on. Dirigibles were docked by means of an electric winch, which hauled in a line from the front of the ship and then tied it to a mast. The body of the dirigible could swing in the breeze, and yet passengers could safely get on and off the dirigible by walking down a gangplank to an open observation platform. The architects and engineers of the Empire State Building consulted with experts, taking tours of the equipment and mooring operations at the U.S. Naval Air Station in Lakehurst, New Jersey. The navy was the leader in the research and development of dirigibles in the United States. The navy even offered its dirigible, the Los Angeles, to be used in testing the mast. The architects also met with the president of a recently formed airship transport company that planned to offer dirigible service across the Pacific Ocean. When asked about the mooring mast, Al Smith commented: [It’s] on the level, all right. No kidding. We’re working on the thing now. One set of engineers here in New York is trying to dope out a practical, workable arrangement and the Government people in Washington are figuring on some safe way of mooring airships to this mast. Designing the Mast. The architects could not simply drop a mooring mast on top of the Empire State Building’s flat roof. A thousand-foot dirigible moored at the top of the building, held by a single cable tether, would add stress to the building’s frame. The stress of the dirigible’s load and the wind pressure would have to be transmitted all the way to the building’s foundation, which was nearly eleven hundred feet below. The steel frame of the Empire State Building would have to be modified and strengthened to accommodate this new situation. Over sixty thousand dollars’ worth of modifications had to be made to the building’s framework. Rather than building a utilitarian mast without any ornamentation, the architects designed a shiny glass and chrome-nickel stainless steel tower that would be illuminated from inside, with a stepped-back design that imitated the overall shape of the building itself. The rocket-shaped mast would have four wings at its corners, of shiny aluminum, and would rise to a conical roof that would house the mooring arm. The winches and control machinery for the dirigible mooring would be housed in the base of the shaft itself, which also housed elevators and stairs to bring passengers down to the eighty-sixth floor, where baggage and ticket areas would be located. The building would now be 102 floors, with a glassed-in observation area on the 101st floor and an open observation platform on the 102nd floor. This observation area was to double as the boarding area for dirigible passengers. Once the architects had designed the mooring mast and made changes to the existing plans for the building’s skeleton, construction proceeded as planned. When the building had been framed to the 85th floor, the roof had to be completed before the framing for the mooring mast could take place. The mast also had a skeleton of steel and was clad in stainless steel with glass windows. Two months after the workers celebrated framing the entire building, they were back to raise an American flag again—this time at the top of the frame for the mooring mast. The Fate of the Mast. The mooring mast of the Empire State Building was destined to never fulfill its purpose, for reasons that should have been apparent before it was ever constructed. The greatest reason was one of safety: Most dirigibles from outside of the United States used hydrogen rather than helium, and hydrogen is highly flammable. When the German dirigible Hindenburg was destroyed by fire in Lakehurst, New Jersey, on May 6, 1937, the owners of the Empire State Building realized how much worse that accident could have been if it had taken place above a densely populated area such as downtown New York. The greatest obstacle to the successful use of the mooring mast was nature itself. The winds on top of the building were constantly shifting due to violent air currents. Even if the dirigible were tethered to the mooring mast, the back of the ship would swivel around and around the mooring mast. Dirigibles moored in open landing fields could be weighted down in the back with lead weights, but using these at the Empire State Building, where they would be dangling high above pedestrians on the street, was neither practical nor safe. The other practical reason why dirigibles could not moor at the Empire State Building was an existing law against airships flying too low over urban areas. This law would make it illegal for a ship to ever tie up to the building or even approach the area, although two dirigibles did attempt to reach the building before the entire idea was dropped. In December 1930, the U.S. Navy dirigible Los Angeles approached the mooring mast but could not get close enough to tie up because of forceful winds. Fearing that the wind would blow the dirigible onto the sharp spires of other buildings in the area, which would puncture the dirigible’s shell, the captain could not even take his hands off the control levers. Two weeks later, another dirigible, the Goodyear blimp Columbia, attempted a publicity stunt where it would tie up and deliver a bundle of newspapers to the Empire State Building. Because the complete dirigible mooring equipment had never been installed, a worker atop the mooring mast would have to catch the bundle of papers on a rope dangling from the blimp. The papers were delivered in this fashion, but after this stunt the idea of using the mooring mast was shelved. In February 1931, Irving Clavan of the building’s architectural office said, “The as yet unsolved problems of mooring air ships to a fixed mast at such a height made it desirable to postpone to a later date the final installation of the landing gear.” By the late 1930s, the idea of using the mooring mast for dirigibles and their passengers had quietly disappeared. Dirigibles, instead of becoming the transportation of the future, had given way to airplanes. The rooms in the Empire State Building that had been set aside for the ticketing and baggage of dirigible passengers were made over into the world’s highest soda fountain and tea garden for use by the sightseers who flocked to the observation decks. The highest open observation deck, intended for disembarking passengers, has never been open to the public. “The Mooring Mast” by Marcia Amidon Lüsted, from The Empire State Building. Copyright © 2004 by Gale, a part of Cengage Learning, Inc. Based on the excerpt, describe the obstacles the builders of the Empire State Building faced in attempting to allow dirigibles to dock there. Support your answer with relevant and specific information from the excerpt.',\n",
    "               7 : 'Write about patience. Being patient means that you are understanding and tolerant. A patient person experience difficulties without complaining. Do only one of the following: write a story about a time when you were patient OR write a story about a time when someone you know was patient OR write a story in your own way about patience.',\n",
    "               8 : 'We all understand the benefits of laughter. For example, someone once said, “Laughter is the shortest distance between two people.” Many other people believe that laughter is an important part of any relationship. Tell a true story in which laughter was one element or part.'\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding prompts column into the dataframe\n",
    "training_set_rel3_df.insert(loc=2, column=\"prompt\", \n",
    "                            value=[prompt_dict[i] for i in training_set_rel3_df[\"essay_set\"]])\n",
    "\n",
    "# Adding the normalized scores column into the dataframe\n",
    "# training_set_rel3_df.insert(loc=4, column=\"scores\", \n",
    "#                             value=[(row[\"domain1_score\"]-2)/10 if row[\"essay_set\"]==1 else \n",
    "#                                    (row[\"domain1_score\"]-1 + row[\"domain2_score\"]-1)/(5+3) if row[\"essay_set\"]==2 else\n",
    "#                                    (row[\"domain1_score\"])/3 if row[\"essay_set\"]==3 else\n",
    "#                                    (row[\"domain1_score\"])/3 if row[\"essay_set\"]==4 else\n",
    "#                                    (row[\"domain1_score\"])/4 if row[\"essay_set\"]==5 else\n",
    "#                                    (row[\"domain1_score\"])/4 if row[\"essay_set\"]==6 else\n",
    "#                                    (row[\"domain1_score\"])/30 if row[\"essay_set\"]==7 else\n",
    "#                                    (row[\"domain1_score\"])/60\n",
    "#                                    for index, row in training_set_rel3_df.iterrows() \n",
    "#                                   ]\n",
    "#                            )\n",
    "\n",
    "# Adding the scores column into the dataframe, which are [0, 10]\n",
    "training_set_rel3_df.insert(loc=4, column=\"scores\", \n",
    "                            value=[np.int(round((row[\"domain1_score\"]-2)/10 * 10)) if row[\"essay_set\"]==1 else \n",
    "                                   np.int(round((row[\"domain1_score\"]-1 + row[\"domain2_score\"]-1)/(5+3) * 10)) if row[\"essay_set\"]==2 else\n",
    "                                   np.int(round((row[\"domain1_score\"])/3 * 10)) if row[\"essay_set\"]==3 else\n",
    "                                   np.int(round((row[\"domain1_score\"])/3 * 10)) if row[\"essay_set\"]==4 else\n",
    "                                   np.int(round((row[\"domain1_score\"])/4 * 10)) if row[\"essay_set\"]==5 else\n",
    "                                   np.int(round((row[\"domain1_score\"])/4 * 10)) if row[\"essay_set\"]==6 else\n",
    "                                   np.int(round((row[\"domain1_score\"])/30 * 10)) if row[\"essay_set\"]==7 else\n",
    "                                   np.int(round((row[\"domain1_score\"])/60 * 10))\n",
    "                                   for index, row in training_set_rel3_df.iterrows() \n",
    "                                  ]\n",
    "                           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and stripping @*** (@CAPS1, @ORGANIZATION1) using the TweetTokenizer with strip_handles=True\n",
    "# Also convert to lowercase and strip punctuation (except @***)\n",
    "tknzr = nltk.tokenize.TweetTokenizer(strip_handles=True)\n",
    "dropped_punctuation = ''.join(s for s in string.punctuation if s!='@')\n",
    "dropped_punctuation = string.punctuation + '”“’—©'\n",
    "\n",
    "# Tokenizing the prompts\n",
    "tk_prompt_dict = {i:tknzr.tokenize(prompt_dict[i]) for i in prompt_dict.keys()}\n",
    "# Removing punctuation\n",
    "for i in tk_prompt_dict.keys():\n",
    "    tk_prompt_dict[i] = list(map(lambda x:x.translate(str.maketrans('', '', dropped_punctuation)).lower(),tk_prompt_dict[i]))\n",
    "    tk_prompt_dict[i] = [str(w).replace(' ','') for w in tk_prompt_dict[i]]\n",
    "    tk_prompt_dict[i] = [s for s in tk_prompt_dict[i] if s]\n",
    "\n",
    "# Adding a column for tokenized prompt\n",
    "training_set_rel3_df.insert(loc=5, column=\"tk_prompt\", \n",
    "                            value=[tk_prompt_dict[i] for i in training_set_rel3_df[\"essay_set\"]])\n",
    "\n",
    "# Tokenizing the essays\n",
    "tk_essay_list = [tknzr.tokenize(row[\"essay\"]) for index, row in training_set_rel3_df.iterrows()]\n",
    "# Removing punctuation\n",
    "for i in range(len(tk_essay_list)):\n",
    "    tk_essay_list[i] = list(map(lambda x:x.translate(str.maketrans('', '', dropped_punctuation)).lower(),tk_essay_list[i]))\n",
    "    tk_essay_list[i] = [str(w).replace(' ','') for w in tk_essay_list[i]]\n",
    "    tk_essay_list[i] = [s for s in tk_essay_list[i] if s]\n",
    "\n",
    "# Adding a column for tokenized essay\n",
    "training_set_rel3_df.insert(loc=6, column=\"tk_essay\", \n",
    "                            value=[tk_essay_list[i] for i in range(len(tk_essay_list))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>prompt</th>\n",
       "      <th>essay</th>\n",
       "      <th>scores</th>\n",
       "      <th>tk_prompt</th>\n",
       "      <th>tk_essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12975</th>\n",
       "      <td>21629</td>\n",
       "      <td>8</td>\n",
       "      <td>We all understand the benefits of laughter. Fo...</td>\n",
       "      <td>When you laugh, is @CAPS5 out of habit, or is ...</td>\n",
       "      <td>7</td>\n",
       "      <td>[we, all, understand, the, benefits, of, laugh...</td>\n",
       "      <td>[when, you, laugh, is, out, of, habit, or, is,...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12976</th>\n",
       "      <td>21630</td>\n",
       "      <td>8</td>\n",
       "      <td>We all understand the benefits of laughter. Fo...</td>\n",
       "      <td>Trippin' on fen...</td>\n",
       "      <td>7</td>\n",
       "      <td>[we, all, understand, the, benefits, of, laugh...</td>\n",
       "      <td>[trippin, on, fences, i, am, years, young, and...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12977</th>\n",
       "      <td>21633</td>\n",
       "      <td>8</td>\n",
       "      <td>We all understand the benefits of laughter. Fo...</td>\n",
       "      <td>Many people believe that laughter can improve...</td>\n",
       "      <td>7</td>\n",
       "      <td>[we, all, understand, the, benefits, of, laugh...</td>\n",
       "      <td>[many, people, believe, that, laughter, can, i...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       essay_id  essay_set                                             prompt  \\\n",
       "12975     21629          8  We all understand the benefits of laughter. Fo...   \n",
       "12976     21630          8  We all understand the benefits of laughter. Fo...   \n",
       "12977     21633          8  We all understand the benefits of laughter. Fo...   \n",
       "\n",
       "                                                   essay  scores  \\\n",
       "12975  When you laugh, is @CAPS5 out of habit, or is ...       7   \n",
       "12976                                 Trippin' on fen...       7   \n",
       "12977   Many people believe that laughter can improve...       7   \n",
       "\n",
       "                                               tk_prompt  \\\n",
       "12975  [we, all, understand, the, benefits, of, laugh...   \n",
       "12976  [we, all, understand, the, benefits, of, laugh...   \n",
       "12977  [we, all, understand, the, benefits, of, laugh...   \n",
       "\n",
       "                                                tk_essay  rater1_domain1  \\\n",
       "12975  [when, you, laugh, is, out, of, habit, or, is,...            20.0   \n",
       "12976  [trippin, on, fences, i, am, years, young, and...            20.0   \n",
       "12977  [many, people, believe, that, laughter, can, i...            20.0   \n",
       "\n",
       "       rater2_domain1  rater3_domain1      ...        rater2_trait3  \\\n",
       "12975            26.0            40.0      ...                  5.0   \n",
       "12976            20.0             NaN      ...                  4.0   \n",
       "12977            20.0             NaN      ...                  4.0   \n",
       "\n",
       "       rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  \\\n",
       "12975            5.0            5.0            5.0            4.0   \n",
       "12976            4.0            4.0            4.0            NaN   \n",
       "12977            4.0            4.0            4.0            NaN   \n",
       "\n",
       "       rater3_trait2  rater3_trait3  rater3_trait4  rater3_trait5  \\\n",
       "12975            4.0            4.0            4.0            4.0   \n",
       "12976            NaN            NaN            NaN            NaN   \n",
       "12977            NaN            NaN            NaN            NaN   \n",
       "\n",
       "       rater3_trait6  \n",
       "12975            4.0  \n",
       "12976            NaN  \n",
       "12977            NaN  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_rel3_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting histogram for prompt and essay token lengths\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,4))\n",
    "fig.subplots_adjust(wspace=.4)\n",
    "\n",
    "prompt_len = [len(training_set_rel3_df[\"tk_prompt\"][index]) for index, row in training_set_rel3_df.iterrows()]\n",
    "essay_len = [len(training_set_rel3_df[\"tk_essay\"][index]) for index, row in training_set_rel3_df.iterrows()]\n",
    "\n",
    "ax[0].hist(x=prompt_len, bins=50, edgecolor=\"w\")\n",
    "ax[0].set_title(\"Histogram of Prompt Length (Tokens)\")\n",
    "ax[0].set_xlabel(\"No. of Tokens\")\n",
    "ax[0].set_ylabel(\"Count\")\n",
    "\n",
    "ax[1].hist(x=essay_len, bins=50, edgecolor=\"w\")\n",
    "ax[1].set_title(\"Histogram of Essay Length (Tokens)\")\n",
    "ax[1].set_xlabel(\"No. of Tokens\")\n",
    "ax[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt lengths = {1153, 836, 45, 111, 1649, 1460, 58, 127}\n",
      "Setting max_len_p = 130\n",
      "\n",
      "Setting max_len = 650, \n",
      " 96.8% of essays are shorter than this length.\n"
     ]
    }
   ],
   "source": [
    "# Setting parameters for max_len\n",
    "print(\"Prompt lengths =\", set(prompt_len))\n",
    "max_len_p = 130\n",
    "print(\"Setting max_len_p =\", max_len_p)\n",
    "print()\n",
    "\n",
    "max_len = 650\n",
    "print(\"Setting max_len = {}, \\n {:.1f}% of essays are shorter than this length.\"\n",
    "      .format(max_len, 100*sum(i <= max_len for i in essay_len) / len(training_set_rel3_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding and trunking for array export\n",
    "# Padding and trunking will be done at the beginning for prompts. \n",
    "training_set_rel3_df.insert(loc=7, column=\"tk_pad_prompt\", \n",
    "                            value=[row[\"tk_prompt\"][-max_len_p:] if len(row[\"tk_prompt\"])>max_len_p else\n",
    "                                   [\"<s>\" for n in range(max_len_p - len(row[\"tk_prompt\"]))] + row[\"tk_prompt\"] \n",
    "                                   if len(row[\"tk_prompt\"])<max_len_p else\n",
    "                                   row[\"tk_prompt\"]\n",
    "                                   for index, row in training_set_rel3_df.iterrows() \n",
    "                                  ]\n",
    "                           )\n",
    "\n",
    "# Padding and trunking will be done at the ending for essays. \n",
    "training_set_rel3_df.insert(loc=8, column=\"tk_pad_essay\", \n",
    "                            value=[row[\"tk_essay\"][:max_len] if len(row[\"tk_essay\"])>max_len else\n",
    "                                   row[\"tk_essay\"] + [\"<s>\" for n in range(max_len - len(row[\"tk_essay\"]))] \n",
    "                                   if len(row[\"tk_essay\"])<max_len else\n",
    "                                   row[\"tk_essay\"]\n",
    "                                   for index, row in training_set_rel3_df.iterrows() \n",
    "                                  ]\n",
    "                           )\n",
    "\n",
    "# Adding columns for the length of the truncated prompt and the essay to identify no. of words before padding\n",
    "training_set_rel3_df.insert(loc=9, column=\"nw_prompt\", \n",
    "                            value=[len(row[\"tk_prompt\"]) if len(row[\"tk_prompt\"])<max_len_p else\n",
    "                                   max_len_p\n",
    "                                   for index, row in training_set_rel3_df.iterrows() \n",
    "                                  ]\n",
    "                           )\n",
    "\n",
    "training_set_rel3_df.insert(loc=10, column=\"nw_essay\", \n",
    "                            value=[len(row[\"tk_essay\"]) if len(row[\"tk_essay\"])<max_len else\n",
    "                                   max_len\n",
    "                                   for index, row in training_set_rel3_df.iterrows() \n",
    "                                  ]\n",
    "                           )\n",
    "\n",
    "# Checks for the length of the tokens\n",
    "assert set([len(training_set_rel3_df[\"tk_pad_prompt\"][index]) \n",
    "            for index, row in training_set_rel3_df.iterrows()])==set([max_len_p]), \"Checks for Prompt padding fail\"\n",
    "assert set([len(training_set_rel3_df[\"tk_pad_essay\"][index])\n",
    "            for index, row in training_set_rel3_df.iterrows()])==set([max_len]), \"Checks for Essay padding fail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train, dev and test sets\n",
    "train_set, test_set = train_test_split(training_set_rel3_df, test_size=0.1, random_state=0)\n",
    "train_set, dev_set = train_test_split(train_set, test_size=15/90, random_state=0)\n",
    "\n",
    "# Testing only on prompt 1\n",
    "# train_set, test_set = train_test_split(training_set_rel3_df[training_set_rel3_df[\"essay_set\"] == 1],\n",
    "#                                                                   test_size=0.1, random_state=0)\n",
    "# train_set, dev_set = train_test_split(train_set, test_size=15/90, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of examples in Train Set : 9732\n",
      "No. of examples in Dev Set   : 1947\n",
      "No. of examples in Test Set  : 1298\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of examples in Train Set :\", len(train_set))\n",
    "print(\"No. of examples in Dev Set   :\", len(dev_set))\n",
    "print(\"No. of examples in Test Set  :\", len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting out as arrays, Note: still need to convert to ids based on the embedding\n",
    "train_set_prompts = np.array(list(train_set[\"tk_pad_prompt\"]))\n",
    "train_set_essays = np.array(list(train_set[\"tk_pad_essay\"]))\n",
    "train_set_labels = np.array(list(train_set[\"scores\"]))\n",
    "\n",
    "dev_set_prompts = np.array(list(dev_set[\"tk_pad_prompt\"]))\n",
    "dev_set_essays = np.array(list(dev_set[\"tk_pad_essay\"]))\n",
    "dev_set_labels = np.array(list(dev_set[\"scores\"]))\n",
    "\n",
    "test_set_prompts = np.array(list(test_set[\"tk_pad_prompt\"]))\n",
    "test_set_essays = np.array(list(test_set[\"tk_pad_essay\"]))\n",
    "test_set_labels = np.array(list(test_set[\"scores\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting out the no. of words prior to padding\n",
    "train_set_prompt_nw = np.array(list(train_set[\"nw_prompt\"]))\n",
    "train_set_essay_nw = np.array(list(train_set[\"nw_essay\"]))\n",
    "\n",
    "dev_set_prompt_nw = np.array(list(dev_set[\"nw_prompt\"]))\n",
    "dev_set_essay_nw = np.array(list(dev_set[\"nw_essay\"]))\n",
    "\n",
    "test_set_prompt_nw = np.array(list(test_set[\"nw_prompt\"]))\n",
    "test_set_essay_nw = np.array(list(test_set[\"nw_essay\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['the', 'mood', 'in', 'the', 'memoir', 'that', 'the', 'author',\n",
       "       'creates', 'is'], dtype='<U35')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the array shape\n",
    "assert train_set_essays.shape == (len(train_set),max_len), \"Checks for array shape fail\"\n",
    "\n",
    "#Checking first 10 tokens of index 0 of train_set_essays\n",
    "train_set_essays[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "A baseline was established with Neural Bag of Words\n",
    "\n",
    "![Neural Bag-of-Words Model](images/neural_bow.png)\n",
    "\n",
    "Using the following notation:\n",
    "- $w^{(i)} \\in \\mathbb{Z}$ for the $i^{th}$ word of the sequence (as an integer index)\n",
    "- $x^{(i)} \\in \\mathbb{R}^d$ for the vector representation (embedding) of $w^{(i)}$\n",
    "- $x \\in \\mathbb{R}^d$ for the fixed-length vector given by summing all the $x^{(i)}$ for an example\n",
    "- $h^{(j)}$ for the hidden state after the $j^{th}$ fully-connected layer\n",
    "- $y$ for the target label ($\\in 1,\\ldots,\\mathtt{num\\_classes}$)\n",
    "\n",
    "Our model is defined as:\n",
    "- **Embedding layer:** $x^{(i)} = W_{embed}[w^{(i)}]$\n",
    "- **Summing vectors:** $x = \\sum_{i=1}^n x^{(i)}$\n",
    "- **Hidden layer(s):** $h^{(j)} = f(h^{(j-1)} W^{(j)} + b^{(j)})$ where $h^{(-1)} = x$ and $j = 0,1,\\ldots,J-1$\n",
    "- **Output layer:** $\\hat{y} = \\hat{P}(y) = \\mathrm{softmax}(h^{(final)} W_{out} + b_{out})$ where $h^{(final)} = h^{(J-1)}$ is the output of the last hidden layer.\n",
    "\n",
    "Logits for the softmax is defined as:\n",
    "$$ \\mathrm{logits} = h^{(final)}W_{out} + b_{out} $$\n",
    "\n",
    "Other dimensions:\n",
    "- `V`: the vocabulary size\n",
    "- `embed_dim`: the embedding dimension $d$\n",
    "- `hidden_dims`: a list of dimensions for the output of each hidden layer (i.e. $\\mathrm{dim}(h^{(j)})$&nbsp;=&nbsp;`hidden_dims[j]`)\n",
    "- `num_classes`: the number of target classes (2 for the binary task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Vocabulary\n",
    "\n",
    "# Set the threshold number of occurences to prune words from the vocab\n",
    "prune_threshold = 5\n",
    "\n",
    "# Defining sorting and unique functions\n",
    "def uniq(lst):\n",
    "    last = object()\n",
    "    for item in lst:\n",
    "        if item == last:\n",
    "            continue\n",
    "        yield item\n",
    "        last = item\n",
    "\n",
    "def sort_and_deduplicate(lst):\n",
    "    return list(uniq(sorted(lst, reverse=False)))\n",
    "\n",
    "def word_ctr(lst):\n",
    "    ctr_list = []\n",
    "    last = object()\n",
    "    for word in lst:\n",
    "        if word == last:\n",
    "            ctr_list[-1] += 1\n",
    "        else:\n",
    "            ctr_list.append(1)\n",
    "        last = word\n",
    "    return ctr_list\n",
    "\n",
    "def prune(lst, ctr_lst, num=10):\n",
    "    assert len(lst) == len(ctr_lst), \"List must have same length\"\n",
    "    for i in range(len(lst)):\n",
    "        if ctr_lst[i] < num and ctr_lst[i] != \"<s>\":\n",
    "            lst[i] = None\n",
    "    return [w for w in lst if w is not None]\n",
    "\n",
    "# Add in words from prompts and essays\n",
    "words = []\n",
    "for key in tk_prompt_dict.keys():\n",
    "    words += tk_prompt_dict[key]\n",
    "for index, row in training_set_rel3_df.iterrows():\n",
    "    words += [w for w in row[\"tk_essay\"]]\n",
    "\n",
    "vocab = sort_and_deduplicate(words)\n",
    "# Pruning words which have less than 10 occurences\n",
    "vocab_ctr = word_ctr(sorted(words, reverse=False))\n",
    "vocab = prune(vocab, vocab_ctr , num=prune_threshold) \n",
    "# Pre-load in the words for padding and unknown,\n",
    "vocab = [\"<s>\", \"<unk>\"] + vocab\n",
    "\n",
    "# Removing numbers but not <s> or <unk>\n",
    "vocab = [w for w in vocab if not any(char.isdigit() for char in w)]\n",
    "\n",
    "# Checking if there are any \"@***\" not parsed out\n",
    "for index, row in training_set_rel3_df.iterrows():\n",
    "    for w in row[\"tk_essay\"]:\n",
    "        if '@' in w:\n",
    "            print(\"Issues with index\", index)\n",
    "            \n",
    "# Defining a dictionary for the vocabulary\n",
    "vocab_dict = {}\n",
    "for ctr in range(len(vocab)):\n",
    "    vocab_dict[vocab[ctr]] = ctr\n",
    "\n",
    "# Defining a get word id function\n",
    "def get_vocab_id(word):\n",
    "    if word in vocab_dict.keys():\n",
    "        return vocab_dict[word]\n",
    "    else:\n",
    "        return vocab_dict[\"<unk>\"]\n",
    "       \n",
    "# Defining a get word from id function\n",
    "def get_vocab_word(ids):\n",
    "    for key, value in vocab_dict.items():\n",
    "        if ids == value: \n",
    "            return key\n",
    "        else:\n",
    "            return \"<unk>\"\n",
    "\n",
    "# Defining a fuction to convert arrays (train_set_essay / train_set_prompts) into word ids\n",
    "def get_id_array(arr):\n",
    "    arr_ids = np.array(list(list(map(get_vocab_id, row)) for row in arr))\n",
    "    return arr_ids\n",
    "\n",
    "# Setting the size of the Vocabulary\n",
    "V = len(vocab_dict)\n",
    "print(\"The size of the vocabulary is\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the word ids for feeding into the model\n",
    "train_set_prompt_ids = get_id_array(train_set_prompts)\n",
    "train_set_essay_ids = get_id_array(train_set_essays)\n",
    "\n",
    "dev_set_prompt_ids = get_id_array(dev_set_prompts)\n",
    "dev_set_essay_ids = get_id_array(dev_set_essays)\n",
    "\n",
    "test_set_prompt_ids = get_id_array(test_set_prompts)\n",
    "test_set_essay_ids = get_id_array(test_set_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For classification:\n",
    "num_score_classes = max(set(test_set_labels)) + 1\n",
    "# For regression:\n",
    "# num_score_classes = 1\n",
    "print(\"No. of class for the classification:\", num_score_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the various layers in the NBOW model\n",
    "\n",
    "def embedding_layer(ids_, V, embed_dim, init_scale=0.001):\n",
    "    \"\"\"Construct an embedding layer.\n",
    "    Args:\n",
    "        ids_: [batch_size, max_len] Tensor of int32, integer ids\n",
    "        V: (int) vocabulary size\n",
    "        embed_dim: (int) embedding dimension\n",
    "        init_scale: (float) scale to initialize embeddings\n",
    "\n",
    "    Returns:\n",
    "        xs_: [batch_size, max_len, embed_dim] Tensor of float32, embeddings for\n",
    "            each element in ids_\n",
    "    \"\"\"\n",
    "    ## Assigning variables\n",
    "    W_embed_ = tf.get_variable('W_embed', shape=[V, embed_dim], dtype=tf.float32,\n",
    "                          initializer=tf.random_uniform_initializer(-init_scale, init_scale))\n",
    "    \n",
    "    ## Looking up embeddings\n",
    "    xs_ = tf.nn.embedding_lookup(params=W_embed_, ids=ids_)\n",
    "\n",
    "    #### END(YOUR CODE) ####\n",
    "    return xs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected_layers(h0_, hidden_dims, activation=tf.tanh,\n",
    "                           dropout_rate=0, is_training=False):\n",
    "    \"\"\"Construct a stack of fully-connected layers.\n",
    "\n",
    "    Args:\n",
    "        h0_: [batch_size, d] Tensor of float32, the input activations\n",
    "        hidden_dims: list(int) dimensions of the output of each layer\n",
    "        activation: TensorFlow function, such as tf.tanh. Passed to\n",
    "            tf.layers.dense.\n",
    "        dropout_rate: if > 0, will apply dropout to activations.\n",
    "        is_training: (bool) if true, is in training mode\n",
    "\n",
    "    Returns:\n",
    "        h_: [batch_size, hidden_dims[-1]] Tensor of float32, the activations of\n",
    "            the last layer constructed by this function.\n",
    "    \"\"\"\n",
    "    h_ = h0_\n",
    "    for i, hdim in enumerate(hidden_dims):\n",
    "        h_ = tf.layers.dense(h_, hdim, activation=activation, name=(\"Hidden_%d\"%i))\n",
    "\n",
    "        if dropout_rate > 0:\n",
    "            h_ = tf.layers.dropout(h_, rate=dropout_rate, training=is_training) \n",
    "\n",
    "    return h_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_output_layer(h_, labels_, num_classes):\n",
    "    \"\"\"Construct a softmax output layer.\n",
    "\n",
    "    Implements:\n",
    "        logits = h W + b\n",
    "        loss = cross_entropy(softmax(logits), labels)\n",
    "\n",
    "    Args:\n",
    "        h_: [batch_size, d] Tensor of float32, the input activations from a\n",
    "            previous layer\n",
    "        labels_: [batch_size] Tensor of int32, the target label ids\n",
    "        num_classes: (int) the number of output classes\n",
    "\n",
    "    Returns: (loss_, logits_)\n",
    "        loss_: scalar Tensor of float32, the cross-entropy loss\n",
    "        logits_: [batch_size, num_classes] Tensor of float32, the logits (hW + b)\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"Logits\"):\n",
    "        \n",
    "        ##Assigning variables\n",
    "        W_out_ = tf.get_variable(\"W_out\", shape=[h_.shape[-1], num_classes], \n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.random_normal_initializer())\n",
    "        b_out_ = tf.get_variable(\"b_out\", shape=[num_classes,],\n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.zeros_initializer())\n",
    "        # Calculating logits\n",
    "        logits_ = tf.matmul(h_, W_out_) + b_out_\n",
    "        \n",
    "#     # If no labels provided, don't try to compute loss.\n",
    "#     if labels_ is None:\n",
    "#         return None, logits_\n",
    "\n",
    "# Should use regression loss??...\n",
    "# tf.losses.mean_squared_error(\n",
    "#     labels,\n",
    "#     predictions,\n",
    "#     weights=1.0,\n",
    "#     scope=None,\n",
    "#     loss_collection=tf.GraphKeys.LOSSES,\n",
    "#     reduction=Reduction.SUM_BY_NONZERO_WEIGHTS\n",
    "# )\n",
    "\n",
    "    with tf.name_scope(\"Softmax\"):       \n",
    "        loss_ = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=labels_, logits=logits_))\n",
    "\n",
    "#     with tf.name_scope(\"MSE\"):\n",
    "#         loss = tf.reduce_mean(tf.square(predictions_ - labels_))\n",
    "#         loss = tf.losses.mean_squared_error(labels=labels_, predictions=predictions_, weights=1.0)\n",
    "\n",
    "    return loss_, logits_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BOW_encoder(ids_, ns_, V, embed_dim, hidden_dims, dropout_rate=0,\n",
    "                is_training=None,\n",
    "                **unused_kw):\n",
    "    \"\"\"Construct a bag-of-words encoder.\n",
    "\n",
    "        - Build the embeddings (using embedding_layer(...))\n",
    "        - Apply the mask to zero-out padding indices, and sum the embeddings\n",
    "            for each example\n",
    "        - Build a stack of hidden layers (using fully_connected_layers(...))\n",
    "\n",
    "    Note that this function returns the final encoding h_ as well as the masked\n",
    "    embeddings xs_. The latter is used for L2 regularization, so that we can\n",
    "    penalize the norm of only those vectors that were actually used for each\n",
    "    example.\n",
    "\n",
    "    Args:\n",
    "        ids_: [batch_size, max_len] Tensor of int32, integer ids\n",
    "        ns_:  [batch_size] Tensor of int32, (clipped) length of each sequence\n",
    "        V: (int) vocabulary size\n",
    "        embed_dim: (int) embedding dimension\n",
    "        hidden_dims: list(int) dimensions of the output of each layer\n",
    "        dropout_rate: (float) rate to use for dropout\n",
    "        is_training: (bool) if true, is in training mode\n",
    "\n",
    "    Returns: (h_, xs_)\n",
    "        h_: [batch_size, hidden_dims[-1]] Tensor of float32, the activations of\n",
    "            the last layer constructed by this function.\n",
    "        xs_: [batch_size, max_len, embed_dim] Tensor of float32, the per-word\n",
    "            embeddings as returned by embedding_layer and with the mask applied\n",
    "            to zero-out the pad indices.\n",
    "    \"\"\"\n",
    "    assert is_training is not None, \"is_training must be explicitly set to True or False\"\n",
    "    # Embedding layer should produce:\n",
    "    #   xs_: [batch_size, max_len, embed_dim]\n",
    "    with tf.variable_scope(\"Embedding_Layer\"):\n",
    "         xs_ = embedding_layer(ids_, V, embed_dim, init_scale=0.001)\n",
    "\n",
    "    # Mask off the padding indices with zeros\n",
    "    #   mask_: [batch_size, max_len, 1] with values of 0.0 or 1.0\n",
    "    mask_ = tf.expand_dims(tf.sequence_mask(ns_, xs_.shape[1],\n",
    "                                           dtype=tf.float32), -1)\n",
    "    # Multiply xs_ by the mask to zero-out pad indices.\n",
    "    xs_ = tf.multiply(xs_, mask_)\n",
    "       \n",
    "    # Sum embeddings: [batch_size, max_len, embed_dim] -> [batch_size, embed_dim]\n",
    "    h0_ = tf.math.reduce_sum(xs_, axis=1)\n",
    "          \n",
    "    # Build a stack of fully-connected layers\n",
    "    h_ = fully_connected_layers(h0_, hidden_dims, activation=tf.tanh,\n",
    "                                dropout_rate=dropout_rate, is_training=is_training)\n",
    "    \n",
    "    return h_, xs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_model_fn(features, labels, mode, params):\n",
    "\n",
    "    # Check if this graph is going to be used for training.\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    if params['encoder_type'] == 'bow':\n",
    "        with tf.variable_scope(\"Encoder\"):\n",
    "            h_, xs_ = BOW_encoder(features['ids'], features['ns'],\n",
    "                                  is_training=is_training,\n",
    "                                  **params)\n",
    "    else:\n",
    "        raise ValueError(\"Error: unsupported encoder type \"\n",
    "                         \"'{:s}'\".format(params['encoder_type']))\n",
    "\n",
    "    # Construct softmax layer and loss functions\n",
    "    with tf.variable_scope(\"Output_Layer\"):\n",
    "        ce_loss_, logits_ = softmax_output_layer(h_, labels, params['num_classes'])\n",
    "\n",
    "    with tf.name_scope(\"Prediction\"):\n",
    "        pred_proba_ = tf.nn.softmax(logits_, name=\"pred_proba\")\n",
    "        pred_max_ = tf.argmax(logits_, 1, name=\"pred_max\")\n",
    "        predictions_dict = {\"proba\": pred_proba_, \"max\": pred_max_}\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # If predict mode, don't bother computing loss.\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          predictions=predictions_dict)\n",
    "\n",
    "    # L2 regularization (weight decay) on parameters, from all layers\n",
    "    with tf.variable_scope(\"Regularization\"):\n",
    "        l2_penalty_ = tf.nn.l2_loss(xs_)  # l2 loss on embeddings\n",
    "        for var_ in tf.trainable_variables():\n",
    "            if \"Embedding_Layer\" in var_.name:\n",
    "                continue\n",
    "            l2_penalty_ += tf.nn.l2_loss(var_)\n",
    "        l2_penalty_ *= params['beta']  # scale by regularization strength\n",
    "        tf.summary.scalar(\"l2_penalty\", l2_penalty_)\n",
    "        regularized_loss_ = ce_loss_ + l2_penalty_\n",
    "\n",
    "    with tf.variable_scope(\"Training\"):\n",
    "        if params['optimizer'] == 'adagrad':\n",
    "            optimizer_ = tf.train.AdagradOptimizer(params['lr'])\n",
    "        else:\n",
    "            optimizer_ = tf.train.GradientDescentOptimizer(params['lr'])\n",
    "        train_op_ = optimizer_.minimize(regularized_loss_,\n",
    "                                        global_step=tf.train.get_global_step())\n",
    "\n",
    "    tf.summary.scalar(\"cross_entropy_loss\", ce_loss_)\n",
    "    eval_metrics = {\"cross_entropy_loss\": tf.metrics.mean(ce_loss_),\n",
    "                    \"accuracy\": tf.metrics.accuracy(labels, pred_max_)}\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      predictions=predictions_dict,\n",
    "                                      loss=regularized_loss_,\n",
    "                                      train_op=train_op_,\n",
    "                                      eval_metric_ops=eval_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x, ns, y = train_set_essay_ids, train_set_essay_nw, train_set_labels\n",
    "batch_size = 36\n",
    "\n",
    "# Specify model hyperparameters as used by model_fn\n",
    "model_params = dict(V=V, embed_dim=100, hidden_dims=[50], num_classes=num_score_classes,\n",
    "                    encoder_type='bow',\n",
    "                    lr=0.1, optimizer='adagrad', beta=0.01)\n",
    "model_fn = classifier_model_fn\n",
    "\n",
    "total_batches = 0\n",
    "total_examples = 0\n",
    "total_loss = 0\n",
    "loss_ema = np.log(2)  # track exponential-moving-average of loss\n",
    "ema_decay = np.exp(-1/10)  # decay parameter for moving average = np.exp(-1/history_length)\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    ##\n",
    "    # Construct the graph here. No session.run calls - just wiring up Tensors.\n",
    "    ##\n",
    "    # Add placeholders so we can feed in data.\n",
    "    x_ph_  = tf.placeholder(tf.int32, shape=[None, x.shape[1]])  # [batch_size, max_len]\n",
    "    ns_ph_ = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "    y_ph_  = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "    \n",
    "    # Construct the graph using model_fn\n",
    "    features = {\"ids\": x_ph_, \"ns\": ns_ph_}  # note that values are Tensors\n",
    "    estimator_spec = model_fn(features, labels=y_ph_, mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                              params=model_params)\n",
    "    loss_     = estimator_spec.loss\n",
    "    train_op_ = estimator_spec.train_op\n",
    "    \n",
    "    ##\n",
    "    # Done constructing the graph, now we can make session.run calls.\n",
    "    ##\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Run a single epoch\n",
    "    t0 = time.time()\n",
    "    for (bx, bns, by) in utils.multi_batch_generator(batch_size, x, ns, y):\n",
    "        # feed NumPy arrays into the placeholder Tensors\n",
    "        feed_dict = {x_ph_: bx, ns_ph_: bns, y_ph_: by}\n",
    "        batch_loss, _ = sess.run([loss_, train_op_], feed_dict=feed_dict)\n",
    "        \n",
    "        # Compute some statistics\n",
    "        total_batches += 1\n",
    "        total_examples += len(bx)\n",
    "        total_loss += batch_loss * len(bx)  # re-scale, since batch loss is mean\n",
    "        # Compute moving average to smooth out noisy per-batch loss\n",
    "        loss_ema = ema_decay * loss_ema + (1 - ema_decay) * batch_loss\n",
    "        \n",
    "        if (total_batches % 25 == 0):\n",
    "            print(\"{:5,} examples, moving-average loss {:.2f}\".format(total_examples, \n",
    "                                                                      loss_ema))    \n",
    "    print(\"Completed one epoch in {:s}\".format(utils.pretty_timedelta(since=t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model hyperparameters as used by model_fn\n",
    "model_params = dict(V=V, embed_dim=100, hidden_dims=[50], num_classes=num_score_classes,\n",
    "                    encoder_type='bow',\n",
    "                    lr=0.1, optimizer='adagrad', beta=0.01)\n",
    "\n",
    "checkpoint_dir = \"/tmp/project/tf_baseline_nbow\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "if os.path.isdir(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "\n",
    "model = tf.estimator.Estimator(model_fn=classifier_model_fn, \n",
    "                               params=model_params,\n",
    "                               model_dir=checkpoint_dir)\n",
    "print(\"\")\n",
    "print(\"To view training (once it starts), run:\\n\")\n",
    "print(\"    tensorboard --logdir='{:s}' --port 6006\".format(checkpoint_dir))\n",
    "print(\"\\nThen in your browser, open: http://localhost:6006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training params, just used in this cell for the input_fn-s\n",
    "train_params = dict(batch_size=batch_size, total_epochs=20, eval_every=2)\n",
    "assert(train_params['total_epochs'] % train_params['eval_every'] == 0)\n",
    "\n",
    "# Construct and train the model, saving checkpoints to the directory above.\n",
    "# Input function for training set batches\n",
    "# Do 'eval_every' epochs at once, followed by evaluating on the dev set.\n",
    "# NOTE: use patch_numpy_io.numpy_input_fn instead of tf.estimator.inputs.numpy_input_fn\n",
    "train_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                    x={\"ids\": train_set_essay_ids, \"ns\": train_set_essay_nw}, y=train_set_labels,\n",
    "                    batch_size=train_params['batch_size'], \n",
    "                    num_epochs=train_params['eval_every'], shuffle=True, seed=42\n",
    "                 )\n",
    "\n",
    "# Input function for dev set batches. As above, but:\n",
    "# - Don't randomize order\n",
    "# - Iterate exactly once (one epoch)\n",
    "dev_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": dev_set_essay_ids, \"ns\": dev_set_essay_nw}, y=dev_set_labels,\n",
    "                    batch_size=batch_size, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "for _ in range(train_params['total_epochs'] // train_params['eval_every']):\n",
    "    # Train for a few epochs, then evaluate on dev\n",
    "    model.train(input_fn=train_input_fn)\n",
    "    eval_metrics = model.evaluate(input_fn=dev_input_fn, name=\"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensorboard charts for the baseline using Neural Bag of Words\n",
    "\n",
    "![Neural Bag-of-Words Training Graphs](images/nbow_train_graphs_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": test_set_essay_ids, \"ns\": test_set_essay_nw}, y=test_set_labels,\n",
    "                    batch_size=batch_size, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "eval_metrics = model.evaluate(input_fn=test_input_fn, name=\"test\")\n",
    "\n",
    "print(\"Accuracy on test set: {:.02%}\".format(eval_metrics['accuracy']))\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the LSTM with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM with Attention](images/LSTM_Project.png)\n",
    "\n",
    "- $h_m$ for last hidden state of LSTM (Essay representation)\n",
    "- $h_{m+n}$ for last hidden state of LSTM with Prompt (Essay representation)\n",
    "\n",
    "Semantic Score ($S_e$)\n",
    "\n",
    "- $S_e = sigmod(w_s h_m + b_s)$\n",
    "- $w_s$ for weighted matrix of the dense layer\n",
    "- $b_s$ stands for the bias\n",
    "- $obj(S_e,\\bar{S_e}) = 1/N \\sum_{i=1}^n (S_i - \\bar{S_i})^2 $ \n",
    "- $S_e$ for predict score set of training samples\n",
    "- $\\bar{S_e}$ for the original hand marked score set\n",
    "\n",
    "Coherence Score ($C_e$)\n",
    "\n",
    "- $C_e = sigmod(w_c h_m + b_c)$\n",
    "- $w_c$ for weighted matrix of the dense layer\n",
    "- $b_c$ stands for the bias\n",
    "- $obj(C_e,\\bar{C_e}) = 1/N \\sum_{i=1}^n (C_i - \\bar{C_i})^2 $ \n",
    "- $C_e$ for predict coherence score set of training samples\n",
    "- $\\bar{C_e}$ for the gold coherence score which is equal to the corresponding hand marked scores\n",
    "\n",
    "Prompt-relevant Score ($P_e$)\n",
    "\n",
    "- $P_e = sigmod(w_p h_{m+n} + b_p)$\n",
    "- $w_p$ for weighted matrix of the dense layer\n",
    "- $b_p$ stands for the bias\n",
    "- $obj(P_e,\\bar{P_e}) = 1/N \\sum_{i=1}^n (P_i - \\bar{P_i})^2 $ \n",
    "- $P_e$ for predict Prompt-relevant score set of training samples\n",
    "- $\\bar{P_e}$ for the gold prompt-relevant score which is equal to the corresponding hand marked scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building Model\n",
    "from keras import initializers, regularizers, optimizers\n",
    "from keras.layers import LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from keras.models import Sequential, load_model, model_from_config\n",
    "import layers; reload(layers)\n",
    "from layers import AttentionWithContext, Addition\n",
    "\n",
    "def get_model(hidden_units):\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    num_classes = 2\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_units, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, hidden_units], return_sequences=True))\n",
    "    #add attention\n",
    "    model.add(AttentionWithContext())\n",
    "    model.add(Addition())\n",
    "    model.add(Dense(num_classes, activation='sigmoid'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_22 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "attention_with_context_17 (A (None, 1, 300)            90600     \n",
      "_________________________________________________________________\n",
      "addition_17 (Addition)       (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 812,402\n",
      "Trainable params: 812,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_22_input to have 3 dimensions, but got array with shape (9732, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-07d0136cd02f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlstm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_set_essay_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tmp/project/rnnlm_trained'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected lstm_22_input to have 3 dimensions, but got array with shape (9732, 1)"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "lstm_model = get_model(300)\n",
    "lstm_model.fit(train_features,train_set_labels, batch_size=64, epochs=50)\n",
    "y_pred = lstm_model.predict(dev_set_essay_ids)\n",
    "lstm_model.save('/tmp/project/rnnlm_trained')\n",
    "\n",
    "#Save Result\n",
    "result = cohen_kappa_score(dev_set_labels,y_pred,weights='quadratic')\n",
    "print(\"Kappa Score: {}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<bert.run_classifier.InputFeatures at 0x7f43c48009b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d9c88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d9d30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4755e80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c48721d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c48720b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d16a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763a58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dda90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4872080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d11d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47ddcf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f441c772e48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c48002b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4872128>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5ef0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd4e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d9320>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c488a9b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5a58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763be0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7e80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac72e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd320>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac55f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac76d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1f98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434dd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1d30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7b38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d19e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9b38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7da0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c485cf60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763dd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd0b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1320>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c48005f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763c18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47632b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1ef0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47631d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9c18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1ac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac55c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5f98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4755f98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4872278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd2e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f97f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47ddd30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47ddc50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dde80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763b70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac72b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763f98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac59b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47630b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f95c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac71d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47639e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dddd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5198>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763a20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba4349e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434a58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1b38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d9a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d9400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434c88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475def0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800c18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c48003c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac58d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1128>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1470>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9ac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800b00>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5cc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f99e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c488af60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d9b38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9198>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4755dd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac53c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434c18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f441d0bdac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4872048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4755eb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f441d0bd978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d9470>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c48002e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c488af98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7d30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d9390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d90b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d2e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1b00>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d9518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1c18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d9550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434e48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5f28>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f441d050860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d9b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d9dd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c488a940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d9f98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d12b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475dc18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd3c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763ba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475db70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f441d050390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4872198>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475df60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4872160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f441d050518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434b38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475de48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac56d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f92b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac75c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800ac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9780>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800cf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5be0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1dd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800ef0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d9748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5b00>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7e48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434f60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47ddc88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f441d0bd828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800780>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800128>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d780>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800d30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d128>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47ddf28>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d97b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47ddfd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d17b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f441d0bd860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d94e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d198>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9668>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1d68>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763cc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f441d0bd748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba4347f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c48008d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800c88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d9080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dda58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5f60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd780>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800eb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d92b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1eb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47635f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763ef0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5128>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1f28>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd6d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac79b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f441d0bd7b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5d68>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9128>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d18d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7be0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d7f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800e48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d9e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475da58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475da90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dde48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800d68>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac57b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47639b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5cf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434f98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9320>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c488ae80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0ff60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800e80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763b00>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763320>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763c88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c48000b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47632e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0fa90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5da0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7b70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763d68>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c488ae10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7a20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d9588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba4348d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac54e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47ddac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47ddb00>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c48004a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f441d0bd4e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475dcc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd6a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9208>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d95c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dda20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba4349b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f96a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475dcf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1e80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d94a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475deb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5e48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f128>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d470>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c48007b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475dac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d14e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d9630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9be0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac77b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47633c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f441d0bd908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475dbe0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434d68>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0fac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0fc88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434f28>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a2b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760f60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800da0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f0f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd7b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f9b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47ddd68>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47ddef0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47ddbe0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5ba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475aa58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd198>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dde10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd1d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0fa58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475acf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d15c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac52e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760b70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac59e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1da0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d2b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800198>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5eb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5470>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f93c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a9b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9e48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800f28>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475ab00>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f441d0bdc18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7dd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1c88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7ac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a9e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d4a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760f28>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5320>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5208>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d99e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47603c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d0b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f4e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763eb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac70b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d7b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0fdd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763f28>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0fe10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760b00>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0fcc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47ddf98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475dba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47601d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c488a8d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1b70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d5f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760be0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800668>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d97f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac74a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475af98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a3c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a7b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7f98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47638d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f441d0bdb38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f5c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475afd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d10b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9470>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475ae10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47630f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434e80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a208>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a780>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f96d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f7f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0fef0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0feb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d9828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0fc50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f6d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760e80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd668>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f94a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9cf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434eb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475dfd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760b38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f2b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd208>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd9e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d8d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7320>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f7b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475aeb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d9198>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800dd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760c88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b0f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f99b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b0b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a668>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434cc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b8d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d9048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475de10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47636a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434ba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac56a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7c18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47637b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475bba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47ddda0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434b00>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b470>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0fcf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475df28>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d14a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47606d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47ddf60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd0f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d9668>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5780>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0ff98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434a20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475bf28>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0ffd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434ac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434cf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475df98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763668>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47606a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800470>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c48000f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475aba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d3c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475bd30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7c88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434be0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475db00>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac57f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9cc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7668>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f1d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9a20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47602b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f3c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b5f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760a20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800208>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5dd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c48004e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b4a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475bf98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7a58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434ef0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c48001d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d9710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d12e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47616d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475abe0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47619b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd8d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475bc88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d19b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434d30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b320>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f6a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760eb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac54a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a128>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800320>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f95f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761ba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0ff28>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac79e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47609b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c48005c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d92e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d16d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800a58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475ae80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475acc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1e48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47600b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761e80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0fd30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d13c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd470>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763ac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b6d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47608d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7ef0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac52b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd5c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475be80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d93c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d15f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7208>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1ba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd9b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47616a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763da0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475bda0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47613c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475dda0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a0b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5c18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd128>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7198>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d89b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a198>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47607b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a4e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f5f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f198>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47612b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b5c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd5f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0fd68>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760470>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0fb38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b2e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761c18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434da0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9ba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8dd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8e48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba434b70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b780>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475be10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac73c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac51d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d7908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d95f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d89e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761c88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763b38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f9e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f94e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475ab70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800a20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d9438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800f98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1208>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475beb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba4347b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8f98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761be0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800be0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761cc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4804ef0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1cf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9b00>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d7a58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47ddb38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475dc50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d0f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a2e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47605c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c48045c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8d30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d7c18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac50b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761d30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760780>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475ad30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0fba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47612e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f92e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475aef0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d7390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760c18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761b00>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475da20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760a58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4804828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0fb00>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c48044e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0fda0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d79e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7f60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4804898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8cf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac50f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1198>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47634a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4804d30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5d30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d7438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47db470>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47db278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d7be0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5ac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c48006a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f97b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47614e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47db438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763d30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5668>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8198>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dbac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47db898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d9358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475aac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761f60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d7940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f90f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c48046a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5b38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1a20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac76a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47611d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a470>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c48040b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763e48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d82b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d75f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47610f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800b38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8ac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47615c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5a20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1a58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f668>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4804e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4804be0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f4a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d7e80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4804f60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a5f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d96d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47604e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4804080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761a20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d96a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d7400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761a58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7128>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761e48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d9128>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d10f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a6a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475ada0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475af60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8f60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5e80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5b70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d84e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b9e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d7240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47614a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c48044a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475dd68>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47db828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7ba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac75f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d6d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4804518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475be48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8f28>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d7860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475bb38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a320>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d84a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4804e48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac78d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4804908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f85f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f8ba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a6d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475bbe0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d7780>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4804c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47db5f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f780>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d7f98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b2b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47634e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a1d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f8e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b7f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8208>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47db4a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4763208>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4804cc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761b70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f8208>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d4e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d320>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d77f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f98d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f90b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760ba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac5c88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760cc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0fb70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d80f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8470>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760da0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f86d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dbb70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475bcc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4804748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac70f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475bb70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4804e80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c48009e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d7ef0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47db588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f83c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d7e48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7cc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761eb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f8588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0fbe0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d86a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8320>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d7e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c48007f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f86a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7cf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d7978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8a20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f9b70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475ba20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f2e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4804cf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d7358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475a048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d7eb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d77b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d7a20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f8160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac77f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f8860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0fe48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f208>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dbc88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b3c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47635c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c48047f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d74a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47db400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47db160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c48048d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47604a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47db518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d6a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761668>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47618d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4804860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475ab38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760dd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d1518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4804a58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f91d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4804c88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f82e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f8a20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4804160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475d080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d83c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4804588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dbd30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f8f98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0fa20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d87f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f8f60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f8278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8b38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761f98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c1ac7f28>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800cc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c48042b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f89b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f8cf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760198>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47610b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47db630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f8d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f0b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4804f98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d8358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dbba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d7048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47dd2b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475b160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47f8da0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4760ac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4800f60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761128>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c47d80b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c475bf60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43bad0f278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43c4761898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f43ba3da048>,\n",
       " ...]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Kappa score after training: \",np.around(np.array(results).mean(),decimals=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict\n",
    "model = get_model()\n",
    "model.load_weights(\"/tmp/project/rnnlm_trained\")\n",
    "preds = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-headed Creativity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Embeddings\n",
    "\n",
    "For the word embeddings, we utilized the pre-trained BERT word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-tensorflow in /home/smus78/anaconda3/lib/python3.6/site-packages (1.0.1)\n",
      "Requirement already satisfied: six in /home/smus78/anaconda3/lib/python3.6/site-packages (from bert-tensorflow) (1.11.0)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tensorflow-hub in /home/smus78/anaconda3/lib/python3.6/site-packages (0.3.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/smus78/anaconda3/lib/python3.6/site-packages (from tensorflow-hub) (1.14.3)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/smus78/anaconda3/lib/python3.6/site-packages (from tensorflow-hub) (1.11.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /home/smus78/anaconda3/lib/python3.6/site-packages (from tensorflow-hub) (3.6.1)\n",
      "Requirement already satisfied: setuptools in /home/smus78/anaconda3/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow-hub) (39.1.0)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0402 23:27:23.608345 139931951523648 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-tensorflow\n",
    "\n",
    "# BERT\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "\n",
    "!pip install tensorflow-hub\n",
    "\n",
    "# TF hub \n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
    "train_InputExamples = train_set.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[\"essay\"], \n",
    "                                                                   text_b = x[\"prompt\"], \n",
    "                                                                   label = x[\"scores\"]), axis = 1)\n",
    "\n",
    "dev_InputExamples = dev_set.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[\"essay\"], \n",
    "                                                                   text_b = x[\"prompt\"], \n",
    "                                                                   label = x[\"scores\"]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of the data using BERT:\n",
    "\n",
    "    Lowercase text (if we're using a BERT lowercase model)\n",
    "    Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "    Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "    Map our words to indexes using a vocab file that BERT provides\n",
    "    Add special \"CLS\" and \"SEP\" tokens (see the readme)\n",
    "    Append \"index\" and \"segment\" tokens to each input (see the BERT paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:26.937438 139931951523648 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# This is a path to an uncased (all lowercase) version of BERT\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "    return bert.tokenization.FullTokenizer(\n",
    "        vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "# Instantiating a BERT Tokenizer\n",
    "bert_tknzr = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 9732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.576540 139931951523648 tf_logging.py:115] Writing example 0 of 9732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.603172 139931951523648 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.609256 139931951523648 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] the mood in the memoir that the author creates is a very warm and relaxed sort of mood . the way the author tells you about his parents ; the self ##lessness and the way they put their kids first before themselves really just makes you think about your own parents and how good they are to you and what they would do for you . the way the author talks about his home makes you think of all the colors in the house . i think that the house would be a golden – red color yellow , white , brown and orange on the insides . it reminds me of the way they would have cooked . they would have cooked with colorful peppers , onions , tor ##till ##as and oils . it makes you think about what colors you have in your home and what you would cook with . the music sounds wonderful . even though they don ' t demonstrate what it actually sounds like , you can easily imagine it . i imagine it laying in the background while all the adults are cooking and the kids are playing in colorful rooms with the golden sunlight shining in through an open window . i love the mood that this memoir puts me in . [SEP] na ##rc ##iso rodriguez from home : the blue ##print ##s of our lives my parents , originally from cuba , arrived in the united states in 1956 . after living for a year in a furnished one - room apartment , twenty - one - year - old raw ##ed ##ia maria and twenty - seven - year - old na ##rc ##iso rodriguez , sr . , could afford to move into a modest , three - room apartment i would soon call home . in 1961 , i was born into this simple house , situated in a two - family , blond - brick building in the iron ##bound section of newark , new jersey . within its walls , my young parents created our traditional cuban home , the very heart of which was the kitchen . my parents both shared cooking duties and un ##wi ##tting ##ly passed on to me their rich culinary skills and a love of cooking that is still with me today ( and for which i am eternal ##ly grateful ) . passionate cuban music ( which i ad ##ore to this day ) filled the air , mixing with the aroma ##s of the kitchen . here , the innocence of childhood , the congregation of family and friends , and endless celebrations that encompassed both , formed the backdrop to life in our warm home . growing up in this environment ins ##till ##ed in me a great sense that “ family ” had nothing to do with being a blood relative . quite the contrary , our neighborhood was made up of mostly spanish , cuban , and italian immigrants at a time when over ##t racism was the norm and segregation prevailed in the united states . in our neighborhood , despite customs elsewhere , all of these cultures came together in great solidarity and friendship . it was a close - knit community of honest , hard ##working immigrants who extended a hand to people who , while not necessarily their own kind , were clearly in need . our landlord and his daughter , ale ##gr ##ia ( my baby ##sit ##ter and first friend ) , lived above us , and ale ##gr ##ia grace ##d our kitchen table for meals more often than not . also at the table were sergio and ed ##el ##mir ##a , my sur ##rogate grandparents who lived in the basement apartment . ( i would not know my “ real ” [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.612725 139931951523648 tf_logging.py:115] tokens: [CLS] the mood in the memoir that the author creates is a very warm and relaxed sort of mood . the way the author tells you about his parents ; the self ##lessness and the way they put their kids first before themselves really just makes you think about your own parents and how good they are to you and what they would do for you . the way the author talks about his home makes you think of all the colors in the house . i think that the house would be a golden – red color yellow , white , brown and orange on the insides . it reminds me of the way they would have cooked . they would have cooked with colorful peppers , onions , tor ##till ##as and oils . it makes you think about what colors you have in your home and what you would cook with . the music sounds wonderful . even though they don ' t demonstrate what it actually sounds like , you can easily imagine it . i imagine it laying in the background while all the adults are cooking and the kids are playing in colorful rooms with the golden sunlight shining in through an open window . i love the mood that this memoir puts me in . [SEP] na ##rc ##iso rodriguez from home : the blue ##print ##s of our lives my parents , originally from cuba , arrived in the united states in 1956 . after living for a year in a furnished one - room apartment , twenty - one - year - old raw ##ed ##ia maria and twenty - seven - year - old na ##rc ##iso rodriguez , sr . , could afford to move into a modest , three - room apartment i would soon call home . in 1961 , i was born into this simple house , situated in a two - family , blond - brick building in the iron ##bound section of newark , new jersey . within its walls , my young parents created our traditional cuban home , the very heart of which was the kitchen . my parents both shared cooking duties and un ##wi ##tting ##ly passed on to me their rich culinary skills and a love of cooking that is still with me today ( and for which i am eternal ##ly grateful ) . passionate cuban music ( which i ad ##ore to this day ) filled the air , mixing with the aroma ##s of the kitchen . here , the innocence of childhood , the congregation of family and friends , and endless celebrations that encompassed both , formed the backdrop to life in our warm home . growing up in this environment ins ##till ##ed in me a great sense that “ family ” had nothing to do with being a blood relative . quite the contrary , our neighborhood was made up of mostly spanish , cuban , and italian immigrants at a time when over ##t racism was the norm and segregation prevailed in the united states . in our neighborhood , despite customs elsewhere , all of these cultures came together in great solidarity and friendship . it was a close - knit community of honest , hard ##working immigrants who extended a hand to people who , while not necessarily their own kind , were clearly in need . our landlord and his daughter , ale ##gr ##ia ( my baby ##sit ##ter and first friend ) , lived above us , and ale ##gr ##ia grace ##d our kitchen table for meals more often than not . also at the table were sergio and ed ##el ##mir ##a , my sur ##rogate grandparents who lived in the basement apartment . ( i would not know my “ real ” [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1996 6888 1999 1996 12558 2008 1996 3166 9005 2003 1037 2200 4010 1998 8363 4066 1997 6888 1012 1996 2126 1996 3166 4136 2017 2055 2010 3008 1025 1996 2969 24913 1998 1996 2126 2027 2404 2037 4268 2034 2077 3209 2428 2074 3084 2017 2228 2055 2115 2219 3008 1998 2129 2204 2027 2024 2000 2017 1998 2054 2027 2052 2079 2005 2017 1012 1996 2126 1996 3166 7566 2055 2010 2188 3084 2017 2228 1997 2035 1996 6087 1999 1996 2160 1012 1045 2228 2008 1996 2160 2052 2022 1037 3585 1516 2417 3609 3756 1010 2317 1010 2829 1998 4589 2006 1996 19008 1012 2009 15537 2033 1997 1996 2126 2027 2052 2031 12984 1012 2027 2052 2031 12984 2007 14231 23582 1010 24444 1010 17153 28345 3022 1998 20631 1012 2009 3084 2017 2228 2055 2054 6087 2017 2031 1999 2115 2188 1998 2054 2017 2052 5660 2007 1012 1996 2189 4165 6919 1012 2130 2295 2027 2123 1005 1056 10580 2054 2009 2941 4165 2066 1010 2017 2064 4089 5674 2009 1012 1045 5674 2009 10201 1999 1996 4281 2096 2035 1996 6001 2024 8434 1998 1996 4268 2024 2652 1999 14231 4734 2007 1996 3585 9325 9716 1999 2083 2019 2330 3332 1012 1045 2293 1996 6888 2008 2023 12558 8509 2033 1999 1012 102 6583 11890 19565 9172 2013 2188 1024 1996 2630 16550 2015 1997 2256 3268 2026 3008 1010 2761 2013 7394 1010 3369 1999 1996 2142 2163 1999 3838 1012 2044 2542 2005 1037 2095 1999 1037 19851 2028 1011 2282 4545 1010 3174 1011 2028 1011 2095 1011 2214 6315 2098 2401 3814 1998 3174 1011 2698 1011 2095 1011 2214 6583 11890 19565 9172 1010 5034 1012 1010 2071 8984 2000 2693 2046 1037 10754 1010 2093 1011 2282 4545 1045 2052 2574 2655 2188 1012 1999 3777 1010 1045 2001 2141 2046 2023 3722 2160 1010 4350 1999 1037 2048 1011 2155 1010 8855 1011 5318 2311 1999 1996 3707 15494 2930 1997 12948 1010 2047 3933 1012 2306 2049 3681 1010 2026 2402 3008 2580 2256 3151 9642 2188 1010 1996 2200 2540 1997 2029 2001 1996 3829 1012 2026 3008 2119 4207 8434 5704 1998 4895 9148 13027 2135 2979 2006 2000 2033 2037 4138 20560 4813 1998 1037 2293 1997 8434 2008 2003 2145 2007 2033 2651 1006 1998 2005 2029 1045 2572 10721 2135 8794 1007 1012 13459 9642 2189 1006 2029 1045 4748 5686 2000 2023 2154 1007 3561 1996 2250 1010 6809 2007 1996 23958 2015 1997 1996 3829 1012 2182 1010 1996 12660 1997 5593 1010 1996 7769 1997 2155 1998 2814 1010 1998 10866 12035 2008 24058 2119 1010 2719 1996 18876 2000 2166 1999 2256 4010 2188 1012 3652 2039 1999 2023 4044 16021 28345 2098 1999 2033 1037 2307 3168 2008 1523 2155 1524 2018 2498 2000 2079 2007 2108 1037 2668 5816 1012 3243 1996 10043 1010 2256 5101 2001 2081 2039 1997 3262 3009 1010 9642 1010 1998 3059 7489 2012 1037 2051 2043 2058 2102 14398 2001 1996 13373 1998 18771 19914 1999 1996 2142 2163 1012 1999 2256 5101 1010 2750 8205 6974 1010 2035 1997 2122 8578 2234 2362 1999 2307 14657 1998 6860 1012 2009 2001 1037 2485 1011 22404 2451 1997 7481 1010 2524 21398 7489 2040 3668 1037 2192 2000 2111 2040 1010 2096 2025 9352 2037 2219 2785 1010 2020 4415 1999 2342 1012 2256 18196 1998 2010 2684 1010 15669 16523 2401 1006 2026 3336 28032 3334 1998 2034 2767 1007 1010 2973 2682 2149 1010 1998 15669 16523 2401 4519 2094 2256 3829 2795 2005 12278 2062 2411 2084 2025 1012 2036 2012 1996 2795 2020 13983 1998 3968 2884 14503 2050 1010 2026 7505 21799 14472 2040 2973 1999 1996 8102 4545 1012 1006 1045 2052 2025 2113 2026 1523 2613 1524 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.614553 139931951523648 tf_logging.py:115] input_ids: 101 1996 6888 1999 1996 12558 2008 1996 3166 9005 2003 1037 2200 4010 1998 8363 4066 1997 6888 1012 1996 2126 1996 3166 4136 2017 2055 2010 3008 1025 1996 2969 24913 1998 1996 2126 2027 2404 2037 4268 2034 2077 3209 2428 2074 3084 2017 2228 2055 2115 2219 3008 1998 2129 2204 2027 2024 2000 2017 1998 2054 2027 2052 2079 2005 2017 1012 1996 2126 1996 3166 7566 2055 2010 2188 3084 2017 2228 1997 2035 1996 6087 1999 1996 2160 1012 1045 2228 2008 1996 2160 2052 2022 1037 3585 1516 2417 3609 3756 1010 2317 1010 2829 1998 4589 2006 1996 19008 1012 2009 15537 2033 1997 1996 2126 2027 2052 2031 12984 1012 2027 2052 2031 12984 2007 14231 23582 1010 24444 1010 17153 28345 3022 1998 20631 1012 2009 3084 2017 2228 2055 2054 6087 2017 2031 1999 2115 2188 1998 2054 2017 2052 5660 2007 1012 1996 2189 4165 6919 1012 2130 2295 2027 2123 1005 1056 10580 2054 2009 2941 4165 2066 1010 2017 2064 4089 5674 2009 1012 1045 5674 2009 10201 1999 1996 4281 2096 2035 1996 6001 2024 8434 1998 1996 4268 2024 2652 1999 14231 4734 2007 1996 3585 9325 9716 1999 2083 2019 2330 3332 1012 1045 2293 1996 6888 2008 2023 12558 8509 2033 1999 1012 102 6583 11890 19565 9172 2013 2188 1024 1996 2630 16550 2015 1997 2256 3268 2026 3008 1010 2761 2013 7394 1010 3369 1999 1996 2142 2163 1999 3838 1012 2044 2542 2005 1037 2095 1999 1037 19851 2028 1011 2282 4545 1010 3174 1011 2028 1011 2095 1011 2214 6315 2098 2401 3814 1998 3174 1011 2698 1011 2095 1011 2214 6583 11890 19565 9172 1010 5034 1012 1010 2071 8984 2000 2693 2046 1037 10754 1010 2093 1011 2282 4545 1045 2052 2574 2655 2188 1012 1999 3777 1010 1045 2001 2141 2046 2023 3722 2160 1010 4350 1999 1037 2048 1011 2155 1010 8855 1011 5318 2311 1999 1996 3707 15494 2930 1997 12948 1010 2047 3933 1012 2306 2049 3681 1010 2026 2402 3008 2580 2256 3151 9642 2188 1010 1996 2200 2540 1997 2029 2001 1996 3829 1012 2026 3008 2119 4207 8434 5704 1998 4895 9148 13027 2135 2979 2006 2000 2033 2037 4138 20560 4813 1998 1037 2293 1997 8434 2008 2003 2145 2007 2033 2651 1006 1998 2005 2029 1045 2572 10721 2135 8794 1007 1012 13459 9642 2189 1006 2029 1045 4748 5686 2000 2023 2154 1007 3561 1996 2250 1010 6809 2007 1996 23958 2015 1997 1996 3829 1012 2182 1010 1996 12660 1997 5593 1010 1996 7769 1997 2155 1998 2814 1010 1998 10866 12035 2008 24058 2119 1010 2719 1996 18876 2000 2166 1999 2256 4010 2188 1012 3652 2039 1999 2023 4044 16021 28345 2098 1999 2033 1037 2307 3168 2008 1523 2155 1524 2018 2498 2000 2079 2007 2108 1037 2668 5816 1012 3243 1996 10043 1010 2256 5101 2001 2081 2039 1997 3262 3009 1010 9642 1010 1998 3059 7489 2012 1037 2051 2043 2058 2102 14398 2001 1996 13373 1998 18771 19914 1999 1996 2142 2163 1012 1999 2256 5101 1010 2750 8205 6974 1010 2035 1997 2122 8578 2234 2362 1999 2307 14657 1998 6860 1012 2009 2001 1037 2485 1011 22404 2451 1997 7481 1010 2524 21398 7489 2040 3668 1037 2192 2000 2111 2040 1010 2096 2025 9352 2037 2219 2785 1010 2020 4415 1999 2342 1012 2256 18196 1998 2010 2684 1010 15669 16523 2401 1006 2026 3336 28032 3334 1998 2034 2767 1007 1010 2973 2682 2149 1010 1998 15669 16523 2401 4519 2094 2256 3829 2795 2005 12278 2062 2411 2084 2025 1012 2036 2012 1996 2795 2020 13983 1998 3968 2884 14503 2050 1010 2026 7505 21799 14472 2040 2973 1999 1996 8102 4545 1012 1006 1045 2052 2025 2113 2026 1523 2613 1524 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.617151 139931951523648 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.618473 139931951523648 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 2 (id = 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.621339 139931951523648 tf_logging.py:115] label: 2 (id = 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.638345 139931951523648 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.641842 139931951523648 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] i think that all books that are found offensive or sexual shouldn ' t be taken off the shelves completely . i just think that they should be placed in a certain section of the library where kids should not be at or not allowed to go in like a section only for adults . i think if a ki ##s see something they think is interest ##ion ##g and will teach them something they didn ##t know before they ' re going to pick it up no matter what unless someone steps in and tells them not to . i don ##t think kids should be exposed to all of that and some kids are getting ahead were they shouldn ' t be in life . if a kid decides to try and check out a book like that either they shall have an i . d . or there parent should be with them com ##firm ##ing they are able to check out that book . many children i think are already exposed to things they shouldn ' t be around and books like that would just make it all worse . some books send out really bad messages but then again that ' s only for adults to read it should be kept out of the way of children . some kids could live in a fan ##as ##y world and have a split personality or something off of things like that and it could really affect someone badly . if a parent wants there kid to have the book or don ##t mind anything about it then that ' s there choice they have permission . i know i wouldn ##t want my kids around that until ##l they were @ nu ##m ##1 years of age or older but i know some parents don ##t really care al ##ot about it . so my best solution for this problem is to keep those books and things out of the reach of children or have a specific age limit . [SEP] censorship in the libraries . \" all of us can think of a book that we hope none of our children or any other children have taken off the shelf . but if i have the right to remove that book from the shelf - - that work i ab ##hor - - then you also have exactly the same right and so does everyone else . and then we have no books left on the shelf for any of us . \" - - katherine paterson , author . write a per ##su ##asi ##ve essay to a newspaper reflecting your views on censorship in libraries . do you believe that certain materials , such as books , music , movies , magazines , etc . , should be removed from the shelves if they are found offensive ? support your position with convincing arguments from your own experience , observations , and / or reading . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.645074 139931951523648 tf_logging.py:115] tokens: [CLS] i think that all books that are found offensive or sexual shouldn ' t be taken off the shelves completely . i just think that they should be placed in a certain section of the library where kids should not be at or not allowed to go in like a section only for adults . i think if a ki ##s see something they think is interest ##ion ##g and will teach them something they didn ##t know before they ' re going to pick it up no matter what unless someone steps in and tells them not to . i don ##t think kids should be exposed to all of that and some kids are getting ahead were they shouldn ' t be in life . if a kid decides to try and check out a book like that either they shall have an i . d . or there parent should be with them com ##firm ##ing they are able to check out that book . many children i think are already exposed to things they shouldn ' t be around and books like that would just make it all worse . some books send out really bad messages but then again that ' s only for adults to read it should be kept out of the way of children . some kids could live in a fan ##as ##y world and have a split personality or something off of things like that and it could really affect someone badly . if a parent wants there kid to have the book or don ##t mind anything about it then that ' s there choice they have permission . i know i wouldn ##t want my kids around that until ##l they were @ nu ##m ##1 years of age or older but i know some parents don ##t really care al ##ot about it . so my best solution for this problem is to keep those books and things out of the reach of children or have a specific age limit . [SEP] censorship in the libraries . \" all of us can think of a book that we hope none of our children or any other children have taken off the shelf . but if i have the right to remove that book from the shelf - - that work i ab ##hor - - then you also have exactly the same right and so does everyone else . and then we have no books left on the shelf for any of us . \" - - katherine paterson , author . write a per ##su ##asi ##ve essay to a newspaper reflecting your views on censorship in libraries . do you believe that certain materials , such as books , music , movies , magazines , etc . , should be removed from the shelves if they are found offensive ? support your position with convincing arguments from your own experience , observations , and / or reading . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 2228 2008 2035 2808 2008 2024 2179 5805 2030 4424 5807 1005 1056 2022 2579 2125 1996 15475 3294 1012 1045 2074 2228 2008 2027 2323 2022 2872 1999 1037 3056 2930 1997 1996 3075 2073 4268 2323 2025 2022 2012 2030 2025 3039 2000 2175 1999 2066 1037 2930 2069 2005 6001 1012 1045 2228 2065 1037 11382 2015 2156 2242 2027 2228 2003 3037 3258 2290 1998 2097 6570 2068 2242 2027 2134 2102 2113 2077 2027 1005 2128 2183 2000 4060 2009 2039 2053 3043 2054 4983 2619 4084 1999 1998 4136 2068 2025 2000 1012 1045 2123 2102 2228 4268 2323 2022 6086 2000 2035 1997 2008 1998 2070 4268 2024 2893 3805 2020 2027 5807 1005 1056 2022 1999 2166 1012 2065 1037 4845 7288 2000 3046 1998 4638 2041 1037 2338 2066 2008 2593 2027 4618 2031 2019 1045 1012 1040 1012 2030 2045 6687 2323 2022 2007 2068 4012 27972 2075 2027 2024 2583 2000 4638 2041 2008 2338 1012 2116 2336 1045 2228 2024 2525 6086 2000 2477 2027 5807 1005 1056 2022 2105 1998 2808 2066 2008 2052 2074 2191 2009 2035 4788 1012 2070 2808 4604 2041 2428 2919 7696 2021 2059 2153 2008 1005 1055 2069 2005 6001 2000 3191 2009 2323 2022 2921 2041 1997 1996 2126 1997 2336 1012 2070 4268 2071 2444 1999 1037 5470 3022 2100 2088 1998 2031 1037 3975 6180 2030 2242 2125 1997 2477 2066 2008 1998 2009 2071 2428 7461 2619 6649 1012 2065 1037 6687 4122 2045 4845 2000 2031 1996 2338 2030 2123 2102 2568 2505 2055 2009 2059 2008 1005 1055 2045 3601 2027 2031 6656 1012 1045 2113 1045 2876 2102 2215 2026 4268 2105 2008 2127 2140 2027 2020 1030 16371 2213 2487 2086 1997 2287 2030 3080 2021 1045 2113 2070 3008 2123 2102 2428 2729 2632 4140 2055 2009 1012 2061 2026 2190 5576 2005 2023 3291 2003 2000 2562 2216 2808 1998 2477 2041 1997 1996 3362 1997 2336 2030 2031 1037 3563 2287 5787 1012 102 15657 1999 1996 8860 1012 1000 2035 1997 2149 2064 2228 1997 1037 2338 2008 2057 3246 3904 1997 2256 2336 2030 2151 2060 2336 2031 2579 2125 1996 11142 1012 2021 2065 1045 2031 1996 2157 2000 6366 2008 2338 2013 1996 11142 1011 1011 2008 2147 1045 11113 16368 1011 1011 2059 2017 2036 2031 3599 1996 2168 2157 1998 2061 2515 3071 2842 1012 1998 2059 2057 2031 2053 2808 2187 2006 1996 11142 2005 2151 1997 2149 1012 1000 1011 1011 9477 19162 1010 3166 1012 4339 1037 2566 6342 21369 3726 9491 2000 1037 3780 10842 2115 5328 2006 15657 1999 8860 1012 2079 2017 2903 2008 3056 4475 1010 2107 2004 2808 1010 2189 1010 5691 1010 7298 1010 4385 1012 1010 2323 2022 3718 2013 1996 15475 2065 2027 2024 2179 5805 1029 2490 2115 2597 2007 13359 9918 2013 2115 2219 3325 1010 9420 1010 1998 1013 2030 3752 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.646921 139931951523648 tf_logging.py:115] input_ids: 101 1045 2228 2008 2035 2808 2008 2024 2179 5805 2030 4424 5807 1005 1056 2022 2579 2125 1996 15475 3294 1012 1045 2074 2228 2008 2027 2323 2022 2872 1999 1037 3056 2930 1997 1996 3075 2073 4268 2323 2025 2022 2012 2030 2025 3039 2000 2175 1999 2066 1037 2930 2069 2005 6001 1012 1045 2228 2065 1037 11382 2015 2156 2242 2027 2228 2003 3037 3258 2290 1998 2097 6570 2068 2242 2027 2134 2102 2113 2077 2027 1005 2128 2183 2000 4060 2009 2039 2053 3043 2054 4983 2619 4084 1999 1998 4136 2068 2025 2000 1012 1045 2123 2102 2228 4268 2323 2022 6086 2000 2035 1997 2008 1998 2070 4268 2024 2893 3805 2020 2027 5807 1005 1056 2022 1999 2166 1012 2065 1037 4845 7288 2000 3046 1998 4638 2041 1037 2338 2066 2008 2593 2027 4618 2031 2019 1045 1012 1040 1012 2030 2045 6687 2323 2022 2007 2068 4012 27972 2075 2027 2024 2583 2000 4638 2041 2008 2338 1012 2116 2336 1045 2228 2024 2525 6086 2000 2477 2027 5807 1005 1056 2022 2105 1998 2808 2066 2008 2052 2074 2191 2009 2035 4788 1012 2070 2808 4604 2041 2428 2919 7696 2021 2059 2153 2008 1005 1055 2069 2005 6001 2000 3191 2009 2323 2022 2921 2041 1997 1996 2126 1997 2336 1012 2070 4268 2071 2444 1999 1037 5470 3022 2100 2088 1998 2031 1037 3975 6180 2030 2242 2125 1997 2477 2066 2008 1998 2009 2071 2428 7461 2619 6649 1012 2065 1037 6687 4122 2045 4845 2000 2031 1996 2338 2030 2123 2102 2568 2505 2055 2009 2059 2008 1005 1055 2045 3601 2027 2031 6656 1012 1045 2113 1045 2876 2102 2215 2026 4268 2105 2008 2127 2140 2027 2020 1030 16371 2213 2487 2086 1997 2287 2030 3080 2021 1045 2113 2070 3008 2123 2102 2428 2729 2632 4140 2055 2009 1012 2061 2026 2190 5576 2005 2023 3291 2003 2000 2562 2216 2808 1998 2477 2041 1997 1996 3362 1997 2336 2030 2031 1037 3563 2287 5787 1012 102 15657 1999 1996 8860 1012 1000 2035 1997 2149 2064 2228 1997 1037 2338 2008 2057 3246 3904 1997 2256 2336 2030 2151 2060 2336 2031 2579 2125 1996 11142 1012 2021 2065 1045 2031 1996 2157 2000 6366 2008 2338 2013 1996 11142 1011 1011 2008 2147 1045 11113 16368 1011 1011 2059 2017 2036 2031 3599 1996 2168 2157 1998 2061 2515 3071 2842 1012 1998 2059 2057 2031 2053 2808 2187 2006 1996 11142 2005 2151 1997 2149 1012 1000 1011 1011 9477 19162 1010 3166 1012 4339 1037 2566 6342 21369 3726 9491 2000 1037 3780 10842 2115 5328 2006 15657 1999 8860 1012 2079 2017 2903 2008 3056 4475 1010 2107 2004 2808 1010 2189 1010 5691 1010 7298 1010 4385 1012 1010 2323 2022 3718 2013 1996 15475 2065 2027 2024 2179 5805 1029 2490 2115 2597 2007 13359 9918 2013 2115 2219 3325 1010 9420 1010 1998 1013 2030 3752 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.648752 139931951523648 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.651908 139931951523648 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 5 (id = 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.653134 139931951523648 tf_logging.py:115] label: 5 (id = 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.707928 139931951523648 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.713320 139931951523648 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] the author added that sentence to the story to show how much sa ##eng cared about the snow and the plants . she says that she much rather do gardening then school work . she loves planting new things . [SEP] winter hi ##bis ##cus by min ##fo ##ng ho sa ##eng , a teenage girl , and her family have moved to the united states from vietnam . as sa ##eng walks home after failing her driver ’ s test , she sees a familiar plant . later , she goes to a fl ##oris ##t shop to see if the plant can be purchased . it was like walking into another world . a hot , moist world exploding with greene ##ry . huge flat leaves , delicate wi ##sp ##s of tend ##ril ##s , ferns and fr ##ond ##s and vines of all shades and shapes grew in seemingly random prof ##usion . “ over there , in the corner , the hi ##bis ##cus . is that what you mean ? ” the fl ##oris ##t pointed at a leaf ##y pot ##ted plant by the corner . there , in a shaft of the wan afternoon sunlight , was a single blood - red blossom , its five petals sp ##lay ##ed back to reveal a long st ##amen tipped with yellow pollen . sa ##eng felt a shock of recognition so intense , it was almost vis ##cera ##l . “ sa ##eb ##ba , ” sa ##eng whispered . a sa ##eb ##ba hedge , tall and lush , had surrounded their garden , its lush green leaves dotted with ve ##rmi ##lion flowers . and sometimes after a monsoon rain , a blossom or two would have blown into the well , so that when she drew the well water , she would find a red blossom floating in the bucket . slowly , sa ##eng walked down the narrow aisle toward the hi ##bis ##cus . orchid ##s , lan ##na bushes , ole ##ander ##s , elephant ear beg ##onia ##s , and bo ##uga ##in ##ville ##a vines surrounded her . plants that she had not even realized she had known but had forgotten drew her back into her childhood world . when she got to the hi ##bis ##cus , she reached out and touched a pet ##al gently . it felt smooth and cool , with a hint of velvet toward the center — just as she had known it would feel . and beside it was yet another old friend , a small shrub with wax ##y leaves and dai ##nty flowers with pu ##rp ##lish petals and white centers . “ madagascar per ##i ##win ##kle , ” its tag announced . how strange to see it in a pot , sa ##eng thought . back home it just grew wild , ju ##tting out from the cracks in brick walls or between tiled roofs . and that rich , sweet scent — that was familiar , too . sa ##eng scanned the greene ##ry around her and found a tall , gang ##ly plant with exquisite little white blossoms on it . “ do ##k malik , ” she said , sa ##vor ##ing the feel of the word on her tongue , even as she silently noted the english name on its tag , “ jasmine . ” one of the blossoms had fallen off , and carefully sa ##eng picked it up and smelled it . she closed her eyes and breathed in , deeply . the familiar fragrance filled her lungs , and sa ##eng could almost feel the light strands of her grandmother ’ s long gray hair , freshly washed , as she comb ##ed it out with the fine - tooth ##ed buffalo - horn [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.716469 139931951523648 tf_logging.py:115] tokens: [CLS] the author added that sentence to the story to show how much sa ##eng cared about the snow and the plants . she says that she much rather do gardening then school work . she loves planting new things . [SEP] winter hi ##bis ##cus by min ##fo ##ng ho sa ##eng , a teenage girl , and her family have moved to the united states from vietnam . as sa ##eng walks home after failing her driver ’ s test , she sees a familiar plant . later , she goes to a fl ##oris ##t shop to see if the plant can be purchased . it was like walking into another world . a hot , moist world exploding with greene ##ry . huge flat leaves , delicate wi ##sp ##s of tend ##ril ##s , ferns and fr ##ond ##s and vines of all shades and shapes grew in seemingly random prof ##usion . “ over there , in the corner , the hi ##bis ##cus . is that what you mean ? ” the fl ##oris ##t pointed at a leaf ##y pot ##ted plant by the corner . there , in a shaft of the wan afternoon sunlight , was a single blood - red blossom , its five petals sp ##lay ##ed back to reveal a long st ##amen tipped with yellow pollen . sa ##eng felt a shock of recognition so intense , it was almost vis ##cera ##l . “ sa ##eb ##ba , ” sa ##eng whispered . a sa ##eb ##ba hedge , tall and lush , had surrounded their garden , its lush green leaves dotted with ve ##rmi ##lion flowers . and sometimes after a monsoon rain , a blossom or two would have blown into the well , so that when she drew the well water , she would find a red blossom floating in the bucket . slowly , sa ##eng walked down the narrow aisle toward the hi ##bis ##cus . orchid ##s , lan ##na bushes , ole ##ander ##s , elephant ear beg ##onia ##s , and bo ##uga ##in ##ville ##a vines surrounded her . plants that she had not even realized she had known but had forgotten drew her back into her childhood world . when she got to the hi ##bis ##cus , she reached out and touched a pet ##al gently . it felt smooth and cool , with a hint of velvet toward the center — just as she had known it would feel . and beside it was yet another old friend , a small shrub with wax ##y leaves and dai ##nty flowers with pu ##rp ##lish petals and white centers . “ madagascar per ##i ##win ##kle , ” its tag announced . how strange to see it in a pot , sa ##eng thought . back home it just grew wild , ju ##tting out from the cracks in brick walls or between tiled roofs . and that rich , sweet scent — that was familiar , too . sa ##eng scanned the greene ##ry around her and found a tall , gang ##ly plant with exquisite little white blossoms on it . “ do ##k malik , ” she said , sa ##vor ##ing the feel of the word on her tongue , even as she silently noted the english name on its tag , “ jasmine . ” one of the blossoms had fallen off , and carefully sa ##eng picked it up and smelled it . she closed her eyes and breathed in , deeply . the familiar fragrance filled her lungs , and sa ##eng could almost feel the light strands of her grandmother ’ s long gray hair , freshly washed , as she comb ##ed it out with the fine - tooth ##ed buffalo - horn [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1996 3166 2794 2008 6251 2000 1996 2466 2000 2265 2129 2172 7842 13159 8725 2055 1996 4586 1998 1996 4264 1012 2016 2758 2008 2016 2172 2738 2079 21529 2059 2082 2147 1012 2016 7459 14685 2047 2477 1012 102 3467 7632 18477 7874 2011 8117 14876 3070 7570 7842 13159 1010 1037 9454 2611 1010 1998 2014 2155 2031 2333 2000 1996 2142 2163 2013 5148 1012 2004 7842 13159 7365 2188 2044 7989 2014 4062 1521 1055 3231 1010 2016 5927 1037 5220 3269 1012 2101 1010 2016 3632 2000 1037 13109 21239 2102 4497 2000 2156 2065 1996 3269 2064 2022 4156 1012 2009 2001 2066 3788 2046 2178 2088 1012 1037 2980 1010 11052 2088 20728 2007 11006 2854 1012 4121 4257 3727 1010 10059 15536 13102 2015 1997 7166 15928 2015 1010 25715 1998 10424 15422 2015 1998 16702 1997 2035 13178 1998 10466 3473 1999 9428 6721 11268 14499 1012 1523 2058 2045 1010 1999 1996 3420 1010 1996 7632 18477 7874 1012 2003 2008 2054 2017 2812 1029 1524 1996 13109 21239 2102 4197 2012 1037 7053 2100 8962 3064 3269 2011 1996 3420 1012 2045 1010 1999 1037 9093 1997 1996 14071 5027 9325 1010 2001 1037 2309 2668 1011 2417 20593 1010 2049 2274 15829 11867 8485 2098 2067 2000 7487 1037 2146 2358 27245 11182 2007 3756 22482 1012 7842 13159 2371 1037 5213 1997 5038 2061 6387 1010 2009 2001 2471 25292 19357 2140 1012 1523 7842 15878 3676 1010 1524 7842 13159 3990 1012 1037 7842 15878 3676 17834 1010 4206 1998 16299 1010 2018 5129 2037 3871 1010 2049 16299 2665 3727 20384 2007 2310 28550 18964 4870 1012 1998 2823 2044 1037 19183 4542 1010 1037 20593 2030 2048 2052 2031 10676 2046 1996 2092 1010 2061 2008 2043 2016 3881 1996 2092 2300 1010 2016 2052 2424 1037 2417 20593 8274 1999 1996 13610 1012 3254 1010 7842 13159 2939 2091 1996 4867 12485 2646 1996 7632 18477 7874 1012 15573 2015 1010 17595 2532 14568 1010 15589 12243 2015 1010 10777 4540 11693 12488 2015 1010 1998 8945 16377 2378 3077 2050 16702 5129 2014 1012 4264 2008 2016 2018 2025 2130 3651 2016 2018 2124 2021 2018 6404 3881 2014 2067 2046 2014 5593 2088 1012 2043 2016 2288 2000 1996 7632 18477 7874 1010 2016 2584 2041 1998 5028 1037 9004 2389 5251 1012 2009 2371 5744 1998 4658 1010 2007 1037 9374 1997 10966 2646 1996 2415 1517 2074 2004 2016 2018 2124 2009 2052 2514 1012 1998 3875 2009 2001 2664 2178 2214 2767 1010 1037 2235 15751 2007 13844 2100 3727 1998 18765 29405 4870 2007 16405 14536 13602 15829 1998 2317 6401 1012 1523 11934 2566 2072 10105 19099 1010 1524 2049 6415 2623 1012 2129 4326 2000 2156 2009 1999 1037 8962 1010 7842 13159 2245 1012 2067 2188 2009 2074 3473 3748 1010 18414 13027 2041 2013 1996 15288 1999 5318 3681 2030 2090 26510 15753 1012 1998 2008 4138 1010 4086 6518 1517 2008 2001 5220 1010 2205 1012 7842 13159 11728 1996 11006 2854 2105 2014 1998 2179 1037 4206 1010 6080 2135 3269 2007 19401 2210 2317 28766 2006 2009 1012 1523 2079 2243 14360 1010 1524 2016 2056 1010 7842 14550 2075 1996 2514 1997 1996 2773 2006 2014 4416 1010 2130 2004 2016 8601 3264 1996 2394 2171 2006 2049 6415 1010 1523 14032 1012 1524 2028 1997 1996 28766 2018 5357 2125 1010 1998 5362 7842 13159 3856 2009 2039 1998 9557 2009 1012 2016 2701 2014 2159 1998 8726 1999 1010 6171 1012 1996 5220 24980 3561 2014 8948 1010 1998 7842 13159 2071 2471 2514 1996 2422 14119 1997 2014 7133 1521 1055 2146 3897 2606 1010 20229 8871 1010 2004 2016 22863 2098 2009 2041 2007 1996 2986 1011 11868 2098 6901 1011 7109 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.719234 139931951523648 tf_logging.py:115] input_ids: 101 1996 3166 2794 2008 6251 2000 1996 2466 2000 2265 2129 2172 7842 13159 8725 2055 1996 4586 1998 1996 4264 1012 2016 2758 2008 2016 2172 2738 2079 21529 2059 2082 2147 1012 2016 7459 14685 2047 2477 1012 102 3467 7632 18477 7874 2011 8117 14876 3070 7570 7842 13159 1010 1037 9454 2611 1010 1998 2014 2155 2031 2333 2000 1996 2142 2163 2013 5148 1012 2004 7842 13159 7365 2188 2044 7989 2014 4062 1521 1055 3231 1010 2016 5927 1037 5220 3269 1012 2101 1010 2016 3632 2000 1037 13109 21239 2102 4497 2000 2156 2065 1996 3269 2064 2022 4156 1012 2009 2001 2066 3788 2046 2178 2088 1012 1037 2980 1010 11052 2088 20728 2007 11006 2854 1012 4121 4257 3727 1010 10059 15536 13102 2015 1997 7166 15928 2015 1010 25715 1998 10424 15422 2015 1998 16702 1997 2035 13178 1998 10466 3473 1999 9428 6721 11268 14499 1012 1523 2058 2045 1010 1999 1996 3420 1010 1996 7632 18477 7874 1012 2003 2008 2054 2017 2812 1029 1524 1996 13109 21239 2102 4197 2012 1037 7053 2100 8962 3064 3269 2011 1996 3420 1012 2045 1010 1999 1037 9093 1997 1996 14071 5027 9325 1010 2001 1037 2309 2668 1011 2417 20593 1010 2049 2274 15829 11867 8485 2098 2067 2000 7487 1037 2146 2358 27245 11182 2007 3756 22482 1012 7842 13159 2371 1037 5213 1997 5038 2061 6387 1010 2009 2001 2471 25292 19357 2140 1012 1523 7842 15878 3676 1010 1524 7842 13159 3990 1012 1037 7842 15878 3676 17834 1010 4206 1998 16299 1010 2018 5129 2037 3871 1010 2049 16299 2665 3727 20384 2007 2310 28550 18964 4870 1012 1998 2823 2044 1037 19183 4542 1010 1037 20593 2030 2048 2052 2031 10676 2046 1996 2092 1010 2061 2008 2043 2016 3881 1996 2092 2300 1010 2016 2052 2424 1037 2417 20593 8274 1999 1996 13610 1012 3254 1010 7842 13159 2939 2091 1996 4867 12485 2646 1996 7632 18477 7874 1012 15573 2015 1010 17595 2532 14568 1010 15589 12243 2015 1010 10777 4540 11693 12488 2015 1010 1998 8945 16377 2378 3077 2050 16702 5129 2014 1012 4264 2008 2016 2018 2025 2130 3651 2016 2018 2124 2021 2018 6404 3881 2014 2067 2046 2014 5593 2088 1012 2043 2016 2288 2000 1996 7632 18477 7874 1010 2016 2584 2041 1998 5028 1037 9004 2389 5251 1012 2009 2371 5744 1998 4658 1010 2007 1037 9374 1997 10966 2646 1996 2415 1517 2074 2004 2016 2018 2124 2009 2052 2514 1012 1998 3875 2009 2001 2664 2178 2214 2767 1010 1037 2235 15751 2007 13844 2100 3727 1998 18765 29405 4870 2007 16405 14536 13602 15829 1998 2317 6401 1012 1523 11934 2566 2072 10105 19099 1010 1524 2049 6415 2623 1012 2129 4326 2000 2156 2009 1999 1037 8962 1010 7842 13159 2245 1012 2067 2188 2009 2074 3473 3748 1010 18414 13027 2041 2013 1996 15288 1999 5318 3681 2030 2090 26510 15753 1012 1998 2008 4138 1010 4086 6518 1517 2008 2001 5220 1010 2205 1012 7842 13159 11728 1996 11006 2854 2105 2014 1998 2179 1037 4206 1010 6080 2135 3269 2007 19401 2210 2317 28766 2006 2009 1012 1523 2079 2243 14360 1010 1524 2016 2056 1010 7842 14550 2075 1996 2514 1997 1996 2773 2006 2014 4416 1010 2130 2004 2016 8601 3264 1996 2394 2171 2006 2049 6415 1010 1523 14032 1012 1524 2028 1997 1996 28766 2018 5357 2125 1010 1998 5362 7842 13159 3856 2009 2039 1998 9557 2009 1012 2016 2701 2014 2159 1998 8726 1999 1010 6171 1012 1996 5220 24980 3561 2014 8948 1010 1998 7842 13159 2071 2471 2514 1996 2422 14119 1997 2014 7133 1521 1055 2146 3897 2606 1010 20229 8871 1010 2004 2016 22863 2098 2009 2041 2007 1996 2986 1011 11868 2098 6901 1011 7109 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.722155 139931951523648 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.723446 139931951523648 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.726236 139931951523648 tf_logging.py:115] label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.740805 139931951523648 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.748727 139931951523648 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] i was off , i was off to the onto rama to be exact . @ caps ##3 @ caps ##1 got to see a whole bunch of cool old cars , @ caps ##1 also got to see some old ones . some cars were old and rusty and some are new and state of the art . @ caps ##4 @ caps ##1 saw a sign . it read come see chip foo ##se ! so @ caps ##1 went and looked around . \" @ caps ##1 found him ! i exclaimed . \" @ caps ##2 \" @ caps ##8 brother yelled . \" @ caps ##3 is ! \" @ caps ##4 @ caps ##1 went and looked at the end of the line . it was @ caps ##5 ! so @ caps ##1 figured to would die down after a half an hour . @ caps ##1 were very wrong . it was @ caps ##5 ! the line @ caps ##1 thought was big was nothing compared to the line now so @ caps ##1 had to wait , and wait , and wait . @ caps ##8 brother go to the point to where he could run a mile if he didn ' t he was going to explode . so i had to go walk around with him . i was re ##li ##ved because standing and doing nothing for @ nu ##m ##1 hours gets bore ##ing real fast . i was getting to the point where i had to get up and walk around or i was going to fun asleep . so i went for a walk with him . @ caps ##1 got back and i sat back down @ caps ##1 waited , and waited , and waited and waited . @ caps ##4 @ caps ##1 heard stop ! \" @ caps ##6 body behind these people have to go and come back after the break . \" @ caps ##7 the security guard . @ caps ##1 were the last people . the people in front of us was a ne ##rdy couple who had no life . @ caps ##8 brother started to cry because he didn ' t get foo ##ses auto ##graph . the kind ne ##rdy couple @ caps ##7 \" here have our spat @ caps ##1 have all day and you @ caps ##2 like you really wanted this auto ##graph . \" @ caps ##8 brother replied \" sniff @ caps ##9 won ' t it ! ! \" @ caps ##1 got it after 51 / @ nu ##m ##1 hours of waiting and waiting and waiting and dying . that was the time i had to be patient . [SEP] write about patience . being patient means that you are understanding and tolerant . a patient person experience difficulties without complaining . do only one of the following : write a story about a time when you were patient or write a story about a time when someone you know was patient or write a story in your own way about patience . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.750312 139931951523648 tf_logging.py:115] tokens: [CLS] i was off , i was off to the onto rama to be exact . @ caps ##3 @ caps ##1 got to see a whole bunch of cool old cars , @ caps ##1 also got to see some old ones . some cars were old and rusty and some are new and state of the art . @ caps ##4 @ caps ##1 saw a sign . it read come see chip foo ##se ! so @ caps ##1 went and looked around . \" @ caps ##1 found him ! i exclaimed . \" @ caps ##2 \" @ caps ##8 brother yelled . \" @ caps ##3 is ! \" @ caps ##4 @ caps ##1 went and looked at the end of the line . it was @ caps ##5 ! so @ caps ##1 figured to would die down after a half an hour . @ caps ##1 were very wrong . it was @ caps ##5 ! the line @ caps ##1 thought was big was nothing compared to the line now so @ caps ##1 had to wait , and wait , and wait . @ caps ##8 brother go to the point to where he could run a mile if he didn ' t he was going to explode . so i had to go walk around with him . i was re ##li ##ved because standing and doing nothing for @ nu ##m ##1 hours gets bore ##ing real fast . i was getting to the point where i had to get up and walk around or i was going to fun asleep . so i went for a walk with him . @ caps ##1 got back and i sat back down @ caps ##1 waited , and waited , and waited and waited . @ caps ##4 @ caps ##1 heard stop ! \" @ caps ##6 body behind these people have to go and come back after the break . \" @ caps ##7 the security guard . @ caps ##1 were the last people . the people in front of us was a ne ##rdy couple who had no life . @ caps ##8 brother started to cry because he didn ' t get foo ##ses auto ##graph . the kind ne ##rdy couple @ caps ##7 \" here have our spat @ caps ##1 have all day and you @ caps ##2 like you really wanted this auto ##graph . \" @ caps ##8 brother replied \" sniff @ caps ##9 won ' t it ! ! \" @ caps ##1 got it after 51 / @ nu ##m ##1 hours of waiting and waiting and waiting and dying . that was the time i had to be patient . [SEP] write about patience . being patient means that you are understanding and tolerant . a patient person experience difficulties without complaining . do only one of the following : write a story about a time when you were patient or write a story about a time when someone you know was patient or write a story in your own way about patience . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 2001 2125 1010 1045 2001 2125 2000 1996 3031 14115 2000 2022 6635 1012 1030 9700 2509 1030 9700 2487 2288 2000 2156 1037 2878 9129 1997 4658 2214 3765 1010 1030 9700 2487 2036 2288 2000 2156 2070 2214 3924 1012 2070 3765 2020 2214 1998 13174 1998 2070 2024 2047 1998 2110 1997 1996 2396 1012 1030 9700 2549 1030 9700 2487 2387 1037 3696 1012 2009 3191 2272 2156 9090 29379 3366 999 2061 1030 9700 2487 2253 1998 2246 2105 1012 1000 1030 9700 2487 2179 2032 999 1045 12713 1012 1000 1030 9700 2475 1000 1030 9700 2620 2567 7581 1012 1000 1030 9700 2509 2003 999 1000 1030 9700 2549 1030 9700 2487 2253 1998 2246 2012 1996 2203 1997 1996 2240 1012 2009 2001 1030 9700 2629 999 2061 1030 9700 2487 6618 2000 2052 3280 2091 2044 1037 2431 2019 3178 1012 1030 9700 2487 2020 2200 3308 1012 2009 2001 1030 9700 2629 999 1996 2240 1030 9700 2487 2245 2001 2502 2001 2498 4102 2000 1996 2240 2085 2061 1030 9700 2487 2018 2000 3524 1010 1998 3524 1010 1998 3524 1012 1030 9700 2620 2567 2175 2000 1996 2391 2000 2073 2002 2071 2448 1037 3542 2065 2002 2134 1005 1056 2002 2001 2183 2000 15044 1012 2061 1045 2018 2000 2175 3328 2105 2007 2032 1012 1045 2001 2128 3669 7178 2138 3061 1998 2725 2498 2005 1030 16371 2213 2487 2847 4152 8501 2075 2613 3435 1012 1045 2001 2893 2000 1996 2391 2073 1045 2018 2000 2131 2039 1998 3328 2105 2030 1045 2001 2183 2000 4569 6680 1012 2061 1045 2253 2005 1037 3328 2007 2032 1012 1030 9700 2487 2288 2067 1998 1045 2938 2067 2091 1030 9700 2487 4741 1010 1998 4741 1010 1998 4741 1998 4741 1012 1030 9700 2549 1030 9700 2487 2657 2644 999 1000 1030 9700 2575 2303 2369 2122 2111 2031 2000 2175 1998 2272 2067 2044 1996 3338 1012 1000 1030 9700 2581 1996 3036 3457 1012 1030 9700 2487 2020 1996 2197 2111 1012 1996 2111 1999 2392 1997 2149 2001 1037 11265 17460 3232 2040 2018 2053 2166 1012 1030 9700 2620 2567 2318 2000 5390 2138 2002 2134 1005 1056 2131 29379 8583 8285 14413 1012 1996 2785 11265 17460 3232 1030 9700 2581 1000 2182 2031 2256 14690 1030 9700 2487 2031 2035 2154 1998 2017 1030 9700 2475 2066 2017 2428 2359 2023 8285 14413 1012 1000 1030 9700 2620 2567 3880 1000 27907 1030 9700 2683 2180 1005 1056 2009 999 999 1000 1030 9700 2487 2288 2009 2044 4868 1013 1030 16371 2213 2487 2847 1997 3403 1998 3403 1998 3403 1998 5996 1012 2008 2001 1996 2051 1045 2018 2000 2022 5776 1012 102 4339 2055 11752 1012 2108 5776 2965 2008 2017 2024 4824 1998 23691 1012 1037 5776 2711 3325 8190 2302 17949 1012 2079 2069 2028 1997 1996 2206 1024 4339 1037 2466 2055 1037 2051 2043 2017 2020 5776 2030 4339 1037 2466 2055 1037 2051 2043 2619 2017 2113 2001 5776 2030 4339 1037 2466 1999 2115 2219 2126 2055 11752 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.752190 139931951523648 tf_logging.py:115] input_ids: 101 1045 2001 2125 1010 1045 2001 2125 2000 1996 3031 14115 2000 2022 6635 1012 1030 9700 2509 1030 9700 2487 2288 2000 2156 1037 2878 9129 1997 4658 2214 3765 1010 1030 9700 2487 2036 2288 2000 2156 2070 2214 3924 1012 2070 3765 2020 2214 1998 13174 1998 2070 2024 2047 1998 2110 1997 1996 2396 1012 1030 9700 2549 1030 9700 2487 2387 1037 3696 1012 2009 3191 2272 2156 9090 29379 3366 999 2061 1030 9700 2487 2253 1998 2246 2105 1012 1000 1030 9700 2487 2179 2032 999 1045 12713 1012 1000 1030 9700 2475 1000 1030 9700 2620 2567 7581 1012 1000 1030 9700 2509 2003 999 1000 1030 9700 2549 1030 9700 2487 2253 1998 2246 2012 1996 2203 1997 1996 2240 1012 2009 2001 1030 9700 2629 999 2061 1030 9700 2487 6618 2000 2052 3280 2091 2044 1037 2431 2019 3178 1012 1030 9700 2487 2020 2200 3308 1012 2009 2001 1030 9700 2629 999 1996 2240 1030 9700 2487 2245 2001 2502 2001 2498 4102 2000 1996 2240 2085 2061 1030 9700 2487 2018 2000 3524 1010 1998 3524 1010 1998 3524 1012 1030 9700 2620 2567 2175 2000 1996 2391 2000 2073 2002 2071 2448 1037 3542 2065 2002 2134 1005 1056 2002 2001 2183 2000 15044 1012 2061 1045 2018 2000 2175 3328 2105 2007 2032 1012 1045 2001 2128 3669 7178 2138 3061 1998 2725 2498 2005 1030 16371 2213 2487 2847 4152 8501 2075 2613 3435 1012 1045 2001 2893 2000 1996 2391 2073 1045 2018 2000 2131 2039 1998 3328 2105 2030 1045 2001 2183 2000 4569 6680 1012 2061 1045 2253 2005 1037 3328 2007 2032 1012 1030 9700 2487 2288 2067 1998 1045 2938 2067 2091 1030 9700 2487 4741 1010 1998 4741 1010 1998 4741 1998 4741 1012 1030 9700 2549 1030 9700 2487 2657 2644 999 1000 1030 9700 2575 2303 2369 2122 2111 2031 2000 2175 1998 2272 2067 2044 1996 3338 1012 1000 1030 9700 2581 1996 3036 3457 1012 1030 9700 2487 2020 1996 2197 2111 1012 1996 2111 1999 2392 1997 2149 2001 1037 11265 17460 3232 2040 2018 2053 2166 1012 1030 9700 2620 2567 2318 2000 5390 2138 2002 2134 1005 1056 2131 29379 8583 8285 14413 1012 1996 2785 11265 17460 3232 1030 9700 2581 1000 2182 2031 2256 14690 1030 9700 2487 2031 2035 2154 1998 2017 1030 9700 2475 2066 2017 2428 2359 2023 8285 14413 1012 1000 1030 9700 2620 2567 3880 1000 27907 1030 9700 2683 2180 1005 1056 2009 999 999 1000 1030 9700 2487 2288 2009 2044 4868 1013 1030 16371 2213 2487 2847 1997 3403 1998 3403 1998 3403 1998 5996 1012 2008 2001 1996 2051 1045 2018 2000 2022 5776 1012 102 4339 2055 11752 1012 2108 5776 2965 2008 2017 2024 4824 1998 23691 1012 1037 5776 2711 3325 8190 2302 17949 1012 2079 2069 2028 1997 1996 2206 1024 4339 1037 2466 2055 1037 2051 2043 2017 2020 5776 2030 4339 1037 2466 2055 1037 2051 2043 2619 2017 2113 2001 5776 2030 4339 1037 2466 1999 2115 2219 2126 2055 11752 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.755543 139931951523648 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.757004 139931951523648 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 6 (id = 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.757999 139931951523648 tf_logging.py:115] label: 6 (id = 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.771790 139931951523648 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.778229 139931951523648 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] dear local newspaper on the subject of computers ; their pro ##s and con ##s , i would have to say that both sides provide ve ##ru important reasons why you should and shouldn ' t use computers , i think that computers are very important to many students just like me . first of all computers help me and my fellow classmates with many as ##is ##gn ##men ##rs . some of us simply don ' t have the time to go to a library and look for a book that might ni ##t even posse ##s the information needed . on the internet you can find the same amount of valuable information or more ! it also helps to develop your hand - eye coordination some pe ##ope ##l say \" you can learn hand - eye co ##ord ##ian ##tion outside too ? although that is true , typing is very fat ##s and neat . as you can see my hand wr ##ii ##t ##ng is not the best but i type very fast and make little mistakes so i do not need the neat ##est hand writing ever . the way i see it computers are the faster ##t way of doing al ##ot of things . so if you spend time writ ##ign neatly you might ni ##t even be able to go outside and see nature . my dad is a environmental ##ist and he uses computers . sometime the easy way isn ; ' t always the best way that ##s why we must have balance . do not in ##fr ##om people to stop using computers but to balance th ##ier time with them . everything is good if it ' s balanced . for example , co ##ffie ##ne , in large amounts it is rather un ##hea ##lth ##y but when you need a small you turn to caf ##fi ##ene and it help ##d , same with computers . besides , computers are now made ever ##w ##her ##e , trying to get rid of them is simply unreasonable . we are already in a recession we don ##t need more trouble . i prefer to talk with my friends eye to eye however if you cannot do that a computer is a perfect alternative . so you see com ##uter ##s are ev ##ry helpful to some balance is the answer ! [SEP] more and more people use computers , but not everyone agrees that this benefits society . those who support advances in technology believe that computers have a positive effect on people . they teach hand - eye coordination , give people the ability to learn about far ##away places and people , and even allow people to talk online with other people . others have different ideas . some experts are concerned that people are spending too much time on their computers and less time exercising , enjoying nature , and interacting with family and friends . write a letter to your local newspaper in which you state your opinion on the effects computers have on people . persuade the readers to agree with you . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.779631 139931951523648 tf_logging.py:115] tokens: [CLS] dear local newspaper on the subject of computers ; their pro ##s and con ##s , i would have to say that both sides provide ve ##ru important reasons why you should and shouldn ' t use computers , i think that computers are very important to many students just like me . first of all computers help me and my fellow classmates with many as ##is ##gn ##men ##rs . some of us simply don ' t have the time to go to a library and look for a book that might ni ##t even posse ##s the information needed . on the internet you can find the same amount of valuable information or more ! it also helps to develop your hand - eye coordination some pe ##ope ##l say \" you can learn hand - eye co ##ord ##ian ##tion outside too ? although that is true , typing is very fat ##s and neat . as you can see my hand wr ##ii ##t ##ng is not the best but i type very fast and make little mistakes so i do not need the neat ##est hand writing ever . the way i see it computers are the faster ##t way of doing al ##ot of things . so if you spend time writ ##ign neatly you might ni ##t even be able to go outside and see nature . my dad is a environmental ##ist and he uses computers . sometime the easy way isn ; ' t always the best way that ##s why we must have balance . do not in ##fr ##om people to stop using computers but to balance th ##ier time with them . everything is good if it ' s balanced . for example , co ##ffie ##ne , in large amounts it is rather un ##hea ##lth ##y but when you need a small you turn to caf ##fi ##ene and it help ##d , same with computers . besides , computers are now made ever ##w ##her ##e , trying to get rid of them is simply unreasonable . we are already in a recession we don ##t need more trouble . i prefer to talk with my friends eye to eye however if you cannot do that a computer is a perfect alternative . so you see com ##uter ##s are ev ##ry helpful to some balance is the answer ! [SEP] more and more people use computers , but not everyone agrees that this benefits society . those who support advances in technology believe that computers have a positive effect on people . they teach hand - eye coordination , give people the ability to learn about far ##away places and people , and even allow people to talk online with other people . others have different ideas . some experts are concerned that people are spending too much time on their computers and less time exercising , enjoying nature , and interacting with family and friends . write a letter to your local newspaper in which you state your opinion on the effects computers have on people . persuade the readers to agree with you . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 6203 2334 3780 2006 1996 3395 1997 7588 1025 2037 4013 2015 1998 9530 2015 1010 1045 2052 2031 2000 2360 2008 2119 3903 3073 2310 6820 2590 4436 2339 2017 2323 1998 5807 1005 1056 2224 7588 1010 1045 2228 2008 7588 2024 2200 2590 2000 2116 2493 2074 2066 2033 1012 2034 1997 2035 7588 2393 2033 1998 2026 3507 19846 2007 2116 2004 2483 16206 3549 2869 1012 2070 1997 2149 3432 2123 1005 1056 2031 1996 2051 2000 2175 2000 1037 3075 1998 2298 2005 1037 2338 2008 2453 9152 2102 2130 25751 2015 1996 2592 2734 1012 2006 1996 4274 2017 2064 2424 1996 2168 3815 1997 7070 2592 2030 2062 999 2009 2036 7126 2000 4503 2115 2192 1011 3239 12016 2070 21877 17635 2140 2360 1000 2017 2064 4553 2192 1011 3239 2522 8551 2937 3508 2648 2205 1029 2348 2008 2003 2995 1010 22868 2003 2200 6638 2015 1998 15708 1012 2004 2017 2064 2156 2026 2192 23277 6137 2102 3070 2003 2025 1996 2190 2021 1045 2828 2200 3435 1998 2191 2210 12051 2061 1045 2079 2025 2342 1996 15708 4355 2192 3015 2412 1012 1996 2126 1045 2156 2009 7588 2024 1996 5514 2102 2126 1997 2725 2632 4140 1997 2477 1012 2061 2065 2017 5247 2051 25697 23773 15981 2017 2453 9152 2102 2130 2022 2583 2000 2175 2648 1998 2156 3267 1012 2026 3611 2003 1037 4483 2923 1998 2002 3594 7588 1012 8811 1996 3733 2126 3475 1025 1005 1056 2467 1996 2190 2126 2008 2015 2339 2057 2442 2031 5703 1012 2079 2025 1999 19699 5358 2111 2000 2644 2478 7588 2021 2000 5703 16215 3771 2051 2007 2068 1012 2673 2003 2204 2065 2009 1005 1055 12042 1012 2005 2742 1010 2522 29055 2638 1010 1999 2312 8310 2009 2003 2738 4895 20192 24658 2100 2021 2043 2017 2342 1037 2235 2017 2735 2000 24689 8873 8625 1998 2009 2393 2094 1010 2168 2007 7588 1012 4661 1010 7588 2024 2085 2081 2412 2860 5886 2063 1010 2667 2000 2131 9436 1997 2068 2003 3432 29205 1012 2057 2024 2525 1999 1037 19396 2057 2123 2102 2342 2062 4390 1012 1045 9544 2000 2831 2007 2026 2814 3239 2000 3239 2174 2065 2017 3685 2079 2008 1037 3274 2003 1037 3819 4522 1012 2061 2017 2156 4012 19901 2015 2024 23408 2854 14044 2000 2070 5703 2003 1996 3437 999 102 2062 1998 2062 2111 2224 7588 1010 2021 2025 3071 10217 2008 2023 6666 2554 1012 2216 2040 2490 9849 1999 2974 2903 2008 7588 2031 1037 3893 3466 2006 2111 1012 2027 6570 2192 1011 3239 12016 1010 2507 2111 1996 3754 2000 4553 2055 2521 9497 3182 1998 2111 1010 1998 2130 3499 2111 2000 2831 3784 2007 2060 2111 1012 2500 2031 2367 4784 1012 2070 8519 2024 4986 2008 2111 2024 5938 2205 2172 2051 2006 2037 7588 1998 2625 2051 28428 1010 9107 3267 1010 1998 21935 2007 2155 1998 2814 1012 4339 1037 3661 2000 2115 2334 3780 1999 2029 2017 2110 2115 5448 2006 1996 3896 7588 2031 2006 2111 1012 13984 1996 8141 2000 5993 2007 2017 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.780998 139931951523648 tf_logging.py:115] input_ids: 101 6203 2334 3780 2006 1996 3395 1997 7588 1025 2037 4013 2015 1998 9530 2015 1010 1045 2052 2031 2000 2360 2008 2119 3903 3073 2310 6820 2590 4436 2339 2017 2323 1998 5807 1005 1056 2224 7588 1010 1045 2228 2008 7588 2024 2200 2590 2000 2116 2493 2074 2066 2033 1012 2034 1997 2035 7588 2393 2033 1998 2026 3507 19846 2007 2116 2004 2483 16206 3549 2869 1012 2070 1997 2149 3432 2123 1005 1056 2031 1996 2051 2000 2175 2000 1037 3075 1998 2298 2005 1037 2338 2008 2453 9152 2102 2130 25751 2015 1996 2592 2734 1012 2006 1996 4274 2017 2064 2424 1996 2168 3815 1997 7070 2592 2030 2062 999 2009 2036 7126 2000 4503 2115 2192 1011 3239 12016 2070 21877 17635 2140 2360 1000 2017 2064 4553 2192 1011 3239 2522 8551 2937 3508 2648 2205 1029 2348 2008 2003 2995 1010 22868 2003 2200 6638 2015 1998 15708 1012 2004 2017 2064 2156 2026 2192 23277 6137 2102 3070 2003 2025 1996 2190 2021 1045 2828 2200 3435 1998 2191 2210 12051 2061 1045 2079 2025 2342 1996 15708 4355 2192 3015 2412 1012 1996 2126 1045 2156 2009 7588 2024 1996 5514 2102 2126 1997 2725 2632 4140 1997 2477 1012 2061 2065 2017 5247 2051 25697 23773 15981 2017 2453 9152 2102 2130 2022 2583 2000 2175 2648 1998 2156 3267 1012 2026 3611 2003 1037 4483 2923 1998 2002 3594 7588 1012 8811 1996 3733 2126 3475 1025 1005 1056 2467 1996 2190 2126 2008 2015 2339 2057 2442 2031 5703 1012 2079 2025 1999 19699 5358 2111 2000 2644 2478 7588 2021 2000 5703 16215 3771 2051 2007 2068 1012 2673 2003 2204 2065 2009 1005 1055 12042 1012 2005 2742 1010 2522 29055 2638 1010 1999 2312 8310 2009 2003 2738 4895 20192 24658 2100 2021 2043 2017 2342 1037 2235 2017 2735 2000 24689 8873 8625 1998 2009 2393 2094 1010 2168 2007 7588 1012 4661 1010 7588 2024 2085 2081 2412 2860 5886 2063 1010 2667 2000 2131 9436 1997 2068 2003 3432 29205 1012 2057 2024 2525 1999 1037 19396 2057 2123 2102 2342 2062 4390 1012 1045 9544 2000 2831 2007 2026 2814 3239 2000 3239 2174 2065 2017 3685 2079 2008 1037 3274 2003 1037 3819 4522 1012 2061 2017 2156 4012 19901 2015 2024 23408 2854 14044 2000 2070 5703 2003 1996 3437 999 102 2062 1998 2062 2111 2224 7588 1010 2021 2025 3071 10217 2008 2023 6666 2554 1012 2216 2040 2490 9849 1999 2974 2903 2008 7588 2031 1037 3893 3466 2006 2111 1012 2027 6570 2192 1011 3239 12016 1010 2507 2111 1996 3754 2000 4553 2055 2521 9497 3182 1998 2111 1010 1998 2130 3499 2111 2000 2831 3784 2007 2060 2111 1012 2500 2031 2367 4784 1012 2070 8519 2024 4986 2008 2111 2024 5938 2205 2172 2051 2006 2037 7588 1998 2625 2051 28428 1010 9107 3267 1010 1998 21935 2007 2155 1998 2814 1012 4339 1037 3661 2000 2115 2334 3780 1999 2029 2017 2110 2115 5448 2006 1996 3896 7588 2031 2006 2111 1012 13984 1996 8141 2000 5993 2007 2017 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.788751 139931951523648 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.789967 139931951523648 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 5 (id = 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:28:27.790894 139931951523648 tf_logging.py:115] label: 5 (id = 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 1947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:21.203934 139931951523648 tf_logging.py:115] Writing example 0 of 1947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:21.240379 139931951523648 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:21.246422 139931951523648 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] the author concludes the story with the saying because it shows that she has con ##fen ##t in her self because she did not pass the test she could take it again and know she can pass the test she was up set about it her mother when she talk to her made her feel good and con ##fen ##t to do or re ##take the test and know she could pass the test so that why i think the author concludes it with that saying at the end of the story . [SEP] winter hi ##bis ##cus by min ##fo ##ng ho sa ##eng , a teenage girl , and her family have moved to the united states from vietnam . as sa ##eng walks home after failing her driver ’ s test , she sees a familiar plant . later , she goes to a fl ##oris ##t shop to see if the plant can be purchased . it was like walking into another world . a hot , moist world exploding with greene ##ry . huge flat leaves , delicate wi ##sp ##s of tend ##ril ##s , ferns and fr ##ond ##s and vines of all shades and shapes grew in seemingly random prof ##usion . “ over there , in the corner , the hi ##bis ##cus . is that what you mean ? ” the fl ##oris ##t pointed at a leaf ##y pot ##ted plant by the corner . there , in a shaft of the wan afternoon sunlight , was a single blood - red blossom , its five petals sp ##lay ##ed back to reveal a long st ##amen tipped with yellow pollen . sa ##eng felt a shock of recognition so intense , it was almost vis ##cera ##l . “ sa ##eb ##ba , ” sa ##eng whispered . a sa ##eb ##ba hedge , tall and lush , had surrounded their garden , its lush green leaves dotted with ve ##rmi ##lion flowers . and sometimes after a monsoon rain , a blossom or two would have blown into the well , so that when she drew the well water , she would find a red blossom floating in the bucket . slowly , sa ##eng walked down the narrow aisle toward the hi ##bis ##cus . orchid ##s , lan ##na bushes , ole ##ander ##s , elephant ear beg ##onia ##s , and bo ##uga ##in ##ville ##a vines surrounded her . plants that she had not even realized she had known but had forgotten drew her back into her childhood world . when she got to the hi ##bis ##cus , she reached out and touched a pet ##al gently . it felt smooth and cool , with a hint of velvet toward the center — just as she had known it would feel . and beside it was yet another old friend , a small shrub with wax ##y leaves and dai ##nty flowers with pu ##rp ##lish petals and white centers . “ madagascar per ##i ##win ##kle , ” its tag announced . how strange to see it in a pot , sa ##eng thought . back home it just grew wild , ju ##tting out from the cracks in brick walls or between tiled roofs . and that rich , sweet scent — that was familiar , too . sa ##eng scanned the greene ##ry around her and found a tall , gang ##ly plant with exquisite little white blossoms on it . “ do ##k malik , ” she said , sa ##vor ##ing the feel of the word on her tongue , even as she silently noted the english name on its tag , “ jasmine . ” one of the blossoms had fallen off , and carefully sa ##eng picked it up and smelled it . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:21.249755 139931951523648 tf_logging.py:115] tokens: [CLS] the author concludes the story with the saying because it shows that she has con ##fen ##t in her self because she did not pass the test she could take it again and know she can pass the test she was up set about it her mother when she talk to her made her feel good and con ##fen ##t to do or re ##take the test and know she could pass the test so that why i think the author concludes it with that saying at the end of the story . [SEP] winter hi ##bis ##cus by min ##fo ##ng ho sa ##eng , a teenage girl , and her family have moved to the united states from vietnam . as sa ##eng walks home after failing her driver ’ s test , she sees a familiar plant . later , she goes to a fl ##oris ##t shop to see if the plant can be purchased . it was like walking into another world . a hot , moist world exploding with greene ##ry . huge flat leaves , delicate wi ##sp ##s of tend ##ril ##s , ferns and fr ##ond ##s and vines of all shades and shapes grew in seemingly random prof ##usion . “ over there , in the corner , the hi ##bis ##cus . is that what you mean ? ” the fl ##oris ##t pointed at a leaf ##y pot ##ted plant by the corner . there , in a shaft of the wan afternoon sunlight , was a single blood - red blossom , its five petals sp ##lay ##ed back to reveal a long st ##amen tipped with yellow pollen . sa ##eng felt a shock of recognition so intense , it was almost vis ##cera ##l . “ sa ##eb ##ba , ” sa ##eng whispered . a sa ##eb ##ba hedge , tall and lush , had surrounded their garden , its lush green leaves dotted with ve ##rmi ##lion flowers . and sometimes after a monsoon rain , a blossom or two would have blown into the well , so that when she drew the well water , she would find a red blossom floating in the bucket . slowly , sa ##eng walked down the narrow aisle toward the hi ##bis ##cus . orchid ##s , lan ##na bushes , ole ##ander ##s , elephant ear beg ##onia ##s , and bo ##uga ##in ##ville ##a vines surrounded her . plants that she had not even realized she had known but had forgotten drew her back into her childhood world . when she got to the hi ##bis ##cus , she reached out and touched a pet ##al gently . it felt smooth and cool , with a hint of velvet toward the center — just as she had known it would feel . and beside it was yet another old friend , a small shrub with wax ##y leaves and dai ##nty flowers with pu ##rp ##lish petals and white centers . “ madagascar per ##i ##win ##kle , ” its tag announced . how strange to see it in a pot , sa ##eng thought . back home it just grew wild , ju ##tting out from the cracks in brick walls or between tiled roofs . and that rich , sweet scent — that was familiar , too . sa ##eng scanned the greene ##ry around her and found a tall , gang ##ly plant with exquisite little white blossoms on it . “ do ##k malik , ” she said , sa ##vor ##ing the feel of the word on her tongue , even as she silently noted the english name on its tag , “ jasmine . ” one of the blossoms had fallen off , and carefully sa ##eng picked it up and smelled it . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1996 3166 14730 1996 2466 2007 1996 3038 2138 2009 3065 2008 2016 2038 9530 18940 2102 1999 2014 2969 2138 2016 2106 2025 3413 1996 3231 2016 2071 2202 2009 2153 1998 2113 2016 2064 3413 1996 3231 2016 2001 2039 2275 2055 2009 2014 2388 2043 2016 2831 2000 2014 2081 2014 2514 2204 1998 9530 18940 2102 2000 2079 2030 2128 15166 1996 3231 1998 2113 2016 2071 3413 1996 3231 2061 2008 2339 1045 2228 1996 3166 14730 2009 2007 2008 3038 2012 1996 2203 1997 1996 2466 1012 102 3467 7632 18477 7874 2011 8117 14876 3070 7570 7842 13159 1010 1037 9454 2611 1010 1998 2014 2155 2031 2333 2000 1996 2142 2163 2013 5148 1012 2004 7842 13159 7365 2188 2044 7989 2014 4062 1521 1055 3231 1010 2016 5927 1037 5220 3269 1012 2101 1010 2016 3632 2000 1037 13109 21239 2102 4497 2000 2156 2065 1996 3269 2064 2022 4156 1012 2009 2001 2066 3788 2046 2178 2088 1012 1037 2980 1010 11052 2088 20728 2007 11006 2854 1012 4121 4257 3727 1010 10059 15536 13102 2015 1997 7166 15928 2015 1010 25715 1998 10424 15422 2015 1998 16702 1997 2035 13178 1998 10466 3473 1999 9428 6721 11268 14499 1012 1523 2058 2045 1010 1999 1996 3420 1010 1996 7632 18477 7874 1012 2003 2008 2054 2017 2812 1029 1524 1996 13109 21239 2102 4197 2012 1037 7053 2100 8962 3064 3269 2011 1996 3420 1012 2045 1010 1999 1037 9093 1997 1996 14071 5027 9325 1010 2001 1037 2309 2668 1011 2417 20593 1010 2049 2274 15829 11867 8485 2098 2067 2000 7487 1037 2146 2358 27245 11182 2007 3756 22482 1012 7842 13159 2371 1037 5213 1997 5038 2061 6387 1010 2009 2001 2471 25292 19357 2140 1012 1523 7842 15878 3676 1010 1524 7842 13159 3990 1012 1037 7842 15878 3676 17834 1010 4206 1998 16299 1010 2018 5129 2037 3871 1010 2049 16299 2665 3727 20384 2007 2310 28550 18964 4870 1012 1998 2823 2044 1037 19183 4542 1010 1037 20593 2030 2048 2052 2031 10676 2046 1996 2092 1010 2061 2008 2043 2016 3881 1996 2092 2300 1010 2016 2052 2424 1037 2417 20593 8274 1999 1996 13610 1012 3254 1010 7842 13159 2939 2091 1996 4867 12485 2646 1996 7632 18477 7874 1012 15573 2015 1010 17595 2532 14568 1010 15589 12243 2015 1010 10777 4540 11693 12488 2015 1010 1998 8945 16377 2378 3077 2050 16702 5129 2014 1012 4264 2008 2016 2018 2025 2130 3651 2016 2018 2124 2021 2018 6404 3881 2014 2067 2046 2014 5593 2088 1012 2043 2016 2288 2000 1996 7632 18477 7874 1010 2016 2584 2041 1998 5028 1037 9004 2389 5251 1012 2009 2371 5744 1998 4658 1010 2007 1037 9374 1997 10966 2646 1996 2415 1517 2074 2004 2016 2018 2124 2009 2052 2514 1012 1998 3875 2009 2001 2664 2178 2214 2767 1010 1037 2235 15751 2007 13844 2100 3727 1998 18765 29405 4870 2007 16405 14536 13602 15829 1998 2317 6401 1012 1523 11934 2566 2072 10105 19099 1010 1524 2049 6415 2623 1012 2129 4326 2000 2156 2009 1999 1037 8962 1010 7842 13159 2245 1012 2067 2188 2009 2074 3473 3748 1010 18414 13027 2041 2013 1996 15288 1999 5318 3681 2030 2090 26510 15753 1012 1998 2008 4138 1010 4086 6518 1517 2008 2001 5220 1010 2205 1012 7842 13159 11728 1996 11006 2854 2105 2014 1998 2179 1037 4206 1010 6080 2135 3269 2007 19401 2210 2317 28766 2006 2009 1012 1523 2079 2243 14360 1010 1524 2016 2056 1010 7842 14550 2075 1996 2514 1997 1996 2773 2006 2014 4416 1010 2130 2004 2016 8601 3264 1996 2394 2171 2006 2049 6415 1010 1523 14032 1012 1524 2028 1997 1996 28766 2018 5357 2125 1010 1998 5362 7842 13159 3856 2009 2039 1998 9557 2009 1012 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:21.251632 139931951523648 tf_logging.py:115] input_ids: 101 1996 3166 14730 1996 2466 2007 1996 3038 2138 2009 3065 2008 2016 2038 9530 18940 2102 1999 2014 2969 2138 2016 2106 2025 3413 1996 3231 2016 2071 2202 2009 2153 1998 2113 2016 2064 3413 1996 3231 2016 2001 2039 2275 2055 2009 2014 2388 2043 2016 2831 2000 2014 2081 2014 2514 2204 1998 9530 18940 2102 2000 2079 2030 2128 15166 1996 3231 1998 2113 2016 2071 3413 1996 3231 2061 2008 2339 1045 2228 1996 3166 14730 2009 2007 2008 3038 2012 1996 2203 1997 1996 2466 1012 102 3467 7632 18477 7874 2011 8117 14876 3070 7570 7842 13159 1010 1037 9454 2611 1010 1998 2014 2155 2031 2333 2000 1996 2142 2163 2013 5148 1012 2004 7842 13159 7365 2188 2044 7989 2014 4062 1521 1055 3231 1010 2016 5927 1037 5220 3269 1012 2101 1010 2016 3632 2000 1037 13109 21239 2102 4497 2000 2156 2065 1996 3269 2064 2022 4156 1012 2009 2001 2066 3788 2046 2178 2088 1012 1037 2980 1010 11052 2088 20728 2007 11006 2854 1012 4121 4257 3727 1010 10059 15536 13102 2015 1997 7166 15928 2015 1010 25715 1998 10424 15422 2015 1998 16702 1997 2035 13178 1998 10466 3473 1999 9428 6721 11268 14499 1012 1523 2058 2045 1010 1999 1996 3420 1010 1996 7632 18477 7874 1012 2003 2008 2054 2017 2812 1029 1524 1996 13109 21239 2102 4197 2012 1037 7053 2100 8962 3064 3269 2011 1996 3420 1012 2045 1010 1999 1037 9093 1997 1996 14071 5027 9325 1010 2001 1037 2309 2668 1011 2417 20593 1010 2049 2274 15829 11867 8485 2098 2067 2000 7487 1037 2146 2358 27245 11182 2007 3756 22482 1012 7842 13159 2371 1037 5213 1997 5038 2061 6387 1010 2009 2001 2471 25292 19357 2140 1012 1523 7842 15878 3676 1010 1524 7842 13159 3990 1012 1037 7842 15878 3676 17834 1010 4206 1998 16299 1010 2018 5129 2037 3871 1010 2049 16299 2665 3727 20384 2007 2310 28550 18964 4870 1012 1998 2823 2044 1037 19183 4542 1010 1037 20593 2030 2048 2052 2031 10676 2046 1996 2092 1010 2061 2008 2043 2016 3881 1996 2092 2300 1010 2016 2052 2424 1037 2417 20593 8274 1999 1996 13610 1012 3254 1010 7842 13159 2939 2091 1996 4867 12485 2646 1996 7632 18477 7874 1012 15573 2015 1010 17595 2532 14568 1010 15589 12243 2015 1010 10777 4540 11693 12488 2015 1010 1998 8945 16377 2378 3077 2050 16702 5129 2014 1012 4264 2008 2016 2018 2025 2130 3651 2016 2018 2124 2021 2018 6404 3881 2014 2067 2046 2014 5593 2088 1012 2043 2016 2288 2000 1996 7632 18477 7874 1010 2016 2584 2041 1998 5028 1037 9004 2389 5251 1012 2009 2371 5744 1998 4658 1010 2007 1037 9374 1997 10966 2646 1996 2415 1517 2074 2004 2016 2018 2124 2009 2052 2514 1012 1998 3875 2009 2001 2664 2178 2214 2767 1010 1037 2235 15751 2007 13844 2100 3727 1998 18765 29405 4870 2007 16405 14536 13602 15829 1998 2317 6401 1012 1523 11934 2566 2072 10105 19099 1010 1524 2049 6415 2623 1012 2129 4326 2000 2156 2009 1999 1037 8962 1010 7842 13159 2245 1012 2067 2188 2009 2074 3473 3748 1010 18414 13027 2041 2013 1996 15288 1999 5318 3681 2030 2090 26510 15753 1012 1998 2008 4138 1010 4086 6518 1517 2008 2001 5220 1010 2205 1012 7842 13159 11728 1996 11006 2854 2105 2014 1998 2179 1037 4206 1010 6080 2135 3269 2007 19401 2210 2317 28766 2006 2009 1012 1523 2079 2243 14360 1010 1524 2016 2056 1010 7842 14550 2075 1996 2514 1997 1996 2773 2006 2014 4416 1010 2130 2004 2016 8601 3264 1996 2394 2171 2006 2049 6415 1010 1523 14032 1012 1524 2028 1997 1996 28766 2018 5357 2125 1010 1998 5362 7842 13159 3856 2009 2039 1998 9557 2009 1012 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:21.254324 139931951523648 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:21.255681 139931951523648 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 3 (id = 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:21.891726 139931951523648 tf_logging.py:115] label: 3 (id = 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.058327 139931951523648 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.065787 139931951523648 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] when you are using technology usually it is work related . a computer is used for work to write essays , make power points , and research , when people use them for facebook , myspace , @ caps ##1 , and other chat live o ##ovo ##o and websites it is more for their pleasure . i only go on facebook when i am checking my schedule for hockey , most kids use it for talk . then there is o ##ovo ##o & online video chat used for talking with friends but why not just call ##em ' up . when i think of facebook or o ##ovo ##o , or even myspace it makes me think of ina ##pro ##pr ##iate pictures kids , and adults post . i think that is very un ##hum ##ane and that is very dangerous to post ot because once it ' s out there you ' ll never get it back . for instance i am always outside ##d hang ##ina ##rou ##nd when it ' s warm out , during winter i can understand going on facebook , but during the @ date ##1 there is no need just go outside and have fun . kids wonder why they gain weight we ' ll if you ' de get off your computer , and quit eating twin ##kie ##s then you ' ll notice you just have to be active . some kids parents don ' t really give a flying fa ##ld ##ood ##le what their kids do all day . they don ' t care if their kid is @ nu ##m ##1 lbs . at the age of @ nu ##m ##2 . they don ' t care if their kids are sitting on the computer eating all - day . if i could tell you what it must feel like to be a couch potato i would but i never go on the computer unless it is for homework . i used the computer just @ time ##1 i typed up all of my homework except for my math . i didn ' t play one game , i didn ' t check email go on the internet or anything i just worked my hardest and got it done . i conclude this with the fact that using a computer is not a bad thing , using it repeatedly and not exercising is . a computer shouldn ' t need to be used for pleasure it ' s not created to do so it ' s created to keep your life simpler . people pay money to buy a expensive computer and mo ##tem and all that expensive technological acc ##es ##ories , to make their lives much simpler not anywhere in the ##s essay did i say to play games because it ' s not . playing games can be done outside , it doesn ' t have to be inside , and def ##inate ##ly doesn ' t have to be on a computer . to make everyone ##s life easier kids , get [SEP] more and more people use computers , but not everyone agrees that this benefits society . those who support advances in technology believe that computers have a positive effect on people . they teach hand - eye coordination , give people the ability to learn about far ##away places and people , and even allow people to talk online with other people . others have different ideas . some experts are concerned that people are spending too much time on their computers and less time exercising , enjoying nature , and interacting with family and friends . write a letter to your local newspaper in which you state your opinion on the effects computers have on people . persuade the readers to agree with you . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.067981 139931951523648 tf_logging.py:115] tokens: [CLS] when you are using technology usually it is work related . a computer is used for work to write essays , make power points , and research , when people use them for facebook , myspace , @ caps ##1 , and other chat live o ##ovo ##o and websites it is more for their pleasure . i only go on facebook when i am checking my schedule for hockey , most kids use it for talk . then there is o ##ovo ##o & online video chat used for talking with friends but why not just call ##em ' up . when i think of facebook or o ##ovo ##o , or even myspace it makes me think of ina ##pro ##pr ##iate pictures kids , and adults post . i think that is very un ##hum ##ane and that is very dangerous to post ot because once it ' s out there you ' ll never get it back . for instance i am always outside ##d hang ##ina ##rou ##nd when it ' s warm out , during winter i can understand going on facebook , but during the @ date ##1 there is no need just go outside and have fun . kids wonder why they gain weight we ' ll if you ' de get off your computer , and quit eating twin ##kie ##s then you ' ll notice you just have to be active . some kids parents don ' t really give a flying fa ##ld ##ood ##le what their kids do all day . they don ' t care if their kid is @ nu ##m ##1 lbs . at the age of @ nu ##m ##2 . they don ' t care if their kids are sitting on the computer eating all - day . if i could tell you what it must feel like to be a couch potato i would but i never go on the computer unless it is for homework . i used the computer just @ time ##1 i typed up all of my homework except for my math . i didn ' t play one game , i didn ' t check email go on the internet or anything i just worked my hardest and got it done . i conclude this with the fact that using a computer is not a bad thing , using it repeatedly and not exercising is . a computer shouldn ' t need to be used for pleasure it ' s not created to do so it ' s created to keep your life simpler . people pay money to buy a expensive computer and mo ##tem and all that expensive technological acc ##es ##ories , to make their lives much simpler not anywhere in the ##s essay did i say to play games because it ' s not . playing games can be done outside , it doesn ' t have to be inside , and def ##inate ##ly doesn ' t have to be on a computer . to make everyone ##s life easier kids , get [SEP] more and more people use computers , but not everyone agrees that this benefits society . those who support advances in technology believe that computers have a positive effect on people . they teach hand - eye coordination , give people the ability to learn about far ##away places and people , and even allow people to talk online with other people . others have different ideas . some experts are concerned that people are spending too much time on their computers and less time exercising , enjoying nature , and interacting with family and friends . write a letter to your local newspaper in which you state your opinion on the effects computers have on people . persuade the readers to agree with you . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2043 2017 2024 2478 2974 2788 2009 2003 2147 3141 1012 1037 3274 2003 2109 2005 2147 2000 4339 8927 1010 2191 2373 2685 1010 1998 2470 1010 2043 2111 2224 2068 2005 9130 1010 24927 1010 1030 9700 2487 1010 1998 2060 11834 2444 1051 16059 2080 1998 11744 2009 2003 2062 2005 2037 5165 1012 1045 2069 2175 2006 9130 2043 1045 2572 9361 2026 6134 2005 3873 1010 2087 4268 2224 2009 2005 2831 1012 2059 2045 2003 1051 16059 2080 1004 3784 2678 11834 2109 2005 3331 2007 2814 2021 2339 2025 2074 2655 6633 1005 2039 1012 2043 1045 2228 1997 9130 2030 1051 16059 2080 1010 2030 2130 24927 2009 3084 2033 2228 1997 27118 21572 18098 13143 4620 4268 1010 1998 6001 2695 1012 1045 2228 2008 2003 2200 4895 28600 7231 1998 2008 2003 2200 4795 2000 2695 27178 2138 2320 2009 1005 1055 2041 2045 2017 1005 2222 2196 2131 2009 2067 1012 2005 6013 1045 2572 2467 2648 2094 6865 3981 22494 4859 2043 2009 1005 1055 4010 2041 1010 2076 3467 1045 2064 3305 2183 2006 9130 1010 2021 2076 1996 1030 3058 2487 2045 2003 2053 2342 2074 2175 2648 1998 2031 4569 1012 4268 4687 2339 2027 5114 3635 2057 1005 2222 2065 2017 1005 2139 2131 2125 2115 3274 1010 1998 8046 5983 5519 11602 2015 2059 2017 1005 2222 5060 2017 2074 2031 2000 2022 3161 1012 2070 4268 3008 2123 1005 1056 2428 2507 1037 3909 6904 6392 17139 2571 2054 2037 4268 2079 2035 2154 1012 2027 2123 1005 1056 2729 2065 2037 4845 2003 1030 16371 2213 2487 20702 1012 2012 1996 2287 1997 1030 16371 2213 2475 1012 2027 2123 1005 1056 2729 2065 2037 4268 2024 3564 2006 1996 3274 5983 2035 1011 2154 1012 2065 1045 2071 2425 2017 2054 2009 2442 2514 2066 2000 2022 1037 6411 14557 1045 2052 2021 1045 2196 2175 2006 1996 3274 4983 2009 2003 2005 19453 1012 1045 2109 1996 3274 2074 1030 2051 2487 1045 21189 2039 2035 1997 2026 19453 3272 2005 2026 8785 1012 1045 2134 1005 1056 2377 2028 2208 1010 1045 2134 1005 1056 4638 10373 2175 2006 1996 4274 2030 2505 1045 2074 2499 2026 18263 1998 2288 2009 2589 1012 1045 16519 2023 2007 1996 2755 2008 2478 1037 3274 2003 2025 1037 2919 2518 1010 2478 2009 8385 1998 2025 28428 2003 1012 1037 3274 5807 1005 1056 2342 2000 2022 2109 2005 5165 2009 1005 1055 2025 2580 2000 2079 2061 2009 1005 1055 2580 2000 2562 2115 2166 16325 1012 2111 3477 2769 2000 4965 1037 6450 3274 1998 9587 18532 1998 2035 2008 6450 10660 16222 2229 18909 1010 2000 2191 2037 3268 2172 16325 2025 5973 1999 1996 2015 9491 2106 1045 2360 2000 2377 2399 2138 2009 1005 1055 2025 1012 2652 2399 2064 2022 2589 2648 1010 2009 2987 1005 1056 2031 2000 2022 2503 1010 1998 13366 14776 2135 2987 1005 1056 2031 2000 2022 2006 1037 3274 1012 2000 2191 3071 2015 2166 6082 4268 1010 2131 102 2062 1998 2062 2111 2224 7588 1010 2021 2025 3071 10217 2008 2023 6666 2554 1012 2216 2040 2490 9849 1999 2974 2903 2008 7588 2031 1037 3893 3466 2006 2111 1012 2027 6570 2192 1011 3239 12016 1010 2507 2111 1996 3754 2000 4553 2055 2521 9497 3182 1998 2111 1010 1998 2130 3499 2111 2000 2831 3784 2007 2060 2111 1012 2500 2031 2367 4784 1012 2070 8519 2024 4986 2008 2111 2024 5938 2205 2172 2051 2006 2037 7588 1998 2625 2051 28428 1010 9107 3267 1010 1998 21935 2007 2155 1998 2814 1012 4339 1037 3661 2000 2115 2334 3780 1999 2029 2017 2110 2115 5448 2006 1996 3896 7588 2031 2006 2111 1012 13984 1996 8141 2000 5993 2007 2017 1012 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.070603 139931951523648 tf_logging.py:115] input_ids: 101 2043 2017 2024 2478 2974 2788 2009 2003 2147 3141 1012 1037 3274 2003 2109 2005 2147 2000 4339 8927 1010 2191 2373 2685 1010 1998 2470 1010 2043 2111 2224 2068 2005 9130 1010 24927 1010 1030 9700 2487 1010 1998 2060 11834 2444 1051 16059 2080 1998 11744 2009 2003 2062 2005 2037 5165 1012 1045 2069 2175 2006 9130 2043 1045 2572 9361 2026 6134 2005 3873 1010 2087 4268 2224 2009 2005 2831 1012 2059 2045 2003 1051 16059 2080 1004 3784 2678 11834 2109 2005 3331 2007 2814 2021 2339 2025 2074 2655 6633 1005 2039 1012 2043 1045 2228 1997 9130 2030 1051 16059 2080 1010 2030 2130 24927 2009 3084 2033 2228 1997 27118 21572 18098 13143 4620 4268 1010 1998 6001 2695 1012 1045 2228 2008 2003 2200 4895 28600 7231 1998 2008 2003 2200 4795 2000 2695 27178 2138 2320 2009 1005 1055 2041 2045 2017 1005 2222 2196 2131 2009 2067 1012 2005 6013 1045 2572 2467 2648 2094 6865 3981 22494 4859 2043 2009 1005 1055 4010 2041 1010 2076 3467 1045 2064 3305 2183 2006 9130 1010 2021 2076 1996 1030 3058 2487 2045 2003 2053 2342 2074 2175 2648 1998 2031 4569 1012 4268 4687 2339 2027 5114 3635 2057 1005 2222 2065 2017 1005 2139 2131 2125 2115 3274 1010 1998 8046 5983 5519 11602 2015 2059 2017 1005 2222 5060 2017 2074 2031 2000 2022 3161 1012 2070 4268 3008 2123 1005 1056 2428 2507 1037 3909 6904 6392 17139 2571 2054 2037 4268 2079 2035 2154 1012 2027 2123 1005 1056 2729 2065 2037 4845 2003 1030 16371 2213 2487 20702 1012 2012 1996 2287 1997 1030 16371 2213 2475 1012 2027 2123 1005 1056 2729 2065 2037 4268 2024 3564 2006 1996 3274 5983 2035 1011 2154 1012 2065 1045 2071 2425 2017 2054 2009 2442 2514 2066 2000 2022 1037 6411 14557 1045 2052 2021 1045 2196 2175 2006 1996 3274 4983 2009 2003 2005 19453 1012 1045 2109 1996 3274 2074 1030 2051 2487 1045 21189 2039 2035 1997 2026 19453 3272 2005 2026 8785 1012 1045 2134 1005 1056 2377 2028 2208 1010 1045 2134 1005 1056 4638 10373 2175 2006 1996 4274 2030 2505 1045 2074 2499 2026 18263 1998 2288 2009 2589 1012 1045 16519 2023 2007 1996 2755 2008 2478 1037 3274 2003 2025 1037 2919 2518 1010 2478 2009 8385 1998 2025 28428 2003 1012 1037 3274 5807 1005 1056 2342 2000 2022 2109 2005 5165 2009 1005 1055 2025 2580 2000 2079 2061 2009 1005 1055 2580 2000 2562 2115 2166 16325 1012 2111 3477 2769 2000 4965 1037 6450 3274 1998 9587 18532 1998 2035 2008 6450 10660 16222 2229 18909 1010 2000 2191 2037 3268 2172 16325 2025 5973 1999 1996 2015 9491 2106 1045 2360 2000 2377 2399 2138 2009 1005 1055 2025 1012 2652 2399 2064 2022 2589 2648 1010 2009 2987 1005 1056 2031 2000 2022 2503 1010 1998 13366 14776 2135 2987 1005 1056 2031 2000 2022 2006 1037 3274 1012 2000 2191 3071 2015 2166 6082 4268 1010 2131 102 2062 1998 2062 2111 2224 7588 1010 2021 2025 3071 10217 2008 2023 6666 2554 1012 2216 2040 2490 9849 1999 2974 2903 2008 7588 2031 1037 3893 3466 2006 2111 1012 2027 6570 2192 1011 3239 12016 1010 2507 2111 1996 3754 2000 4553 2055 2521 9497 3182 1998 2111 1010 1998 2130 3499 2111 2000 2831 3784 2007 2060 2111 1012 2500 2031 2367 4784 1012 2070 8519 2024 4986 2008 2111 2024 5938 2205 2172 2051 2006 2037 7588 1998 2625 2051 28428 1010 9107 3267 1010 1998 21935 2007 2155 1998 2814 1012 4339 1037 3661 2000 2115 2334 3780 1999 2029 2017 2110 2115 5448 2006 1996 3896 7588 2031 2006 2111 1012 13984 1996 8141 2000 5993 2007 2017 1012 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.073677 139931951523648 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.075047 139931951523648 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 8 (id = 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.077980 139931951523648 tf_logging.py:115] label: 8 (id = 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.132437 139931951523648 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.138787 139931951523648 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] the builders of the @ organization ##1 thought that they had a great idea and that they were pioneers . they soon realized that there were some problems with their ideas . one major problem was safety . as noted in paragraph @ nu ##m ##1 , most @ location ##3 dir ##ig ##ible ##s used hydrogen , which was highly fl ##am ##mable . new york is one of the biggest cities in the united states , and it was far too densely populated to take that risk . another problem was wind . at those heights , the wind currents were unpredictable and winds reached high speed . if the dir ##ig ##ible were to be blown around while in the downtown area and were to hit a spire of the building , there would certainly be quite a few casualties . after the risks and rewards were balanced , they realized it just wasn ' t worth it . [SEP] the moor ##ing mast by marcia amid ##on lust ##ed . when the empire state building was conceived , it was planned as the world ’ s tallest building , taller even than the new chrysler building that was being constructed at forty - second street and lexington avenue in new york . at seventy - seven stories , it was the tallest building before the empire state began construction , and al smith was determined to outs ##trip it in height . the architect building the chrysler building , however , had a trick up his sleeve . he secretly constructed a 185 - foot spire inside the building , and then shocked the public and the media by ho ##ist ##ing it up to the top of the chrysler building , bringing it to a height of 1 , 04 ##6 feet , 46 feet taller than the originally announced height of the empire state building . al smith realized that he was close to losing the title of world ’ s tallest building , and on december 11 , 1929 , he announced that the empire state would now reach the height of 1 , 250 feet . he would add a top or a hat to the building that would be even more distinctive than any other building in the city . john tau ##rana ##c describes the plan : [ the top of the empire state building ] would be more than ornamental , more than a spire or dome or a pyramid put there to add a desired few feet to the height of the building or to mask something as mundane as a water tank . their top , they said , would serve a higher calling . the empire state building would be equipped for an age of transportation that was then only the dream of aviation pioneers . this dream of the aviation pioneers was travel by dir ##ig ##ible , or zeppelin , and the empire state building was going to have a moor ##ing mast at its top for docking these new airship ##s , which would accommodate passengers on already existing transatlantic routes and new routes that were yet to come . the age of dir ##ig ##ible ##s . by the 1920s , dir ##ig ##ible ##s were being hailed as the transportation of the future . also known today as b ##lim ##ps , dir ##ig ##ible ##s were actually enormous steel - framed balloons , with envelope ##s of cotton fabric filled with hydrogen and helium to make them lighter than air . unlike a balloon , a dir ##ig ##ible could be maneuver ##ed by the use of propellers and rudder ##s , and passengers could ride in the go ##ndo ##la , or enclosed compartment , under the balloon . dir ##ig ##ible ##s had a top speed [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.142421 139931951523648 tf_logging.py:115] tokens: [CLS] the builders of the @ organization ##1 thought that they had a great idea and that they were pioneers . they soon realized that there were some problems with their ideas . one major problem was safety . as noted in paragraph @ nu ##m ##1 , most @ location ##3 dir ##ig ##ible ##s used hydrogen , which was highly fl ##am ##mable . new york is one of the biggest cities in the united states , and it was far too densely populated to take that risk . another problem was wind . at those heights , the wind currents were unpredictable and winds reached high speed . if the dir ##ig ##ible were to be blown around while in the downtown area and were to hit a spire of the building , there would certainly be quite a few casualties . after the risks and rewards were balanced , they realized it just wasn ' t worth it . [SEP] the moor ##ing mast by marcia amid ##on lust ##ed . when the empire state building was conceived , it was planned as the world ’ s tallest building , taller even than the new chrysler building that was being constructed at forty - second street and lexington avenue in new york . at seventy - seven stories , it was the tallest building before the empire state began construction , and al smith was determined to outs ##trip it in height . the architect building the chrysler building , however , had a trick up his sleeve . he secretly constructed a 185 - foot spire inside the building , and then shocked the public and the media by ho ##ist ##ing it up to the top of the chrysler building , bringing it to a height of 1 , 04 ##6 feet , 46 feet taller than the originally announced height of the empire state building . al smith realized that he was close to losing the title of world ’ s tallest building , and on december 11 , 1929 , he announced that the empire state would now reach the height of 1 , 250 feet . he would add a top or a hat to the building that would be even more distinctive than any other building in the city . john tau ##rana ##c describes the plan : [ the top of the empire state building ] would be more than ornamental , more than a spire or dome or a pyramid put there to add a desired few feet to the height of the building or to mask something as mundane as a water tank . their top , they said , would serve a higher calling . the empire state building would be equipped for an age of transportation that was then only the dream of aviation pioneers . this dream of the aviation pioneers was travel by dir ##ig ##ible , or zeppelin , and the empire state building was going to have a moor ##ing mast at its top for docking these new airship ##s , which would accommodate passengers on already existing transatlantic routes and new routes that were yet to come . the age of dir ##ig ##ible ##s . by the 1920s , dir ##ig ##ible ##s were being hailed as the transportation of the future . also known today as b ##lim ##ps , dir ##ig ##ible ##s were actually enormous steel - framed balloons , with envelope ##s of cotton fabric filled with hydrogen and helium to make them lighter than air . unlike a balloon , a dir ##ig ##ible could be maneuver ##ed by the use of propellers and rudder ##s , and passengers could ride in the go ##ndo ##la , or enclosed compartment , under the balloon . dir ##ig ##ible ##s had a top speed [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1996 16472 1997 1996 1030 3029 2487 2245 2008 2027 2018 1037 2307 2801 1998 2008 2027 2020 13200 1012 2027 2574 3651 2008 2045 2020 2070 3471 2007 2037 4784 1012 2028 2350 3291 2001 3808 1012 2004 3264 1999 20423 1030 16371 2213 2487 1010 2087 1030 3295 2509 16101 8004 7028 2015 2109 9732 1010 2029 2001 3811 13109 3286 24088 1012 2047 2259 2003 2028 1997 1996 5221 3655 1999 1996 2142 2163 1010 1998 2009 2001 2521 2205 19441 10357 2000 2202 2008 3891 1012 2178 3291 2001 3612 1012 2012 2216 7535 1010 1996 3612 14731 2020 21446 1998 7266 2584 2152 3177 1012 2065 1996 16101 8004 7028 2020 2000 2022 10676 2105 2096 1999 1996 5116 2181 1998 2020 2000 2718 1037 19823 1997 1996 2311 1010 2045 2052 5121 2022 3243 1037 2261 8664 1012 2044 1996 10831 1998 19054 2020 12042 1010 2027 3651 2009 2074 2347 1005 1056 4276 2009 1012 102 1996 16808 2075 15429 2011 22548 13463 2239 11516 2098 1012 2043 1996 3400 2110 2311 2001 10141 1010 2009 2001 3740 2004 1996 2088 1521 1055 13747 2311 1010 12283 2130 2084 1996 2047 17714 2311 2008 2001 2108 3833 2012 5659 1011 2117 2395 1998 14521 3927 1999 2047 2259 1012 2012 10920 1011 2698 3441 1010 2009 2001 1996 13747 2311 2077 1996 3400 2110 2211 2810 1010 1998 2632 3044 2001 4340 2000 21100 24901 2009 1999 4578 1012 1996 4944 2311 1996 17714 2311 1010 2174 1010 2018 1037 7577 2039 2010 10353 1012 2002 10082 3833 1037 15376 1011 3329 19823 2503 1996 2311 1010 1998 2059 7135 1996 2270 1998 1996 2865 2011 7570 2923 2075 2009 2039 2000 1996 2327 1997 1996 17714 2311 1010 5026 2009 2000 1037 4578 1997 1015 1010 5840 2575 2519 1010 4805 2519 12283 2084 1996 2761 2623 4578 1997 1996 3400 2110 2311 1012 2632 3044 3651 2008 2002 2001 2485 2000 3974 1996 2516 1997 2088 1521 1055 13747 2311 1010 1998 2006 2285 2340 1010 4612 1010 2002 2623 2008 1996 3400 2110 2052 2085 3362 1996 4578 1997 1015 1010 5539 2519 1012 2002 2052 5587 1037 2327 2030 1037 6045 2000 1996 2311 2008 2052 2022 2130 2062 8200 2084 2151 2060 2311 1999 1996 2103 1012 2198 19982 16737 2278 5577 1996 2933 1024 1031 1996 2327 1997 1996 3400 2110 2311 1033 2052 2022 2062 2084 18200 1010 2062 2084 1037 19823 2030 8514 2030 1037 11918 2404 2045 2000 5587 1037 9059 2261 2519 2000 1996 4578 1997 1996 2311 2030 2000 7308 2242 2004 24684 2004 1037 2300 4951 1012 2037 2327 1010 2027 2056 1010 2052 3710 1037 3020 4214 1012 1996 3400 2110 2311 2052 2022 6055 2005 2019 2287 1997 5193 2008 2001 2059 2069 1996 3959 1997 5734 13200 1012 2023 3959 1997 1996 5734 13200 2001 3604 2011 16101 8004 7028 1010 2030 22116 1010 1998 1996 3400 2110 2311 2001 2183 2000 2031 1037 16808 2075 15429 2012 2049 2327 2005 25776 2122 2047 27636 2015 1010 2029 2052 8752 5467 2006 2525 4493 26617 5847 1998 2047 5847 2008 2020 2664 2000 2272 1012 1996 2287 1997 16101 8004 7028 2015 1012 2011 1996 6641 1010 16101 8004 7028 2015 2020 2108 16586 2004 1996 5193 1997 1996 2925 1012 2036 2124 2651 2004 1038 17960 4523 1010 16101 8004 7028 2015 2020 2941 8216 3886 1011 10366 22163 1010 2007 11255 2015 1997 6557 8313 3561 2007 9732 1998 22764 2000 2191 2068 9442 2084 2250 1012 4406 1037 13212 1010 1037 16101 8004 7028 2071 2022 17519 2098 2011 1996 2224 1997 23405 1998 24049 2015 1010 1998 5467 2071 4536 1999 1996 2175 15482 2721 1010 2030 10837 15273 1010 2104 1996 13212 1012 16101 8004 7028 2015 2018 1037 2327 3177 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.144423 139931951523648 tf_logging.py:115] input_ids: 101 1996 16472 1997 1996 1030 3029 2487 2245 2008 2027 2018 1037 2307 2801 1998 2008 2027 2020 13200 1012 2027 2574 3651 2008 2045 2020 2070 3471 2007 2037 4784 1012 2028 2350 3291 2001 3808 1012 2004 3264 1999 20423 1030 16371 2213 2487 1010 2087 1030 3295 2509 16101 8004 7028 2015 2109 9732 1010 2029 2001 3811 13109 3286 24088 1012 2047 2259 2003 2028 1997 1996 5221 3655 1999 1996 2142 2163 1010 1998 2009 2001 2521 2205 19441 10357 2000 2202 2008 3891 1012 2178 3291 2001 3612 1012 2012 2216 7535 1010 1996 3612 14731 2020 21446 1998 7266 2584 2152 3177 1012 2065 1996 16101 8004 7028 2020 2000 2022 10676 2105 2096 1999 1996 5116 2181 1998 2020 2000 2718 1037 19823 1997 1996 2311 1010 2045 2052 5121 2022 3243 1037 2261 8664 1012 2044 1996 10831 1998 19054 2020 12042 1010 2027 3651 2009 2074 2347 1005 1056 4276 2009 1012 102 1996 16808 2075 15429 2011 22548 13463 2239 11516 2098 1012 2043 1996 3400 2110 2311 2001 10141 1010 2009 2001 3740 2004 1996 2088 1521 1055 13747 2311 1010 12283 2130 2084 1996 2047 17714 2311 2008 2001 2108 3833 2012 5659 1011 2117 2395 1998 14521 3927 1999 2047 2259 1012 2012 10920 1011 2698 3441 1010 2009 2001 1996 13747 2311 2077 1996 3400 2110 2211 2810 1010 1998 2632 3044 2001 4340 2000 21100 24901 2009 1999 4578 1012 1996 4944 2311 1996 17714 2311 1010 2174 1010 2018 1037 7577 2039 2010 10353 1012 2002 10082 3833 1037 15376 1011 3329 19823 2503 1996 2311 1010 1998 2059 7135 1996 2270 1998 1996 2865 2011 7570 2923 2075 2009 2039 2000 1996 2327 1997 1996 17714 2311 1010 5026 2009 2000 1037 4578 1997 1015 1010 5840 2575 2519 1010 4805 2519 12283 2084 1996 2761 2623 4578 1997 1996 3400 2110 2311 1012 2632 3044 3651 2008 2002 2001 2485 2000 3974 1996 2516 1997 2088 1521 1055 13747 2311 1010 1998 2006 2285 2340 1010 4612 1010 2002 2623 2008 1996 3400 2110 2052 2085 3362 1996 4578 1997 1015 1010 5539 2519 1012 2002 2052 5587 1037 2327 2030 1037 6045 2000 1996 2311 2008 2052 2022 2130 2062 8200 2084 2151 2060 2311 1999 1996 2103 1012 2198 19982 16737 2278 5577 1996 2933 1024 1031 1996 2327 1997 1996 3400 2110 2311 1033 2052 2022 2062 2084 18200 1010 2062 2084 1037 19823 2030 8514 2030 1037 11918 2404 2045 2000 5587 1037 9059 2261 2519 2000 1996 4578 1997 1996 2311 2030 2000 7308 2242 2004 24684 2004 1037 2300 4951 1012 2037 2327 1010 2027 2056 1010 2052 3710 1037 3020 4214 1012 1996 3400 2110 2311 2052 2022 6055 2005 2019 2287 1997 5193 2008 2001 2059 2069 1996 3959 1997 5734 13200 1012 2023 3959 1997 1996 5734 13200 2001 3604 2011 16101 8004 7028 1010 2030 22116 1010 1998 1996 3400 2110 2311 2001 2183 2000 2031 1037 16808 2075 15429 2012 2049 2327 2005 25776 2122 2047 27636 2015 1010 2029 2052 8752 5467 2006 2525 4493 26617 5847 1998 2047 5847 2008 2020 2664 2000 2272 1012 1996 2287 1997 16101 8004 7028 2015 1012 2011 1996 6641 1010 16101 8004 7028 2015 2020 2108 16586 2004 1996 5193 1997 1996 2925 1012 2036 2124 2651 2004 1038 17960 4523 1010 16101 8004 7028 2015 2020 2941 8216 3886 1011 10366 22163 1010 2007 11255 2015 1997 6557 8313 3561 2007 9732 1998 22764 2000 2191 2068 9442 2084 2250 1012 4406 1037 13212 1010 1037 16101 8004 7028 2071 2022 17519 2098 2011 1996 2224 1997 23405 1998 24049 2015 1010 1998 5467 2071 4536 1999 1996 2175 15482 2721 1010 2030 10837 15273 1010 2104 1996 13212 1012 16101 8004 7028 2015 2018 1037 2327 3177 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.147367 139931951523648 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.148771 139931951523648 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 8 (id = 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.152224 139931951523648 tf_logging.py:115] label: 8 (id = 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.178014 139931951523648 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.181623 139931951523648 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] i have had many great times with my grandpa . he is a very funny and loving man . when my cousin , my grandpa and i get together for a camping trip there is trouble coming no matter what . a few years ago he took me and my cousin on a two day camping trip along the @ organization ##1 . the first campground we arrived at had no open sites so we moved on to the next campground . @ caps ##1 campground had tons of open camps ##ites so we picked the best one we could find and stayed there for the first @ time ##1 . later that evening we learned why there was no one at the campground . the reason was that there was a ra ##co ##on problem a few days before and people were scared to stay in the campground for fear of attack or having their food stolen . we were lucky and never saw the ra ##co ##on and we never lost any of our food . the next @ time ##2 on the way out we saw a sign that read \" caution ra ##co ##on problem . \" @ caps ##1 made us laugh all the way to @ location ##1 , which is a small town on the @ organization ##1 . once in @ location ##1 we stopped for gas and more food to last through the @ time ##1 . while at the gas station we had little bit of an issue but it was very funny . a young blond woman came out to help us and she wasn ' t the smart ##est person i have met . she charged us three times the amount of gasoline than we bought and couldn ' t tell us where the nearest grocery store was . well , as my grandpa is rolling up his window he mutter ##s quite loudly \" dumb blonde \" , while the woman was standing right there . she didn ##t say a word and my gan ##dp ##a drove off with me and cousin laughing so hard we could barely breath . my grandpa asked us why we were laughing so hard , so we told him what the circumstances were when he said dumb blonde . he was so oblivious to the situation that he didn ' t notice what came out of his mouth at the time . what even made it better was that he drove off like nothing had happened . the next @ time ##1 we stayed at a di ##ifer ##ent campground with no ra ##co ##ons . still the @ time ##1 was filled with laughter . we were roast ##ing marsh ##mel ##low ##s over the fire after our dinner and for some odd reason my grandpa could not keep the marsh ##mel ##low on his stick . he wasted about six marsh ##mel ##low before he even got one that he could eat . and every ##time he dropped one into the fire it got more and more funny . we laughed and laughed until our stomach hurt . the best part was the ##at my grandpa didn ' t think it was funny so he just looked at me and my cousin like we were stupid or something . in the tent that @ time ##1 we were so tired everything seemed funny to us . we laughed at each other until two in the @ time ##2 . that ##s when [SEP] we all understand the benefits of laughter . for example , someone once said , “ laughter is the shortest distance between two people . ” many other people believe that laughter is an important part of any relationship . tell a true story in which laughter was one element or part . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.183356 139931951523648 tf_logging.py:115] tokens: [CLS] i have had many great times with my grandpa . he is a very funny and loving man . when my cousin , my grandpa and i get together for a camping trip there is trouble coming no matter what . a few years ago he took me and my cousin on a two day camping trip along the @ organization ##1 . the first campground we arrived at had no open sites so we moved on to the next campground . @ caps ##1 campground had tons of open camps ##ites so we picked the best one we could find and stayed there for the first @ time ##1 . later that evening we learned why there was no one at the campground . the reason was that there was a ra ##co ##on problem a few days before and people were scared to stay in the campground for fear of attack or having their food stolen . we were lucky and never saw the ra ##co ##on and we never lost any of our food . the next @ time ##2 on the way out we saw a sign that read \" caution ra ##co ##on problem . \" @ caps ##1 made us laugh all the way to @ location ##1 , which is a small town on the @ organization ##1 . once in @ location ##1 we stopped for gas and more food to last through the @ time ##1 . while at the gas station we had little bit of an issue but it was very funny . a young blond woman came out to help us and she wasn ' t the smart ##est person i have met . she charged us three times the amount of gasoline than we bought and couldn ' t tell us where the nearest grocery store was . well , as my grandpa is rolling up his window he mutter ##s quite loudly \" dumb blonde \" , while the woman was standing right there . she didn ##t say a word and my gan ##dp ##a drove off with me and cousin laughing so hard we could barely breath . my grandpa asked us why we were laughing so hard , so we told him what the circumstances were when he said dumb blonde . he was so oblivious to the situation that he didn ' t notice what came out of his mouth at the time . what even made it better was that he drove off like nothing had happened . the next @ time ##1 we stayed at a di ##ifer ##ent campground with no ra ##co ##ons . still the @ time ##1 was filled with laughter . we were roast ##ing marsh ##mel ##low ##s over the fire after our dinner and for some odd reason my grandpa could not keep the marsh ##mel ##low on his stick . he wasted about six marsh ##mel ##low before he even got one that he could eat . and every ##time he dropped one into the fire it got more and more funny . we laughed and laughed until our stomach hurt . the best part was the ##at my grandpa didn ' t think it was funny so he just looked at me and my cousin like we were stupid or something . in the tent that @ time ##1 we were so tired everything seemed funny to us . we laughed at each other until two in the @ time ##2 . that ##s when [SEP] we all understand the benefits of laughter . for example , someone once said , “ laughter is the shortest distance between two people . ” many other people believe that laughter is an important part of any relationship . tell a true story in which laughter was one element or part . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 2031 2018 2116 2307 2335 2007 2026 15310 1012 2002 2003 1037 2200 6057 1998 8295 2158 1012 2043 2026 5542 1010 2026 15310 1998 1045 2131 2362 2005 1037 13215 4440 2045 2003 4390 2746 2053 3043 2054 1012 1037 2261 2086 3283 2002 2165 2033 1998 2026 5542 2006 1037 2048 2154 13215 4440 2247 1996 1030 3029 2487 1012 1996 2034 29144 2057 3369 2012 2018 2053 2330 4573 2061 2057 2333 2006 2000 1996 2279 29144 1012 1030 9700 2487 29144 2018 6197 1997 2330 7958 7616 2061 2057 3856 1996 2190 2028 2057 2071 2424 1998 4370 2045 2005 1996 2034 1030 2051 2487 1012 2101 2008 3944 2057 4342 2339 2045 2001 2053 2028 2012 1996 29144 1012 1996 3114 2001 2008 2045 2001 1037 10958 3597 2239 3291 1037 2261 2420 2077 1998 2111 2020 6015 2000 2994 1999 1996 29144 2005 3571 1997 2886 2030 2383 2037 2833 7376 1012 2057 2020 5341 1998 2196 2387 1996 10958 3597 2239 1998 2057 2196 2439 2151 1997 2256 2833 1012 1996 2279 1030 2051 2475 2006 1996 2126 2041 2057 2387 1037 3696 2008 3191 1000 14046 10958 3597 2239 3291 1012 1000 1030 9700 2487 2081 2149 4756 2035 1996 2126 2000 1030 3295 2487 1010 2029 2003 1037 2235 2237 2006 1996 1030 3029 2487 1012 2320 1999 1030 3295 2487 2057 3030 2005 3806 1998 2062 2833 2000 2197 2083 1996 1030 2051 2487 1012 2096 2012 1996 3806 2276 2057 2018 2210 2978 1997 2019 3277 2021 2009 2001 2200 6057 1012 1037 2402 8855 2450 2234 2041 2000 2393 2149 1998 2016 2347 1005 1056 1996 6047 4355 2711 1045 2031 2777 1012 2016 5338 2149 2093 2335 1996 3815 1997 13753 2084 2057 4149 1998 2481 1005 1056 2425 2149 2073 1996 7205 13025 3573 2001 1012 2092 1010 2004 2026 15310 2003 5291 2039 2010 3332 2002 23457 2015 3243 9928 1000 12873 9081 1000 1010 2096 1996 2450 2001 3061 2157 2045 1012 2016 2134 2102 2360 1037 2773 1998 2026 25957 18927 2050 5225 2125 2007 2033 1998 5542 5870 2061 2524 2057 2071 4510 3052 1012 2026 15310 2356 2149 2339 2057 2020 5870 2061 2524 1010 2061 2057 2409 2032 2054 1996 6214 2020 2043 2002 2056 12873 9081 1012 2002 2001 2061 18333 2000 1996 3663 2008 2002 2134 1005 1056 5060 2054 2234 2041 1997 2010 2677 2012 1996 2051 1012 2054 2130 2081 2009 2488 2001 2008 2002 5225 2125 2066 2498 2018 3047 1012 1996 2279 1030 2051 2487 2057 4370 2012 1037 4487 23780 4765 29144 2007 2053 10958 3597 5644 1012 2145 1996 1030 2051 2487 2001 3561 2007 7239 1012 2057 2020 25043 2075 9409 10199 8261 2015 2058 1996 2543 2044 2256 4596 1998 2005 2070 5976 3114 2026 15310 2071 2025 2562 1996 9409 10199 8261 2006 2010 6293 1012 2002 13842 2055 2416 9409 10199 8261 2077 2002 2130 2288 2028 2008 2002 2071 4521 1012 1998 2296 7292 2002 3333 2028 2046 1996 2543 2009 2288 2062 1998 2062 6057 1012 2057 4191 1998 4191 2127 2256 4308 3480 1012 1996 2190 2112 2001 1996 4017 2026 15310 2134 1005 1056 2228 2009 2001 6057 2061 2002 2074 2246 2012 2033 1998 2026 5542 2066 2057 2020 5236 2030 2242 1012 1999 1996 9311 2008 1030 2051 2487 2057 2020 2061 5458 2673 2790 6057 2000 2149 1012 2057 4191 2012 2169 2060 2127 2048 1999 1996 1030 2051 2475 1012 2008 2015 2043 102 2057 2035 3305 1996 6666 1997 7239 1012 2005 2742 1010 2619 2320 2056 1010 1523 7239 2003 1996 20047 3292 2090 2048 2111 1012 1524 2116 2060 2111 2903 2008 7239 2003 2019 2590 2112 1997 2151 3276 1012 2425 1037 2995 2466 1999 2029 7239 2001 2028 5783 2030 2112 1012 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.187426 139931951523648 tf_logging.py:115] input_ids: 101 1045 2031 2018 2116 2307 2335 2007 2026 15310 1012 2002 2003 1037 2200 6057 1998 8295 2158 1012 2043 2026 5542 1010 2026 15310 1998 1045 2131 2362 2005 1037 13215 4440 2045 2003 4390 2746 2053 3043 2054 1012 1037 2261 2086 3283 2002 2165 2033 1998 2026 5542 2006 1037 2048 2154 13215 4440 2247 1996 1030 3029 2487 1012 1996 2034 29144 2057 3369 2012 2018 2053 2330 4573 2061 2057 2333 2006 2000 1996 2279 29144 1012 1030 9700 2487 29144 2018 6197 1997 2330 7958 7616 2061 2057 3856 1996 2190 2028 2057 2071 2424 1998 4370 2045 2005 1996 2034 1030 2051 2487 1012 2101 2008 3944 2057 4342 2339 2045 2001 2053 2028 2012 1996 29144 1012 1996 3114 2001 2008 2045 2001 1037 10958 3597 2239 3291 1037 2261 2420 2077 1998 2111 2020 6015 2000 2994 1999 1996 29144 2005 3571 1997 2886 2030 2383 2037 2833 7376 1012 2057 2020 5341 1998 2196 2387 1996 10958 3597 2239 1998 2057 2196 2439 2151 1997 2256 2833 1012 1996 2279 1030 2051 2475 2006 1996 2126 2041 2057 2387 1037 3696 2008 3191 1000 14046 10958 3597 2239 3291 1012 1000 1030 9700 2487 2081 2149 4756 2035 1996 2126 2000 1030 3295 2487 1010 2029 2003 1037 2235 2237 2006 1996 1030 3029 2487 1012 2320 1999 1030 3295 2487 2057 3030 2005 3806 1998 2062 2833 2000 2197 2083 1996 1030 2051 2487 1012 2096 2012 1996 3806 2276 2057 2018 2210 2978 1997 2019 3277 2021 2009 2001 2200 6057 1012 1037 2402 8855 2450 2234 2041 2000 2393 2149 1998 2016 2347 1005 1056 1996 6047 4355 2711 1045 2031 2777 1012 2016 5338 2149 2093 2335 1996 3815 1997 13753 2084 2057 4149 1998 2481 1005 1056 2425 2149 2073 1996 7205 13025 3573 2001 1012 2092 1010 2004 2026 15310 2003 5291 2039 2010 3332 2002 23457 2015 3243 9928 1000 12873 9081 1000 1010 2096 1996 2450 2001 3061 2157 2045 1012 2016 2134 2102 2360 1037 2773 1998 2026 25957 18927 2050 5225 2125 2007 2033 1998 5542 5870 2061 2524 2057 2071 4510 3052 1012 2026 15310 2356 2149 2339 2057 2020 5870 2061 2524 1010 2061 2057 2409 2032 2054 1996 6214 2020 2043 2002 2056 12873 9081 1012 2002 2001 2061 18333 2000 1996 3663 2008 2002 2134 1005 1056 5060 2054 2234 2041 1997 2010 2677 2012 1996 2051 1012 2054 2130 2081 2009 2488 2001 2008 2002 5225 2125 2066 2498 2018 3047 1012 1996 2279 1030 2051 2487 2057 4370 2012 1037 4487 23780 4765 29144 2007 2053 10958 3597 5644 1012 2145 1996 1030 2051 2487 2001 3561 2007 7239 1012 2057 2020 25043 2075 9409 10199 8261 2015 2058 1996 2543 2044 2256 4596 1998 2005 2070 5976 3114 2026 15310 2071 2025 2562 1996 9409 10199 8261 2006 2010 6293 1012 2002 13842 2055 2416 9409 10199 8261 2077 2002 2130 2288 2028 2008 2002 2071 4521 1012 1998 2296 7292 2002 3333 2028 2046 1996 2543 2009 2288 2062 1998 2062 6057 1012 2057 4191 1998 4191 2127 2256 4308 3480 1012 1996 2190 2112 2001 1996 4017 2026 15310 2134 1005 1056 2228 2009 2001 6057 2061 2002 2074 2246 2012 2033 1998 2026 5542 2066 2057 2020 5236 2030 2242 1012 1999 1996 9311 2008 1030 2051 2487 2057 2020 2061 5458 2673 2790 6057 2000 2149 1012 2057 4191 2012 2169 2060 2127 2048 1999 1996 1030 2051 2475 1012 2008 2015 2043 102 2057 2035 3305 1996 6666 1997 7239 1012 2005 2742 1010 2619 2320 2056 1010 1523 7239 2003 1996 20047 3292 2090 2048 2111 1012 1524 2116 2060 2111 2903 2008 7239 2003 2019 2590 2112 1997 2151 3276 1012 2425 1037 2995 2466 1999 2029 7239 2001 2028 5783 2030 2112 1012 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.189250 139931951523648 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.190671 139931951523648 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 7 (id = 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.194042 139931951523648 tf_logging.py:115] label: 7 (id = 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.202929 139931951523648 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.207342 139931951523648 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pg @ nu ##m ##1 : @ caps ##1 her crying ! i just wanted to get out of the car and walk home . there were @ nu ##m ##2 of us in a suburban that were all sm ##ush ##ed together ! so we couldn ##t even more our arms i felt like i was locked in jail with chains on me . it was @ nu ##m ##3 o ##cl ##ock in the morning and i saw a big sign that read welcome to @ caps ##2 . so at that point we only had a hour or so left i was so excited . we finally made it to our destination key wrap . it was the longest trip of my life ! but i try ##ed my hardest to stay patient and wait . but in the long run i realized i didn ##t are and i had a whole weekend of waves and a temperature of @ nu ##m ##4 ##° @ caps ##3 degrees . when i thought about how long it was to get here i didn ##t even want to think about how long it is going to take to get home ! but how i know i have the patients to go home ! [SEP] write about patience . being patient means that you are understanding and tolerant . a patient person experience difficulties without complaining . do only one of the following : write a story about a time when you were patient or write a story about a time when someone you know was patient or write a story in your own way about patience . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.208878 139931951523648 tf_logging.py:115] tokens: [CLS] pg @ nu ##m ##1 : @ caps ##1 her crying ! i just wanted to get out of the car and walk home . there were @ nu ##m ##2 of us in a suburban that were all sm ##ush ##ed together ! so we couldn ##t even more our arms i felt like i was locked in jail with chains on me . it was @ nu ##m ##3 o ##cl ##ock in the morning and i saw a big sign that read welcome to @ caps ##2 . so at that point we only had a hour or so left i was so excited . we finally made it to our destination key wrap . it was the longest trip of my life ! but i try ##ed my hardest to stay patient and wait . but in the long run i realized i didn ##t are and i had a whole weekend of waves and a temperature of @ nu ##m ##4 ##° @ caps ##3 degrees . when i thought about how long it was to get here i didn ##t even want to think about how long it is going to take to get home ! but how i know i have the patients to go home ! [SEP] write about patience . being patient means that you are understanding and tolerant . a patient person experience difficulties without complaining . do only one of the following : write a story about a time when you were patient or write a story about a time when someone you know was patient or write a story in your own way about patience . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 18720 1030 16371 2213 2487 1024 1030 9700 2487 2014 6933 999 1045 2074 2359 2000 2131 2041 1997 1996 2482 1998 3328 2188 1012 2045 2020 1030 16371 2213 2475 1997 2149 1999 1037 9282 2008 2020 2035 15488 20668 2098 2362 999 2061 2057 2481 2102 2130 2062 2256 2608 1045 2371 2066 1045 2001 5299 1999 7173 2007 8859 2006 2033 1012 2009 2001 1030 16371 2213 2509 1051 20464 7432 1999 1996 2851 1998 1045 2387 1037 2502 3696 2008 3191 6160 2000 1030 9700 2475 1012 2061 2012 2008 2391 2057 2069 2018 1037 3178 2030 2061 2187 1045 2001 2061 7568 1012 2057 2633 2081 2009 2000 2256 7688 3145 10236 1012 2009 2001 1996 6493 4440 1997 2026 2166 999 2021 1045 3046 2098 2026 18263 2000 2994 5776 1998 3524 1012 2021 1999 1996 2146 2448 1045 3651 1045 2134 2102 2024 1998 1045 2018 1037 2878 5353 1997 5975 1998 1037 4860 1997 1030 16371 2213 2549 7737 1030 9700 2509 5445 1012 2043 1045 2245 2055 2129 2146 2009 2001 2000 2131 2182 1045 2134 2102 2130 2215 2000 2228 2055 2129 2146 2009 2003 2183 2000 2202 2000 2131 2188 999 2021 2129 1045 2113 1045 2031 1996 5022 2000 2175 2188 999 102 4339 2055 11752 1012 2108 5776 2965 2008 2017 2024 4824 1998 23691 1012 1037 5776 2711 3325 8190 2302 17949 1012 2079 2069 2028 1997 1996 2206 1024 4339 1037 2466 2055 1037 2051 2043 2017 2020 5776 2030 4339 1037 2466 2055 1037 2051 2043 2619 2017 2113 2001 5776 2030 4339 1037 2466 1999 2115 2219 2126 2055 11752 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.212539 139931951523648 tf_logging.py:115] input_ids: 101 18720 1030 16371 2213 2487 1024 1030 9700 2487 2014 6933 999 1045 2074 2359 2000 2131 2041 1997 1996 2482 1998 3328 2188 1012 2045 2020 1030 16371 2213 2475 1997 2149 1999 1037 9282 2008 2020 2035 15488 20668 2098 2362 999 2061 2057 2481 2102 2130 2062 2256 2608 1045 2371 2066 1045 2001 5299 1999 7173 2007 8859 2006 2033 1012 2009 2001 1030 16371 2213 2509 1051 20464 7432 1999 1996 2851 1998 1045 2387 1037 2502 3696 2008 3191 6160 2000 1030 9700 2475 1012 2061 2012 2008 2391 2057 2069 2018 1037 3178 2030 2061 2187 1045 2001 2061 7568 1012 2057 2633 2081 2009 2000 2256 7688 3145 10236 1012 2009 2001 1996 6493 4440 1997 2026 2166 999 2021 1045 3046 2098 2026 18263 2000 2994 5776 1998 3524 1012 2021 1999 1996 2146 2448 1045 3651 1045 2134 2102 2024 1998 1045 2018 1037 2878 5353 1997 5975 1998 1037 4860 1997 1030 16371 2213 2549 7737 1030 9700 2509 5445 1012 2043 1045 2245 2055 2129 2146 2009 2001 2000 2131 2182 1045 2134 2102 2130 2215 2000 2228 2055 2129 2146 2009 2003 2183 2000 2202 2000 2131 2188 999 2021 2129 1045 2113 1045 2031 1996 5022 2000 2175 2188 999 102 4339 2055 11752 1012 2108 5776 2965 2008 2017 2024 4824 1998 23691 1012 1037 5776 2711 3325 8190 2302 17949 1012 2079 2069 2028 1997 1996 2206 1024 4339 1037 2466 2055 1037 2051 2043 2017 2020 5776 2030 4339 1037 2466 2055 1037 2051 2043 2619 2017 2113 2001 5776 2030 4339 1037 2466 1999 2115 2219 2126 2055 11752 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.214119 139931951523648 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.217528 139931951523648 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 8 (id = 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 23:31:22.218674 139931951523648 tf_logging.py:115] label: 8 (id = 8)\n"
     ]
    }
   ],
   "source": [
    "# max_len for essays and max_len_p for prompts have been previously defined\n",
    "\n",
    "# form prompt 1, lable_list is 0-10\n",
    "label_list = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, max_len, bert_tknzr)\n",
    "dev_features = bert.run_classifier.convert_examples_to_features(dev_InputExamples, label_list, max_len, bert_tknzr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9732"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking length of the train_features, this should equal to the number of train_set examples\n",
    "len(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels):\n",
    "    \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "    bert_module = hub.Module(\n",
    "        BERT_MODEL_HUB,\n",
    "        trainable=True)\n",
    "    bert_inputs = dict(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids)\n",
    "    bert_outputs = bert_module(\n",
    "        inputs=bert_inputs,\n",
    "        signature=\"tokens\",\n",
    "        as_dict=True)\n",
    "\n",
    "    # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "    # Use \"sequence_outputs\" for token-level output.\n",
    "    output_layer = bert_outputs[\"pooled_output\"]\n",
    "\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "    # Create our own layer to tune for politeness data.\n",
    "    output_weights = tf.get_variable(\n",
    "        \"output_weights\", [num_labels, hidden_size],\n",
    "        initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "    output_bias = tf.get_variable(\n",
    "        \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "\n",
    "      # Dropout helps prevent overfitting\n",
    "        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "        # Convert labels into one-hot encoding\n",
    "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "        # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "        if is_predicting:\n",
    "            return (predicted_labels, log_probs)\n",
    "\n",
    "        # If we're train/eval, compute loss between predicted and actual label\n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "        return (loss, predicted_labels, log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
