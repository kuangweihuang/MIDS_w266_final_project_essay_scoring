{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W266 Project - Essay Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "\n",
    "The data is obtained from the Automated Student Assessment Prize (ASAP) AES dataset (https://www.kaggle.com/c/asap-aes/data), which contains essays written by students ranging from Grade 7 to Grade 10. The dataset consists of 8 essay sets, each with a different topic or prompt, with a total of 12,978 essays with scores.\n",
    "\n",
    "Each of the sets of essays was generated from a single prompt. Selected essays range from an average length of 150 to 550 words per response. Some of the essays are dependent upon source information and others are not. All responses were written by students ranging in grade levels from Grade 7 to Grade 10. All essays were hand graded and were double-scored. Each of the eight data sets has its own unique characteristics. The variability is intended to test the limits of your scoring engine's capabilities.\n",
    "\n",
    "The training data is provided in three formats: a tab-separated value (TSV) file, a Microsoft Excel 2010 spreadsheet, and a Microsoft Excel 2003 spreadsheet.  The current release of the training data contains essay sets 1-6.  Sets 7-8 will be released on February 10, 2012.  Each of these files contains 28 columns:\n",
    "\n",
    "    essay_id: A unique identifier for each individual student essay\n",
    "    essay_set: 1-8, an id for each set of essays\n",
    "    essay: The ascii text of a student's response\n",
    "    rater1_domain1: Rater 1's domain 1 score; all essays have this\n",
    "    rater2_domain1: Rater 2's domain 1 score; all essays have this\n",
    "    rater3_domain1: Rater 3's domain 1 score; only some essays in set 8 have this.\n",
    "    domain1_score: Resolved score between the raters; all essays have this\n",
    "    rater1_domain2: Rater 1's domain 2 score; only essays in set 2 have this\n",
    "    rater2_domain2: Rater 2's domain 2 score; only essays in set 2 have this\n",
    "    domain2_score: Resolved score between the raters; only essays in set 2 have this\n",
    "    rater1_trait1 score - rater3_trait6 score: trait scores for sets 7-8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up ML libraries\n",
    "\n",
    "Importing the relevant NLP and tensorflow libraries for our use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.000010\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kw.UNLOVEDPC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import datetime\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "start = timer()\n",
    "end = timer()\n",
    "print(timedelta(seconds=end-start))\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Pandas and SKLearn\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, patched_numpy_io\n",
    "#from w266_common import utils, vocabulary, tf_embed_viz\n",
    "\n",
    "# Your code\n",
    "#import rnnlm; reload(rnnlm)\n",
    "#import rnnlm_test; reload(rnnlm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the data\n",
    "\n",
    "Data from AES dataset is stored in the `/data/` folder.  We will begin by loading the training dataset `training_set_rel3.tsv` and partitioning it into train, test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of rows in full data set: 12978\n"
     ]
    }
   ],
   "source": [
    "training_set_rel3_df = pd.read_csv(\"data/training_set_rel3.csv\")\n",
    "#training_set_rel3_df.head()\n",
    "print(\"No. of rows in full data set:\", len(training_set_rel3_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0             4.0             4.0             NaN            8.0   \n",
       "1             5.0             4.0             NaN            9.0   \n",
       "2             4.0             3.0             NaN            7.0   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score      ...        \\\n",
       "0             NaN             NaN            NaN      ...         \n",
       "1             NaN             NaN            NaN      ...         \n",
       "2             NaN             NaN            NaN      ...         \n",
       "\n",
       "   rater2_trait3  rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  \\\n",
       "0            NaN            NaN            NaN            NaN            NaN   \n",
       "1            NaN            NaN            NaN            NaN            NaN   \n",
       "2            NaN            NaN            NaN            NaN            NaN   \n",
       "\n",
       "   rater3_trait2  rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n",
       "0            NaN            NaN            NaN            NaN            NaN  \n",
       "1            NaN            NaN            NaN            NaN            NaN  \n",
       "2            NaN            NaN            NaN            NaN            NaN  \n",
       "\n",
       "[3 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_rel3_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of rows in full data set: 12977\n"
     ]
    }
   ],
   "source": [
    "# Dropping data with no scores in \"domain1_score\"\n",
    "training_set_rel3_df.dropna(axis=0, subset=['domain1_score'], inplace=True)\n",
    "print(\"No. of rows in full data set:\", len(training_set_rel3_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the prompts\n",
    "prompt_dict = {1 : 'More and more people use computers, but not everyone agrees that this benefits society. Those who support advances in technology believe that computers have a positive effect on people. They teach hand-eye coordination, give people the ability to learn about faraway places and people, and even allow people to talk online with other people. Others have different ideas. Some experts are concerned that people are spending too much time on their computers and less time exercising, enjoying nature, and interacting with family and friends. Write a letter to your local newspaper in which you state your opinion on the effects computers have on people. Persuade the readers to agree with you.',\n",
    "               2 : 'Censorship in the Libraries. \"All of us can think of a book that we hope none of our children or any other children have taken off the shelf. But if I have the right to remove that book from the shelf -- that work I abhor -- then you also have exactly the same right and so does everyone else. And then we have no books left on the shelf for any of us.\" --Katherine Paterson, Author. Write a persuasive essay to a newspaper reflecting your views on censorship in libraries. Do you believe that certain materials, such as books, music, movies, magazines, etc., should be removed from the shelves if they are found offensive? Support your position with convincing arguments from your own experience, observations, and/or reading.',\n",
    "               3 : 'ROUGH ROAD AHEAD: Do Not Exceed Posted Speed Limit -- by Joe Kurmaskie. FORGET THAT OLD SAYING ABOUT NEVER taking candy from strangers. No, a better piece of advice for the solo cyclist would be, “Never accept travel advice from a collection of old-timers who haven’t left the confines of their porches since Carter was in office.” It’s not that a group of old guys doesn’t know the terrain. With age comes wisdom and all that, but the world is a fluid place. Things change. At a reservoir campground outside of Lodi, California, I enjoyed the serenity of an early-summer evening and some lively conversation with these old codgers. What I shouldn’t have done was let them have a peek at my map. Like a foolish youth, the next morning I followed their advice and launched out at first light along a “shortcut” that was to slice away hours from my ride to Yosemite National Park. They’d sounded so sure of themselves when pointing out landmarks and spouting off towns I would come to along this breezy jaunt. Things began well enough. I rode into the morning with strong legs and a smile on my face. About forty miles into the pedal, I arrived at the first “town.” This place might have been a thriving little spot at one time—say, before the last world war—but on that morning it fit the traditional definition of a ghost town. I chuckled, checked my water supply, and moved on. The sun was beginning to beat down, but I barely noticed it. The cool pines and rushing rivers of Yosemite had my name written all over them. Twenty miles up the road, I came to a fork of sorts. One ramshackle shed, several rusty pumps, and a corral that couldn’t hold in the lamest mule greeted me. This sight was troubling. I had been hitting my water bottles pretty regularly, and I was traveling through the high deserts of California in June. I got down on my hands and knees, working the handle of the rusted water pump with all my strength. A tarlike substance oozed out, followed by brackish water feeling somewhere in the neighborhood of two hundred degrees. I pumped that handle for several minutes, but the water wouldn’t cool down. It didn’t matter. When I tried a drop or two, it had the flavor of battery acid. The old guys had sworn the next town was only eighteen miles down the road. I could make that! I would conserve my water and go inward for an hour or so—a test of my inner spirit. Not two miles into this next section of the ride, I noticed the terrain changing. Flat road was replaced by short, rolling hills. After I had crested the first few of these, a large highway sign jumped out at me. It read: ROUGH ROAD AHEAD: DO NOT EXCEED POSTED SPEED LIMIT. The speed limit was 55 mph. I was doing a water-depleting 12 mph. Sometimes life can feel so cruel. I toiled on. At some point, tumbleweeds crossed my path and a ridiculously large snake—it really did look like a diamondback—blocked the majority of the pavement in front of me. I eased past, trying to keep my balance in my dehydrated state. The water bottles contained only a few tantalizing sips. Wide rings of dried sweat circled my shirt, and the growing realization that I could drop from heatstroke on a gorgeous day in June simply because I listened to some gentlemen who hadn’t been off their porch in decades, caused me to laugh. It was a sad, hopeless laugh, mind you, but at least I still had the energy to feel sorry for myself. There was no one in sight, not a building, car, or structure of any kind. I began breaking the ride down into distances I could see on the horizon, telling myself that if I could make it that far, I’d be fine. Over one long, crippling hill, a building came into view. I wiped the sweat from my eyes to make sure it wasn’t a mirage, and tried not to get too excited. With what I believed was my last burst of energy, I maneuvered down the hill. In an ironic twist that should please all sadists reading this, the building—abandoned years earlier, by the looks of it—had been a Welch’s Grape Juice factory and bottling plant. A sandblasted picture of a young boy pouring a refreshing glass of juice into his mouth could still be seen. I hung my head. That smoky blues tune “Summertime” rattled around in the dry honeycombs of my deteriorating brain. I got back on the bike, but not before I gathered up a few pebbles and stuck them in my mouth. I’d read once that sucking on stones helps take your mind off thirst by allowing what spit you have left to circulate. With any luck I’d hit a bump and lodge one in my throat. It didn’t really matter. I was going to die and the birds would pick me clean, leaving only some expensive outdoor gear and a diary with the last entry in praise of old men, their wisdom, and their keen sense of direction. I made a mental note to change that paragraph if it looked like I was going to lose consciousness for the last time. Somehow, I climbed away from the abandoned factory of juices and dreams, slowly gaining elevation while losing hope. Then, as easily as rounding a bend, my troubles, thirst, and fear were all behind me. GARY AND WILBER’S FISH CAMP—IF YOU WANT BAIT FOR THE BIG ONES, WE’RE YOUR BEST BET! “And the only bet,” I remember thinking. As I stumbled into a rather modern bathroom and drank deeply from the sink, I had an overwhelming urge to seek out Gary and Wilber, kiss them, and buy some bait—any bait, even though I didn’t own a rod or reel. An old guy sitting in a chair under some shade nodded in my direction. Cool water dripped from my head as I slumped against the wall beside him. “Where you headed in such a hurry?” “Yosemite,” I whispered. “Know the best way to get there?” I watched him from the corner of my eye for a long moment. He was even older than the group I’d listened to in Lodi. “Yes, sir! I own a very good map.” And I promised myself right then that I’d always stick to it in the future. “Rough Road Ahead” by Joe Kurmaskie, from Metal Cowboy, copyright © 1999 Joe Kurmaskie. Write a response that explains how the features of the setting affect the cyclist. In your response, include examples from the essay that support your conclusion.',\n",
    "               4 : 'Winter Hibiscus by Minfong Ho Saeng, a teenage girl, and her family have moved to the United States from Vietnam. As Saeng walks home after failing her driver’s test, she sees a familiar plant. Later, she goes to a florist shop to see if the plant can be purchased. It was like walking into another world. A hot, moist world exploding with greenery. Huge flat leaves, delicate wisps of tendrils, ferns and fronds and vines of all shades and shapes grew in seemingly random profusion. “Over there, in the corner, the hibiscus. Is that what you mean?” The florist pointed at a leafy potted plant by the corner. There, in a shaft of the wan afternoon sunlight, was a single blood-red blossom, its five petals splayed back to reveal a long stamen tipped with yellow pollen. Saeng felt a shock of recognition so intense, it was almost visceral. “Saebba,” Saeng whispered. A saebba hedge, tall and lush, had surrounded their garden, its lush green leaves dotted with vermilion flowers. And sometimes after a monsoon rain, a blossom or two would have blown into the well, so that when she drew the well water, she would find a red blossom floating in the bucket. Slowly, Saeng walked down the narrow aisle toward the hibiscus. Orchids, lanna bushes, oleanders, elephant ear begonias, and bougainvillea vines surrounded her. Plants that she had not even realized she had known but had forgotten drew her back into her childhood world. When she got to the hibiscus, she reached out and touched a petal gently. It felt smooth and cool, with a hint of velvet toward the center—just as she had known it would feel. And beside it was yet another old friend, a small shrub with waxy leaves and dainty flowers with purplish petals and white centers. “Madagascar periwinkle,” its tag announced. How strange to see it in a pot, Saeng thought. Back home it just grew wild, jutting out from the cracks in brick walls or between tiled roofs. And that rich, sweet scent—that was familiar, too. Saeng scanned the greenery around her and found a tall, gangly plant with exquisite little white blossoms on it.  “Dok Malik,” she said, savoring the feel of the word on her tongue, even as she silently noted the English name on its tag, “jasmine.” One of the blossoms had fallen off, and carefully Saeng picked it up and smelled it. She closed her eyes and breathed in, deeply. The familiar fragrance filled her lungs, and Saeng could almost feel the light strands of her grandmother’s long gray hair, freshly washed, as she combed it out with the fine-toothed buffalo-horn comb. And when the sun had dried it, Saeng would help the gnarled old fingers knot the hair into a bun, then slip a dok Malik bud into it. Saeng looked at the white bud in her hand now, small and fragile. Gently, she closed her palm around it and held it tight. That, at least, she could hold on to. But where was the fine-toothed comb? The hibiscus hedge? The well? Her gentle grandmother? A wave of loss so deep and strong that it stung Saeng’s eyes now swept over her. A blink, a channel switch, a boat ride into the night, and it was all gone. Irretrievably, irrevocably gone. And in the warm moist shelter of the greenhouse, Saeng broke down and wept. It was already dusk when Saeng reached home. The wind was blowing harder, tearing off the last remnants of green in the chicory weeds that were growing out of the cracks in the sidewalk. As if oblivious to the cold, her mother was still out in the vegetable garden, digging up the last of the onions with a rusty trowel. She did not see Saeng until the girl had quietly knelt down next to her. Her smile of welcome warmed Saeng. “Ghup ma laio le? You’re back?” she said cheerfully. “Goodness, it’s past five. What took you so long? How did it go? Did you—?” Then she noticed the potted plant that Saeng was holding, its leaves quivering in the wind. Mrs. Panouvong uttered a small cry of surprise and delight. “Dok faeng-noi!” she said. “Where did you get it?” “I bought it,” Saeng answered, dreading her mother’s next question. “How much?” For answer Saeng handed her mother some coins. “That’s all?” Mrs. Panouvong said, appalled, “Oh, but I forgot! You and the Lambert boy ate Bee-Maags . . . .” “No, we didn’t, Mother,” Saeng said. “Then what else—?” “Nothing else. I paid over nineteen dollars for it.” “You what?” Her mother stared at her incredulously. “But how could you? All the seeds for this vegetable garden didn’t cost that much! You know how much we—” She paused, as she noticed the tearstains on her daughter’s cheeks and her puffy eyes. “What happened?” she asked, more gently. “I—I failed the test,” Saeng said. For a long moment Mrs. Panouvong said nothing. Saeng did not dare look her mother in the eye. Instead, she stared at the hibiscus plant and nervously tore off a leaf, shredding it to bits. Her mother reached out and brushed the fragments of green off Saeng’s hands. “It’s a beautiful plant, this dok faeng-noi,” she finally said. “I’m glad you got it.” “It’s—it’s not a real one,” Saeng mumbled. “I mean, not like the kind we had at—at—” She found that she was still too shaky to say the words at home, lest she burst into tears again. “Not like the kind we had before,” she said. “I know,” her mother said quietly. “I’ve seen this kind blooming along the lake. Its flowers aren’t as pretty, but it’s strong enough to make it through the cold months here, this winter hibiscus. That’s what matters.” She tipped the pot and deftly eased the ball of soil out, balancing the rest of the plant in her other hand. “Look how root-bound it is, poor thing,” she said. “Let’s plant it, right now.” She went over to the corner of the vegetable patch and started to dig a hole in the ground. The soil was cold and hard, and she had trouble thrusting the shovel into it. Wisps of her gray hair trailed out in the breeze, and her slight frown deepened the wrinkles around her eyes. There was a frail, wiry beauty to her that touched Saeng deeply. “Here, let me help, Mother,” she offered, getting up and taking the shovel away from her. Mrs. Panouvong made no resistance. “I’ll bring in the hot peppers and bitter melons, then, and start dinner. How would you like an omelet with slices of the bitter melon?” “I’d love it,” Saeng said. Left alone in the garden, Saeng dug out a hole and carefully lowered the “winter hibiscus” into it. She could hear the sounds of cooking from the kitchen now, the beating of eggs against a bowl, the sizzle of hot oil in the pan. The pungent smell of bitter melon wafted out, and Saeng’s mouth watered. It was a cultivated taste, she had discovered—none of her classmates or friends, not even Mrs. Lambert, liked it—this sharp, bitter melon that left a golden aftertaste on the tongue. But she had grown up eating it and, she admitted to herself, much preferred it to a Big Mac. The “winter hibiscus” was in the ground now, and Saeng tamped down the soil around it. Overhead, a flock of Canada geese flew by, their faint honks clear and—yes—familiar to Saeng now. Almost reluctantly, she realized that many of the things that she had thought of as strange before had become, through the quiet repetition of season upon season, almost familiar to her now. Like the geese. She lifted her head and watched as their distinctive V was etched against the evening sky, slowly fading into the distance. When they come back, Saeng vowed silently to herself, in the spring, when the snows melt and the geese return and this hibiscus is budding, then I will take that test again. “Winter Hibiscus” by Minfong Ho, copyright © 1993 by Minfong Ho, from Join In, Multiethnic Short Stories, by Donald R. Gallo, ed. Read the last paragraph of the story. \"When they come back, Saeng vowed silently to herself, in the spring, when the snows melt and the geese return and this hibiscus is budding, then I will take that test again.\" Write a response that explains why the author concludes the story with this paragraph. In your response, include details and examples from the story that support your ideas.',\n",
    "               5 : 'Narciso Rodriguez from Home: The Blueprints of Our Lives My parents, originally from Cuba, arrived in the United States in 1956. After living for a year in a furnished one-room apartment, twenty-one-year-old Rawedia Maria and twenty-seven-year-old Narciso Rodriguez, Sr., could afford to move into a modest, three-room apartment I would soon call home. In 1961, I was born into this simple house, situated in a two-family, blond-brick building in the Ironbound section of Newark, New Jersey. Within its walls, my young parents created our traditional Cuban home, the very heart of which was the kitchen. My parents both shared cooking duties and unwittingly passed on to me their rich culinary skills and a love of cooking that is still with me today (and for which I am eternally grateful). Passionate Cuban music (which I adore to this day) filled the air, mixing with the aromas of the kitchen. Here, the innocence of childhood, the congregation of family and friends, and endless celebrations that encompassed both, formed the backdrop to life in our warm home. Growing up in this environment instilled in me a great sense that “family” had nothing to do with being a blood relative. Quite the contrary, our neighborhood was made up of mostly Spanish, Cuban, and Italian immigrants at a time when overt racism was the norm and segregation prevailed in the United States. In our neighborhood, despite customs elsewhere, all of these cultures came together in great solidarity and friendship. It was a close-knit community of honest, hardworking immigrants who extended a hand to people who, while not necessarily their own kind, were clearly in need. Our landlord and his daughter, Alegria (my babysitter and first friend), lived above us, and Alegria graced our kitchen table for meals more often than not. Also at the table were Sergio and Edelmira, my surrogate grandparents who lived in the basement apartment. (I would not know my “real” grandparents, Narciso the Elder and Consuelo, until 1970 when they were allowed to leave Cuba.) My aunts Bertha and Juanita and my cousins Arnold, Maria, and Rosemary also all lived nearby and regularly joined us at our table. Countless extended family members came and went — and there was often someone staying with us temporarily until they were able to get back on their feet. My parents always kept their arms and their door open to the many people we considered family, knowing that they would do the same for us. My mother and father had come to this country with such courage, without any knowledge of the language or the culture. They came selflessly, as many immigrants do, to give their children a better life, even though it meant leaving behind their families, friends, and careers in the country they loved. They struggled both personally and financially, braving the harsh northern winters while yearning for their native tropics and facing cultural hardships. The barriers to work were strong and high, and my parents both had to accept that they might not be able to find the kind of jobs they deserved. In Cuba, Narciso, Sr., had worked in a laboratory and Rawedia Maria had studied chemical engineering. In the United States, they had to start their lives over entirely, taking whatever work they could find. The faith that this struggle would lead them and their children to better times drove them to endure these hard times. I will always be grateful to my parents for their love and sacrifice. I’ve often told them that what they did was a much more courageous thing than I could have ever done. I’ve often told them of my admiration for their strength and perseverance, and I’ve thanked them repeatedly. But, in reality, there is no way to express my gratitude for the spirit of generosity impressed upon me at such an early age and the demonstration of how important family and friends are. These are two lessons that my parents did not just tell me. They showed me with their lives, and these teachings have been the basis of my life. It was in this simple house that my parents welcomed other refugees to celebrate their arrival to this country and where I celebrated my first birthdays. It was in the warmth of the kitchen in this humble house where a Cuban feast (albeit a frugal Cuban feast) always filled the air with not just scent and music but life and love. It was here where I learned the real definition of “family.” And for this, I will never forget that house or its gracious neighborhood or the many things I learned there about how to love. I will never forget how my parents turned this simple house into a home. — Narciso Rodriguez, Fashion designer. Hometown: Newark, New Jersey. “Narciso Rodriguez” by Narciso Rodriguez, from Home: The Blueprints of Our Lives. Copyright © 2006 by John Edwards. Describe the mood created by the author in the memoir. Support your answer with relevant and specific information from the memoir.',\n",
    "               6 : 'The Mooring Mast by Marcia Amidon Lüsted. When the Empire State Building was conceived, it was planned as the world’s tallest building, taller even than the new Chrysler Building that was being constructed at Forty-second Street and Lexington Avenue in New York. At seventy-seven stories, it was the tallest building before the Empire State began construction, and Al Smith was determined to outstrip it in height. The architect building the Chrysler Building, however, had a trick up his sleeve. He secretly constructed a 185-foot spire inside the building, and then shocked the public and the media by hoisting it up to the top of the Chrysler Building, bringing it to a height of 1,046 feet, 46 feet taller than the originally announced height of the Empire State Building. Al Smith realized that he was close to losing the title of world’s tallest building, and on December 11, 1929, he announced that the Empire State would now reach the height of 1,250 feet. He would add a top or a hat to the building that would be even more distinctive than any other building in the city. John Tauranac describes the plan: [The top of the Empire State Building] would be more than ornamental, more than a spire or dome or a pyramid put there to add a desired few feet to the height of the building or to mask something as mundane as a water tank. Their top, they said, would serve a higher calling. The Empire State Building would be equipped for an age of transportation that was then only the dream of aviation pioneers. This dream of the aviation pioneers was travel by dirigible, or zeppelin, and the Empire State Building was going to have a mooring mast at its top for docking these new airships, which would accommodate passengers on already existing transatlantic routes and new routes that were yet to come. The Age of Dirigibles. By the 1920s, dirigibles were being hailed as the transportation of the future. Also known today as blimps, dirigibles were actually enormous steel-framed balloons, with envelopes of cotton fabric filled with hydrogen and helium to make them lighter than air. Unlike a balloon, a dirigible could be maneuvered by the use of propellers and rudders, and passengers could ride in the gondola, or enclosed compartment, under the balloon.Dirigibles had a top speed of eighty miles per hour, and they could cruise at seventy miles per hour for thousands of miles without needing refueling. Some were as long as one thousand feet, the same length as four blocks in New York City. The one obstacle to their expanded use in New York City was the lack of a suitable landing area. Al Smith saw an opportunity for his Empire State Building: A mooring mast added to the top of the building would allow dirigibles to anchor there for several hours for refueling or service, and to let passengers off and on. Dirigibles were docked by means of an electric winch, which hauled in a line from the front of the ship and then tied it to a mast. The body of the dirigible could swing in the breeze, and yet passengers could safely get on and off the dirigible by walking down a gangplank to an open observation platform. The architects and engineers of the Empire State Building consulted with experts, taking tours of the equipment and mooring operations at the U.S. Naval Air Station in Lakehurst, New Jersey. The navy was the leader in the research and development of dirigibles in the United States. The navy even offered its dirigible, the Los Angeles, to be used in testing the mast. The architects also met with the president of a recently formed airship transport company that planned to offer dirigible service across the Pacific Ocean. When asked about the mooring mast, Al Smith commented: [It’s] on the level, all right. No kidding. We’re working on the thing now. One set of engineers here in New York is trying to dope out a practical, workable arrangement and the Government people in Washington are figuring on some safe way of mooring airships to this mast. Designing the Mast. The architects could not simply drop a mooring mast on top of the Empire State Building’s flat roof. A thousand-foot dirigible moored at the top of the building, held by a single cable tether, would add stress to the building’s frame. The stress of the dirigible’s load and the wind pressure would have to be transmitted all the way to the building’s foundation, which was nearly eleven hundred feet below. The steel frame of the Empire State Building would have to be modified and strengthened to accommodate this new situation. Over sixty thousand dollars’ worth of modifications had to be made to the building’s framework. Rather than building a utilitarian mast without any ornamentation, the architects designed a shiny glass and chrome-nickel stainless steel tower that would be illuminated from inside, with a stepped-back design that imitated the overall shape of the building itself. The rocket-shaped mast would have four wings at its corners, of shiny aluminum, and would rise to a conical roof that would house the mooring arm. The winches and control machinery for the dirigible mooring would be housed in the base of the shaft itself, which also housed elevators and stairs to bring passengers down to the eighty-sixth floor, where baggage and ticket areas would be located. The building would now be 102 floors, with a glassed-in observation area on the 101st floor and an open observation platform on the 102nd floor. This observation area was to double as the boarding area for dirigible passengers. Once the architects had designed the mooring mast and made changes to the existing plans for the building’s skeleton, construction proceeded as planned. When the building had been framed to the 85th floor, the roof had to be completed before the framing for the mooring mast could take place. The mast also had a skeleton of steel and was clad in stainless steel with glass windows. Two months after the workers celebrated framing the entire building, they were back to raise an American flag again—this time at the top of the frame for the mooring mast. The Fate of the Mast. The mooring mast of the Empire State Building was destined to never fulfill its purpose, for reasons that should have been apparent before it was ever constructed. The greatest reason was one of safety: Most dirigibles from outside of the United States used hydrogen rather than helium, and hydrogen is highly flammable. When the German dirigible Hindenburg was destroyed by fire in Lakehurst, New Jersey, on May 6, 1937, the owners of the Empire State Building realized how much worse that accident could have been if it had taken place above a densely populated area such as downtown New York. The greatest obstacle to the successful use of the mooring mast was nature itself. The winds on top of the building were constantly shifting due to violent air currents. Even if the dirigible were tethered to the mooring mast, the back of the ship would swivel around and around the mooring mast. Dirigibles moored in open landing fields could be weighted down in the back with lead weights, but using these at the Empire State Building, where they would be dangling high above pedestrians on the street, was neither practical nor safe. The other practical reason why dirigibles could not moor at the Empire State Building was an existing law against airships flying too low over urban areas. This law would make it illegal for a ship to ever tie up to the building or even approach the area, although two dirigibles did attempt to reach the building before the entire idea was dropped. In December 1930, the U.S. Navy dirigible Los Angeles approached the mooring mast but could not get close enough to tie up because of forceful winds. Fearing that the wind would blow the dirigible onto the sharp spires of other buildings in the area, which would puncture the dirigible’s shell, the captain could not even take his hands off the control levers. Two weeks later, another dirigible, the Goodyear blimp Columbia, attempted a publicity stunt where it would tie up and deliver a bundle of newspapers to the Empire State Building. Because the complete dirigible mooring equipment had never been installed, a worker atop the mooring mast would have to catch the bundle of papers on a rope dangling from the blimp. The papers were delivered in this fashion, but after this stunt the idea of using the mooring mast was shelved. In February 1931, Irving Clavan of the building’s architectural office said, “The as yet unsolved problems of mooring air ships to a fixed mast at such a height made it desirable to postpone to a later date the final installation of the landing gear.” By the late 1930s, the idea of using the mooring mast for dirigibles and their passengers had quietly disappeared. Dirigibles, instead of becoming the transportation of the future, had given way to airplanes. The rooms in the Empire State Building that had been set aside for the ticketing and baggage of dirigible passengers were made over into the world’s highest soda fountain and tea garden for use by the sightseers who flocked to the observation decks. The highest open observation deck, intended for disembarking passengers, has never been open to the public. “The Mooring Mast” by Marcia Amidon Lüsted, from The Empire State Building. Copyright © 2004 by Gale, a part of Cengage Learning, Inc. Based on the excerpt, describe the obstacles the builders of the Empire State Building faced in attempting to allow dirigibles to dock there. Support your answer with relevant and specific information from the excerpt.',\n",
    "               7 : 'Write about patience. Being patient means that you are understanding and tolerant. A patient person experience difficulties without complaining. Do only one of the following: write a story about a time when you were patient OR write a story about a time when someone you know was patient OR write a story in your own way about patience.',\n",
    "               8 : 'We all understand the benefits of laughter. For example, someone once said, “Laughter is the shortest distance between two people.” Many other people believe that laughter is an important part of any relationship. Tell a true story in which laughter was one element or part.'\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding prompts column into the dataframe\n",
    "training_set_rel3_df.insert(loc=2, column=\"prompt\", \n",
    "                            value=[prompt_dict[i] for i in training_set_rel3_df[\"essay_set\"]])\n",
    "\n",
    "# Adding the normalized scores column into the dataframe\n",
    "training_set_rel3_df.insert(loc=4, column=\"scores\", \n",
    "                            value=[(row[\"domain1_score\"]-2)/10 if row[\"essay_set\"]==1 else \n",
    "                                   (row[\"domain1_score\"]-1 + row[\"domain2_score\"]-1)/(5+3) if row[\"essay_set\"]==2 else\n",
    "                                   (row[\"domain1_score\"])/3 if row[\"essay_set\"]==3 else\n",
    "                                   (row[\"domain1_score\"])/3 if row[\"essay_set\"]==4 else\n",
    "                                   (row[\"domain1_score\"])/4 if row[\"essay_set\"]==5 else\n",
    "                                   (row[\"domain1_score\"])/4 if row[\"essay_set\"]==6 else\n",
    "                                   (row[\"domain1_score\"])/30 if row[\"essay_set\"]==7 else\n",
    "                                   (row[\"domain1_score\"])/60\n",
    "                                   for index, row in training_set_rel3_df.iterrows() \n",
    "                                  ]\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and stripping @*** (@CAPS1, @ORGANIZATION1) using the TweetTokenizer with strip_handles=True\n",
    "# Also convert to lowercase and strip punctuation (except @***)\n",
    "tknzr = nltk.tokenize.TweetTokenizer(strip_handles=True)\n",
    "dropped_punctuation = ''.join(s for s in string.punctuation if s!='@')\n",
    "dropped_punctuation = string.punctuation + '”“’—©'\n",
    "\n",
    "# Tokenizing the prompts\n",
    "tk_prompt_dict = {i:tknzr.tokenize(prompt_dict[i]) for i in prompt_dict.keys()}\n",
    "# Removing punctuation\n",
    "for i in tk_prompt_dict.keys():\n",
    "    tk_prompt_dict[i] = list(map(lambda x:x.translate(str.maketrans('', '', dropped_punctuation)).lower(),tk_prompt_dict[i]))\n",
    "    tk_prompt_dict[i] = [str(w).replace(' ','') for w in tk_prompt_dict[i]]\n",
    "    tk_prompt_dict[i] = [s for s in tk_prompt_dict[i] if s]\n",
    "\n",
    "# Adding a column for tokenized prompt\n",
    "training_set_rel3_df.insert(loc=5, column=\"tk_prompt\", \n",
    "                            value=[tk_prompt_dict[i] for i in training_set_rel3_df[\"essay_set\"]])\n",
    "\n",
    "# Tokenizing the essays\n",
    "tk_essay_list = [tknzr.tokenize(row[\"essay\"]) for index, row in training_set_rel3_df.iterrows()]\n",
    "# Removing punctuation\n",
    "for i in range(len(tk_essay_list)):\n",
    "    tk_essay_list[i] = list(map(lambda x:x.translate(str.maketrans('', '', dropped_punctuation)).lower(),tk_essay_list[i]))\n",
    "    tk_essay_list[i] = [str(w).replace(' ','') for w in tk_essay_list[i]]\n",
    "    tk_essay_list[i] = [s for s in tk_essay_list[i] if s]\n",
    "\n",
    "# Adding a column for tokenized essay\n",
    "training_set_rel3_df.insert(loc=6, column=\"tk_essay\", \n",
    "                            value=[tk_essay_list[i] for i in range(len(tk_essay_list))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>prompt</th>\n",
       "      <th>essay</th>\n",
       "      <th>scores</th>\n",
       "      <th>tk_prompt</th>\n",
       "      <th>tk_essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12975</th>\n",
       "      <td>21629</td>\n",
       "      <td>8</td>\n",
       "      <td>We all understand the benefits of laughter. Fo...</td>\n",
       "      <td>When you laugh, is @CAPS5 out of habit, or is ...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>[we, all, understand, the, benefits, of, laugh...</td>\n",
       "      <td>[when, you, laugh, is, out, of, habit, or, is,...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12976</th>\n",
       "      <td>21630</td>\n",
       "      <td>8</td>\n",
       "      <td>We all understand the benefits of laughter. Fo...</td>\n",
       "      <td>Trippin' on fen...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>[we, all, understand, the, benefits, of, laugh...</td>\n",
       "      <td>[trippin, on, fences, i, am, years, young, and...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12977</th>\n",
       "      <td>21633</td>\n",
       "      <td>8</td>\n",
       "      <td>We all understand the benefits of laughter. Fo...</td>\n",
       "      <td>Many people believe that laughter can improve...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>[we, all, understand, the, benefits, of, laugh...</td>\n",
       "      <td>[many, people, believe, that, laughter, can, i...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       essay_id  essay_set                                             prompt  \\\n",
       "12975     21629          8  We all understand the benefits of laughter. Fo...   \n",
       "12976     21630          8  We all understand the benefits of laughter. Fo...   \n",
       "12977     21633          8  We all understand the benefits of laughter. Fo...   \n",
       "\n",
       "                                                   essay    scores  \\\n",
       "12975  When you laugh, is @CAPS5 out of habit, or is ...  0.666667   \n",
       "12976                                 Trippin' on fen...  0.666667   \n",
       "12977   Many people believe that laughter can improve...  0.666667   \n",
       "\n",
       "                                               tk_prompt  \\\n",
       "12975  [we, all, understand, the, benefits, of, laugh...   \n",
       "12976  [we, all, understand, the, benefits, of, laugh...   \n",
       "12977  [we, all, understand, the, benefits, of, laugh...   \n",
       "\n",
       "                                                tk_essay  rater1_domain1  \\\n",
       "12975  [when, you, laugh, is, out, of, habit, or, is,...            20.0   \n",
       "12976  [trippin, on, fences, i, am, years, young, and...            20.0   \n",
       "12977  [many, people, believe, that, laughter, can, i...            20.0   \n",
       "\n",
       "       rater2_domain1  rater3_domain1      ...        rater2_trait3  \\\n",
       "12975            26.0            40.0      ...                  5.0   \n",
       "12976            20.0             NaN      ...                  4.0   \n",
       "12977            20.0             NaN      ...                  4.0   \n",
       "\n",
       "       rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  \\\n",
       "12975            5.0            5.0            5.0            4.0   \n",
       "12976            4.0            4.0            4.0            NaN   \n",
       "12977            4.0            4.0            4.0            NaN   \n",
       "\n",
       "       rater3_trait2  rater3_trait3  rater3_trait4  rater3_trait5  \\\n",
       "12975            4.0            4.0            4.0            4.0   \n",
       "12976            NaN            NaN            NaN            NaN   \n",
       "12977            NaN            NaN            NaN            NaN   \n",
       "\n",
       "       rater3_trait6  \n",
       "12975            4.0  \n",
       "12976            NaN  \n",
       "12977            NaN  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_rel3_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting histogram for prompt and essay token lengths\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,4))\n",
    "fig.subplots_adjust(wspace=.4)\n",
    "\n",
    "prompt_len = [len(training_set_rel3_df[\"tk_prompt\"][index]) for index, row in training_set_rel3_df.iterrows()]\n",
    "essay_len = [len(training_set_rel3_df[\"tk_essay\"][index]) for index, row in training_set_rel3_df.iterrows()]\n",
    "\n",
    "ax[0].hist(x=prompt_len, bins=50, edgecolor=\"w\")\n",
    "ax[0].set_title(\"Histogram of Prompt Length (Tokens)\")\n",
    "ax[0].set_xlabel(\"No. of Tokens\")\n",
    "ax[0].set_ylabel(\"Count\")\n",
    "\n",
    "ax[1].hist(x=essay_len, bins=50, edgecolor=\"w\")\n",
    "ax[1].set_title(\"Histogram of Essay Length (Tokens)\")\n",
    "ax[1].set_xlabel(\"No. of Tokens\")\n",
    "ax[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt lengths = {1153, 836, 45, 111, 1649, 1460, 58, 127}\n",
      "Setting max_len_p = 130\n",
      "\n",
      "Setting max_len = 650, \n",
      " 96.8% of essays are shorter than this length.\n"
     ]
    }
   ],
   "source": [
    "# Setting parameters for max_len\n",
    "print(\"Prompt lengths =\", set(prompt_len))\n",
    "max_len_p = 130\n",
    "print(\"Setting max_len_p =\", max_len_p)\n",
    "print()\n",
    "\n",
    "max_len = 650\n",
    "print(\"Setting max_len = {}, \\n {:.1f}% of essays are shorter than this length.\"\n",
    "      .format(max_len, 100*sum(i <= max_len for i in essay_len) / len(training_set_rel3_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEW VERSION -- with BERT trunking at 512 words\n",
    "\n",
    "# Padding and trunking for array export\n",
    "# Padding will be done at the ending for prompts, but trunking will be at the begining for length > max_len_p. \n",
    "training_set_rel3_df.insert(loc=7, column=\"tk_pad_prompt\", \n",
    "                            value=[row[\"tk_prompt\"][-max_len_p:] if len(row[\"tk_prompt\"])>max_len_p else\n",
    "                                   row[\"tk_prompt\"] + [\"<pad>\" for n in range(max_len_p - len(row[\"tk_prompt\"]))]  \n",
    "                                   if len(row[\"tk_prompt\"])<max_len_p else\n",
    "                                   row[\"tk_prompt\"]\n",
    "                                   for index, row in training_set_rel3_df.iterrows() \n",
    "                                  ]\n",
    "                           )\n",
    "\n",
    "# Padding and trunking will be done at the ending for essays. \n",
    "training_set_rel3_df.insert(loc=8, column=\"tk_pad_essay\", \n",
    "                            value=[row[\"tk_essay\"][:max_len] if len(row[\"tk_essay\"])>max_len else\n",
    "                                   row[\"tk_essay\"] + [\"<pad>\" for n in range(max_len - len(row[\"tk_essay\"]))] \n",
    "                                   if len(row[\"tk_essay\"])<max_len else\n",
    "                                   row[\"tk_essay\"]\n",
    "                                   for index, row in training_set_rel3_df.iterrows() \n",
    "                                  ]\n",
    "                           )\n",
    "\n",
    "# Adding columns for the length of the truncated prompt and the essay to identify no. of words before padding\n",
    "training_set_rel3_df.insert(loc=9, column=\"nw_prompt\", \n",
    "                            value=[len(row[\"tk_prompt\"]) if len(row[\"tk_prompt\"])<max_len_p else\n",
    "                                   max_len_p\n",
    "                                   for index, row in training_set_rel3_df.iterrows() \n",
    "                                  ]\n",
    "                           )\n",
    "\n",
    "training_set_rel3_df.insert(loc=10, column=\"nw_essay\", \n",
    "                            value=[len(row[\"tk_essay\"]) if len(row[\"tk_essay\"])<max_len else\n",
    "                                   max_len\n",
    "                                   for index, row in training_set_rel3_df.iterrows() \n",
    "                                  ]\n",
    "                           )\n",
    "\n",
    "# Adding columns for only truncated prompt and essay, for later BERT embeddings max length is 512\n",
    "max_len_bert = 512\n",
    "\n",
    "training_set_rel3_df.insert(loc=11, column=\"tk_trunc_prompt\", \n",
    "                            value=[row[\"tk_prompt\"][-max_len_p:] if len(row[\"tk_prompt\"])>max_len_p else\n",
    "                                   row[\"tk_prompt\"]\n",
    "                                   for index, row in training_set_rel3_df.iterrows() \n",
    "                                  ]\n",
    "                           )\n",
    "\n",
    "training_set_rel3_df.insert(loc=12, column=\"tk_trunc_essay\", \n",
    "                            value=[row[\"tk_essay\"][:max_len_bert] if len(row[\"tk_essay\"])>max_len_bert else\n",
    "                                   row[\"tk_essay\"]\n",
    "                                   for index, row in training_set_rel3_df.iterrows() \n",
    "                                  ]\n",
    "                           )\n",
    "\n",
    "# Checks for the length of the tokens\n",
    "assert set([len(training_set_rel3_df[\"tk_pad_prompt\"][index]) \n",
    "            for index, row in training_set_rel3_df.iterrows()])==set([max_len_p]), \"Checks for Prompt padding fail\"\n",
    "assert set([len(training_set_rel3_df[\"tk_pad_essay\"][index])\n",
    "            for index, row in training_set_rel3_df.iterrows()])==set([max_len]), \"Checks for Essay padding fail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Padding and trunking for array export\n",
    "# # Padding will be done at the ending for prompts, but trunking will be at the begining for length > max_len_p. \n",
    "# training_set_rel3_df.insert(loc=7, column=\"tk_pad_prompt\", \n",
    "#                             value=[row[\"tk_prompt\"][-max_len_p:] if len(row[\"tk_prompt\"])>max_len_p else\n",
    "#                                    row[\"tk_prompt\"] + [\"<pad>\" for n in range(max_len_p - len(row[\"tk_prompt\"]))]  \n",
    "#                                    if len(row[\"tk_prompt\"])<max_len_p else\n",
    "#                                    row[\"tk_prompt\"]\n",
    "#                                    for index, row in training_set_rel3_df.iterrows() \n",
    "#                                   ]\n",
    "#                            )\n",
    "\n",
    "# # Padding and trunking will be done at the ending for essays. \n",
    "# training_set_rel3_df.insert(loc=8, column=\"tk_pad_essay\", \n",
    "#                             value=[row[\"tk_essay\"][:max_len] if len(row[\"tk_essay\"])>max_len else\n",
    "#                                    row[\"tk_essay\"] + [\"<pad>\" for n in range(max_len - len(row[\"tk_essay\"]))] \n",
    "#                                    if len(row[\"tk_essay\"])<max_len else\n",
    "#                                    row[\"tk_essay\"]\n",
    "#                                    for index, row in training_set_rel3_df.iterrows() \n",
    "#                                   ]\n",
    "#                            )\n",
    "\n",
    "# # Adding columns for the length of the truncated prompt and the essay to identify no. of words before padding\n",
    "# training_set_rel3_df.insert(loc=9, column=\"nw_prompt\", \n",
    "#                             value=[len(row[\"tk_prompt\"]) if len(row[\"tk_prompt\"])<max_len_p else\n",
    "#                                    max_len_p\n",
    "#                                    for index, row in training_set_rel3_df.iterrows() \n",
    "#                                   ]\n",
    "#                            )\n",
    "\n",
    "# training_set_rel3_df.insert(loc=10, column=\"nw_essay\", \n",
    "#                             value=[len(row[\"tk_essay\"]) if len(row[\"tk_essay\"])<max_len else\n",
    "#                                    max_len\n",
    "#                                    for index, row in training_set_rel3_df.iterrows() \n",
    "#                                   ]\n",
    "#                            )\n",
    "\n",
    "# # Checks for the length of the tokens\n",
    "# assert set([len(training_set_rel3_df[\"tk_pad_prompt\"][index]) \n",
    "#             for index, row in training_set_rel3_df.iterrows()])==set([max_len_p]), \"Checks for Prompt padding fail\"\n",
    "# assert set([len(training_set_rel3_df[\"tk_pad_essay\"][index])\n",
    "#             for index, row in training_set_rel3_df.iterrows()])==set([max_len]), \"Checks for Essay padding fail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train, dev and test sets\n",
    "train_set, test_set = train_test_split(training_set_rel3_df, test_size=0.1, random_state=0)\n",
    "train_set, dev_set = train_test_split(train_set, test_size=15/90, random_state=0)\n",
    "\n",
    "# Testing only on prompt 1\n",
    "# train_set, test_set = train_test_split(training_set_rel3_df[training_set_rel3_df[\"essay_set\"] == 1],\n",
    "#                                                                   test_size=0.1, random_state=0)\n",
    "# train_set, dev_set = train_test_split(train_set, test_size=15/90, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of examples in Train Set : 9732\n",
      "No. of examples in Dev Set   : 1947\n",
      "No. of examples in Test Set  : 1298\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of examples in Train Set :\", len(train_set))\n",
    "print(\"No. of examples in Dev Set   :\", len(dev_set))\n",
    "print(\"No. of examples in Test Set  :\", len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting out as arrays, Note: still need to convert to ids based on the embedding\n",
    "train_set_prompts = np.array(list(train_set[\"tk_pad_prompt\"]))\n",
    "train_set_essays = np.array(list(train_set[\"tk_pad_essay\"]))\n",
    "train_set_labels = np.array(list(train_set[\"scores\"]))\n",
    "\n",
    "dev_set_prompts = np.array(list(dev_set[\"tk_pad_prompt\"]))\n",
    "dev_set_essays = np.array(list(dev_set[\"tk_pad_essay\"]))\n",
    "dev_set_labels = np.array(list(dev_set[\"scores\"]))\n",
    "\n",
    "test_set_prompts = np.array(list(test_set[\"tk_pad_prompt\"]))\n",
    "test_set_essays = np.array(list(test_set[\"tk_pad_essay\"]))\n",
    "test_set_labels = np.array(list(test_set[\"scores\"]))\n",
    "\n",
    "# Separate extraction for BERT implementation which are only truncated to max length 512, no padding at this stage.\n",
    "train_set_prompts_bert = np.array(list(train_set[\"tk_trunc_prompt\"])) \n",
    "train_set_essays_bert = np.array(list(train_set[\"tk_trunc_essay\"]))\n",
    "train_set_labels = np.array(list(train_set[\"scores\"]))\n",
    "\n",
    "dev_set_prompts_bert = np.array(list(dev_set[\"tk_trunc_prompt\"]))\n",
    "dev_set_essays_bert = np.array(list(dev_set[\"tk_trunc_essay\"]))\n",
    "dev_set_labels = np.array(list(dev_set[\"scores\"]))\n",
    "\n",
    "test_set_prompts_bert = np.array(list(test_set[\"tk_trunc_prompt\"]))\n",
    "test_set_essays_bert = np.array(list(test_set[\"tk_trunc_essay\"]))\n",
    "test_set_labels = np.array(list(test_set[\"scores\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting out the no. of words prior to padding just in case\n",
    "train_set_prompt_nw = np.array(list(train_set[\"nw_prompt\"]))\n",
    "train_set_essay_nw = np.array(list(train_set[\"nw_essay\"]))\n",
    "\n",
    "dev_set_prompt_nw = np.array(list(dev_set[\"nw_prompt\"]))\n",
    "dev_set_essay_nw = np.array(list(dev_set[\"nw_essay\"]))\n",
    "\n",
    "test_set_prompt_nw = np.array(list(test_set[\"nw_prompt\"]))\n",
    "test_set_essay_nw = np.array(list(test_set[\"nw_essay\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['the', 'mood', 'in', 'the', 'memoir', 'that', 'the', 'author',\n",
       "       'creates', 'is'], dtype='<U35')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the array shape\n",
    "assert train_set_essays.shape == (len(train_set),max_len), \"Checks for array shape fail\"\n",
    "\n",
    "#Checking first 10 tokens of index 0 of train_set_essays\n",
    "train_set_essays[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "A baseline was established with Neural Bag of Words\n",
    "\n",
    "![Neural Bag-of-Words Model](images/neural_bow.png)\n",
    "\n",
    "Using the following notation:\n",
    "- $w^{(i)} \\in \\mathbb{Z}$ for the $i^{th}$ word of the sequence (as an integer index)\n",
    "- $x^{(i)} \\in \\mathbb{R}^d$ for the vector representation (embedding) of $w^{(i)}$\n",
    "- $x \\in \\mathbb{R}^d$ for the fixed-length vector given by summing all the $x^{(i)}$ for an example\n",
    "- $h^{(j)}$ for the hidden state after the $j^{th}$ fully-connected layer\n",
    "- $y$ for the target label ($\\in 1,\\ldots,\\mathtt{num\\_classes}$)\n",
    "\n",
    "Our model is defined as:\n",
    "- **Embedding layer:** $x^{(i)} = W_{embed}[w^{(i)}]$\n",
    "- **Summing vectors:** $x = \\sum_{i=1}^n x^{(i)}$\n",
    "- **Hidden layer(s):** $h^{(j)} = f(h^{(j-1)} W^{(j)} + b^{(j)})$ where $h^{(-1)} = x$ and $j = 0,1,\\ldots,J-1$\n",
    "- **Output layer:** $\\hat{y} = \\hat{P}(y) = \\mathrm{softmax}(h^{(final)} W_{out} + b_{out})$ where $h^{(final)} = h^{(J-1)}$ is the output of the last hidden layer.\n",
    "\n",
    "Logits for the softmax is defined as:\n",
    "$$ \\mathrm{logits} = h^{(final)}W_{out} + b_{out} $$\n",
    "\n",
    "Other dimensions:\n",
    "- `V`: the vocabulary size\n",
    "- `embed_dim`: the embedding dimension $d$\n",
    "- `hidden_dims`: a list of dimensions for the output of each hidden layer (i.e. $\\mathrm{dim}(h^{(j)})$&nbsp;=&nbsp;`hidden_dims[j]`)\n",
    "- `num_classes`: the number of target classes (2 for the binary task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary is 13382\n"
     ]
    }
   ],
   "source": [
    "# Building the Vocabulary\n",
    "\n",
    "# Set the threshold number of occurences to prune words from the vocab\n",
    "prune_threshold = 3\n",
    "\n",
    "# Defining sorting and unique functions\n",
    "def uniq(lst):\n",
    "    last = object()\n",
    "    for item in lst:\n",
    "        if item == last:\n",
    "            continue\n",
    "        yield item\n",
    "        last = item\n",
    "\n",
    "def sort_and_deduplicate(lst):\n",
    "    return list(uniq(sorted(lst, reverse=False)))\n",
    "\n",
    "def word_ctr(lst):\n",
    "    ctr_list = []\n",
    "    last = object()\n",
    "    for word in lst:\n",
    "        if word == last:\n",
    "            ctr_list[-1] += 1\n",
    "        else:\n",
    "            ctr_list.append(1)\n",
    "        last = word\n",
    "    return ctr_list\n",
    "\n",
    "def prune(lst, ctr_lst, num=10):\n",
    "    assert len(lst) == len(ctr_lst), \"List must have same length\"\n",
    "    for i in range(len(lst)):\n",
    "        if ctr_lst[i] < num and ctr_lst[i] != \"<pad>\":\n",
    "            lst[i] = None\n",
    "    return [w for w in lst if w is not None]\n",
    "\n",
    "# Add in words from prompts and essays\n",
    "words = []\n",
    "for key in tk_prompt_dict.keys():\n",
    "    words += tk_prompt_dict[key]\n",
    "for index, row in training_set_rel3_df.iterrows():\n",
    "    words += [w for w in row[\"tk_essay\"]]\n",
    "\n",
    "vocab = sort_and_deduplicate(words)\n",
    "# Pruning words which have less than the threshold occurences\n",
    "vocab_ctr = word_ctr(sorted(words, reverse=False))\n",
    "vocab = prune(vocab, vocab_ctr , num=prune_threshold) \n",
    "# Pre-load in the words for padding and unknown,\n",
    "vocab = [\"<pad>\", \"<unk>\"] + vocab\n",
    "\n",
    "# Removing numbers but not <pad> or <unk>\n",
    "vocab = [w for w in vocab if not any(char.isdigit() for char in w)]\n",
    "\n",
    "# Checking if there are any \"@***\" not parsed out\n",
    "for index, row in training_set_rel3_df.iterrows():\n",
    "    for w in row[\"tk_essay\"]:\n",
    "        if '@' in w:\n",
    "            print(\"Issues with index\", index)\n",
    "            \n",
    "# Defining a dictionary for the vocabulary\n",
    "vocab_dict = {}\n",
    "for ctr in range(len(vocab)):\n",
    "    vocab_dict[vocab[ctr]] = ctr\n",
    "\n",
    "# Defining a get word id function\n",
    "def get_vocab_id(word):\n",
    "    if word in vocab_dict.keys():\n",
    "        return vocab_dict[word]\n",
    "    else:\n",
    "        return vocab_dict[\"<unk>\"]\n",
    "       \n",
    "# Defining a get word from id function\n",
    "def get_vocab_word(ids):\n",
    "    for key, value in vocab_dict.items():\n",
    "        if ids == value: \n",
    "            return key\n",
    "        else:\n",
    "            return \"<unk>\"\n",
    "\n",
    "# Defining a fuction to convert arrays (train_set_essay / train_set_prompts) into word ids\n",
    "def get_id_array(arr):\n",
    "    arr_ids = np.array(list(list(map(get_vocab_id, row)) for row in arr))\n",
    "    return arr_ids\n",
    "\n",
    "# Setting the size of the Vocabulary\n",
    "V = len(vocab_dict)\n",
    "print(\"The size of the vocabulary is\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the word ids for feeding into the model\n",
    "train_set_prompt_ids = get_id_array(train_set_prompts)\n",
    "train_set_essay_ids = get_id_array(train_set_essays)\n",
    "\n",
    "dev_set_prompt_ids = get_id_array(dev_set_prompts)\n",
    "dev_set_essay_ids = get_id_array(dev_set_essays)\n",
    "\n",
    "test_set_prompt_ids = get_id_array(test_set_prompts)\n",
    "test_set_essay_ids = get_id_array(test_set_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of class for the classification: 1\n"
     ]
    }
   ],
   "source": [
    "# For regression:\n",
    "num_score_classes = 1\n",
    "print(\"No. of class for the classification:\", num_score_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the various layers in the NBOW model\n",
    "\n",
    "def embedding_layer(ids_, V, embed_dim, init_scale=0.001):\n",
    "    \"\"\"Construct an embedding layer.\n",
    "    Args:\n",
    "        ids_: [batch_size, max_len] Tensor of int32, integer ids\n",
    "        V: (int) vocabulary size\n",
    "        embed_dim: (int) embedding dimension\n",
    "        init_scale: (float) scale to initialize embeddings\n",
    "\n",
    "    Returns:\n",
    "        xs_: [batch_size, max_len, embed_dim] Tensor of float32, embeddings for\n",
    "            each element in ids_\n",
    "    \"\"\"\n",
    "    ## Assigning variables\n",
    "    W_embed_ = tf.get_variable('W_embed', shape=[V, embed_dim], dtype=tf.float32,\n",
    "                          initializer=tf.random_uniform_initializer(-init_scale, init_scale))\n",
    "    \n",
    "    ## Looking up embeddings\n",
    "    xs_ = tf.nn.embedding_lookup(params=W_embed_, ids=ids_)\n",
    "\n",
    "    return xs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected_layers(h0_, hidden_dims, activation=tf.tanh,\n",
    "                           dropout_rate=0, is_training=False):\n",
    "    \"\"\"Construct a stack of fully-connected layers.\n",
    "\n",
    "    Args:\n",
    "        h0_: [batch_size, d] Tensor of float32, the input activations\n",
    "        hidden_dims: list(int) dimensions of the output of each layer\n",
    "        activation: TensorFlow function, such as tf.tanh. Passed to\n",
    "            tf.layers.dense.\n",
    "        dropout_rate: if > 0, will apply dropout to activations.\n",
    "        is_training: (bool) if true, is in training mode\n",
    "\n",
    "    Returns:\n",
    "        h_: [batch_size, hidden_dims[-1]] Tensor of float32, the activations of\n",
    "            the last layer constructed by this function.\n",
    "    \"\"\"\n",
    "    h_ = h0_\n",
    "    for i, hdim in enumerate(hidden_dims):\n",
    "        h_ = tf.layers.dense(h_, hdim, activation=activation, name=(\"Hidden_%d\"%i))\n",
    "\n",
    "        if dropout_rate > 0:\n",
    "            h_ = tf.layers.dropout(h_, rate=dropout_rate, training=is_training) \n",
    "\n",
    "    return h_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_output_layer(h_, labels_, num_classes):\n",
    "    \"\"\"Construct a softmax output layer.\n",
    "\n",
    "    Implements:\n",
    "        output = h W + b\n",
    "        loss = tf.losses.mean_squared_error(labels=labels_, predictions=predictions)\n",
    "\n",
    "    Args:\n",
    "        h_: [batch_size, d] Tensor of float32, the input activations from a\n",
    "            previous layer\n",
    "        labels_: [batch_size] Tensor of float32, the target label ids\n",
    "        num_classes: (int) the number of output classes\n",
    "\n",
    "    Returns: (loss_, logits_)\n",
    "        loss_: scalar Tensor of float32, the cross-entropy loss\n",
    "        logits_: [batch_size, num_classes] Tensor of float32, the logits (hW + b)\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"Predictions\"):\n",
    "        \n",
    "        ##Assigning variables\n",
    "        W_out_ = tf.get_variable(\"W_out\", shape=[h_.shape[-1], num_classes], \n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.random_normal_initializer())\n",
    "        b_out_ = tf.get_variable(\"b_out\", shape=[num_classes,],\n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.zeros_initializer())\n",
    "        # Calculating the predictions\n",
    "        predictions_ = tf.matmul(h_, W_out_) + b_out_\n",
    "        \n",
    "    # If no labels provided, don't try to compute loss.\n",
    "    if labels_ is None:\n",
    "         return None, predictions_\n",
    "\n",
    "    with tf.name_scope(\"MSE\"):\n",
    "        loss_ = tf.losses.mean_squared_error(labels=labels_, predictions=tf.reshape(predictions_, [-1]), weights=1.0)\n",
    "\n",
    "    return loss_, predictions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BOW_encoder(ids_, ns_, V, embed_dim, hidden_dims, dropout_rate=0,\n",
    "                is_training=None,\n",
    "                **unused_kw):\n",
    "    \"\"\"Construct a bag-of-words encoder.\n",
    "\n",
    "        - Build the embeddings (using embedding_layer(...))\n",
    "        - Apply the mask to zero-out padding indices, and sum the embeddings\n",
    "            for each example\n",
    "        - Build a stack of hidden layers (using fully_connected_layers(...))\n",
    "\n",
    "    Note that this function returns the final encoding h_ as well as the masked\n",
    "    embeddings xs_. The latter is used for L2 regularization, so that we can\n",
    "    penalize the norm of only those vectors that were actually used for each\n",
    "    example.\n",
    "\n",
    "    Args:\n",
    "        ids_: [batch_size, max_len] Tensor of int32, integer ids\n",
    "        ns_:  [batch_size] Tensor of int32, (clipped) length of each sequence\n",
    "        V: (int) vocabulary size\n",
    "        embed_dim: (int) embedding dimension\n",
    "        hidden_dims: list(int) dimensions of the output of each layer\n",
    "        dropout_rate: (float) rate to use for dropout\n",
    "        is_training: (bool) if true, is in training mode\n",
    "\n",
    "    Returns: (h_, xs_)\n",
    "        h_: [batch_size, hidden_dims[-1]] Tensor of float32, the activations of\n",
    "            the last layer constructed by this function.\n",
    "        xs_: [batch_size, max_len, embed_dim] Tensor of float32, the per-word\n",
    "            embeddings as returned by embedding_layer and with the mask applied\n",
    "            to zero-out the pad indices.\n",
    "    \"\"\"\n",
    "    assert is_training is not None, \"is_training must be explicitly set to True or False\"\n",
    "    # Embedding layer should produce:\n",
    "    #   xs_: [batch_size, max_len, embed_dim]\n",
    "    with tf.variable_scope(\"Embedding_Layer\"):\n",
    "         xs_ = embedding_layer(ids_, V, embed_dim, init_scale=0.001)\n",
    "\n",
    "    # Mask off the padding indices with zeros\n",
    "    #   mask_: [batch_size, max_len, 1] with values of 0.0 or 1.0\n",
    "    mask_ = tf.expand_dims(tf.sequence_mask(ns_, xs_.shape[1],\n",
    "                                           dtype=tf.float32), -1)\n",
    "    # Multiply xs_ by the mask to zero-out pad indices.\n",
    "    xs_ = tf.multiply(xs_, mask_)\n",
    "       \n",
    "    # Sum embeddings: [batch_size, max_len, embed_dim] -> [batch_size, embed_dim]\n",
    "    h0_ = tf.math.reduce_sum(xs_, axis=1)\n",
    "          \n",
    "    # Build a stack of fully-connected layers\n",
    "    h_ = fully_connected_layers(h0_, hidden_dims, activation=tf.tanh,\n",
    "                                dropout_rate=dropout_rate, is_training=is_training)\n",
    "    \n",
    "    return h_, xs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_model_fn(features, labels, mode, params):\n",
    "       \n",
    "    # Check if this graph is going to be used for training.\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    if params['encoder_type'] == 'bow':\n",
    "        with tf.variable_scope(\"Encoder\"):\n",
    "            h_, xs_ = BOW_encoder(features['ids'], features['ns'],\n",
    "                                  is_training=is_training,\n",
    "                                  **params)\n",
    "    else:\n",
    "        raise ValueError(\"Error: unsupported encoder type \"\n",
    "                         \"'{:s}'\".format(params['encoder_type']))\n",
    "\n",
    "    # Construct output mse layer and loss functions\n",
    "    with tf.variable_scope(\"Output_Layer\"):\n",
    "        mse_loss_, predictions_ = mse_output_layer(h_, labels, params['num_classes'])\n",
    "\n",
    "    with tf.name_scope(\"Prediction\"):\n",
    "        predictions_dict = {\"y_hat\": predictions_}\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # If predict mode, don't bother computing loss.\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          predictions=predictions_dict)\n",
    "\n",
    "    # L2 regularization (weight decay) on parameters, from all layers\n",
    "    with tf.variable_scope(\"Regularization\"):\n",
    "        l2_penalty_ = tf.nn.l2_loss(xs_)  # l2 loss on embeddings\n",
    "        for var_ in tf.trainable_variables():\n",
    "            if \"Embedding_Layer\" in var_.name:\n",
    "                continue\n",
    "            l2_penalty_ += tf.nn.l2_loss(var_)\n",
    "        l2_penalty_ *= params['beta']  # scale by regularization strength\n",
    "        tf.summary.scalar(\"l2_penalty\", l2_penalty_)\n",
    "        regularized_loss_ = mse_loss_ + l2_penalty_\n",
    "\n",
    "    with tf.variable_scope(\"Training\"):\n",
    "        if params['optimizer'] == 'adagrad':\n",
    "            optimizer_ = tf.train.AdagradOptimizer(params['lr'])\n",
    "        else:\n",
    "            optimizer_ = tf.train.GradientDescentOptimizer(params['lr'])\n",
    "        train_op_ = optimizer_.minimize(regularized_loss_,\n",
    "                                        global_step=tf.train.get_global_step())\n",
    "        \n",
    "    \n",
    "    tf.summary.scalar(\"mse_loss\", mse_loss_)\n",
    "    eval_metrics = {\"mse_loss\": tf.metrics.mean(mse_loss_)}\n",
    "    \n",
    "#                     \"accuracy\": tf.metrics.accuracy(labels, tf.reshape(predictions_, [-1]))}\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      predictions=predictions_dict,\n",
    "                                      loss=regularized_loss_,\n",
    "                                      train_op=train_op_,\n",
    "                                      eval_metric_ops=eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/project/tf_baseline_nbow20190407-1450', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000217FA05EB00>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "\n",
      "To view training (once it starts), run:\n",
      "\n",
      "    tensorboard --logdir='/tmp/project/tf_baseline_nbow20190407-1450' --port 6006\n",
      "\n",
      "Then in your browser, open: http://localhost:6006\n"
     ]
    }
   ],
   "source": [
    "# Specify model hyperparameters as used by model_fn\n",
    "model_params = dict(V=V, embed_dim=100, hidden_dims=[50], num_classes=num_score_classes,\n",
    "                    encoder_type='bow',\n",
    "                    lr=0.1, optimizer='adagrad', beta=0.01)\n",
    "\n",
    "# Setting the directory to save the checkpoints to\n",
    "checkpoint_dir = \"/tmp/project/tf_baseline_nbow\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "if os.path.isdir(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "\n",
    "model = tf.estimator.Estimator(model_fn=regression_model_fn, \n",
    "                               params=model_params,\n",
    "                               model_dir=checkpoint_dir)\n",
    "\n",
    "print(\"\")\n",
    "print(\"To view training (once it starts), run:\\n\")\n",
    "print(\"    tensorboard --logdir='{:s}' --port 6006\".format(checkpoint_dir))\n",
    "print(\"\\nThen in your browser, open: http://localhost:6006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.1627486, step = 1\n",
      "INFO:tensorflow:global_step/sec: 50.5422\n",
      "INFO:tensorflow:loss = 22.099787, step = 101 (1.980 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.2879\n",
      "INFO:tensorflow:loss = 7.941211, step = 201 (1.950 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.6565\n",
      "INFO:tensorflow:loss = 4.907332, step = 301 (1.974 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.5238\n",
      "INFO:tensorflow:loss = 3.3604016, step = 401 (1.941 sec)\n",
      "INFO:tensorflow:global_step/sec: 53.07\n",
      "INFO:tensorflow:loss = 2.6565607, step = 501 (1.884 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.1577\n",
      "INFO:tensorflow:loss = 1.7226905, step = 601 (1.955 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 609 into /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.92573714.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-07-06:50:29\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-609\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-07-06:50:30\n",
      "INFO:tensorflow:Saving dict for global step 609: global_step = 609, loss = 1.5990155, mse_loss = 0.039294366\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 609: /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-609\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-609\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 609 into /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.8388975, step = 610\n",
      "INFO:tensorflow:global_step/sec: 51.4187\n",
      "INFO:tensorflow:loss = 1.4543366, step = 710 (1.946 sec)\n",
      "INFO:tensorflow:global_step/sec: 53.4357\n",
      "INFO:tensorflow:loss = 0.96460676, step = 810 (1.871 sec)\n",
      "INFO:tensorflow:global_step/sec: 54.3577\n",
      "INFO:tensorflow:loss = 0.7711671, step = 910 (1.840 sec)\n",
      "INFO:tensorflow:global_step/sec: 54.1097\n",
      "INFO:tensorflow:loss = 0.69172776, step = 1010 (1.848 sec)\n",
      "INFO:tensorflow:global_step/sec: 53.936\n",
      "INFO:tensorflow:loss = 0.7104066, step = 1110 (1.854 sec)\n",
      "INFO:tensorflow:global_step/sec: 53.6918\n",
      "INFO:tensorflow:loss = 0.6074878, step = 1210 (1.862 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1218 into /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.45748466.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-07-06:50:44\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-1218\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-07-06:50:45\n",
      "INFO:tensorflow:Saving dict for global step 1218: global_step = 1218, loss = 0.5869805, mse_loss = 0.037316926\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1218: /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-1218\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-1218\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1218 into /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt.\n",
      "INFO:tensorflow:loss = 0.66881865, step = 1219\n",
      "INFO:tensorflow:global_step/sec: 53.9216\n",
      "INFO:tensorflow:loss = 0.5672992, step = 1319 (1.856 sec)\n",
      "INFO:tensorflow:global_step/sec: 53.7491\n",
      "INFO:tensorflow:loss = 0.49811634, step = 1419 (1.860 sec)\n",
      "INFO:tensorflow:global_step/sec: 54.2553\n",
      "INFO:tensorflow:loss = 0.4570415, step = 1519 (1.843 sec)\n",
      "INFO:tensorflow:global_step/sec: 54.1097\n",
      "INFO:tensorflow:loss = 0.49475104, step = 1619 (1.848 sec)\n",
      "INFO:tensorflow:global_step/sec: 53.3649\n",
      "INFO:tensorflow:loss = 0.47401348, step = 1719 (1.874 sec)\n",
      "INFO:tensorflow:global_step/sec: 53.8352\n",
      "INFO:tensorflow:loss = 0.4360044, step = 1819 (1.858 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1827 into /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.3745503.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-07-06:50:59\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-1827\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-07-06:50:59\n",
      "INFO:tensorflow:Saving dict for global step 1827: global_step = 1827, loss = 0.43122664, mse_loss = 0.02480631\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1827: /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-1827\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-1827\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1827 into /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt.\n",
      "INFO:tensorflow:loss = 0.4971954, step = 1828\n",
      "INFO:tensorflow:global_step/sec: 52.1098\n",
      "INFO:tensorflow:loss = 0.43034887, step = 1928 (1.920 sec)\n",
      "INFO:tensorflow:global_step/sec: 54.4164\n",
      "INFO:tensorflow:loss = 0.3975584, step = 2028 (1.838 sec)\n",
      "INFO:tensorflow:global_step/sec: 53.7061\n",
      "INFO:tensorflow:loss = 0.39161372, step = 2128 (1.862 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.0829\n",
      "INFO:tensorflow:loss = 0.42691636, step = 2228 (1.920 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.1318\n",
      "INFO:tensorflow:loss = 0.40560475, step = 2328 (1.956 sec)\n",
      "INFO:tensorflow:global_step/sec: 53.464\n",
      "INFO:tensorflow:loss = 0.38034955, step = 2428 (1.870 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2436 into /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.34307405.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-07-06:51:13\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-2436\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-07-06:51:14\n",
      "INFO:tensorflow:Saving dict for global step 2436: global_step = 2436, loss = 0.38242, mse_loss = 0.023402616\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2436: /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-2436\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-2436\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 2436 into /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt.\n",
      "INFO:tensorflow:loss = 0.4288907, step = 2437\n",
      "INFO:tensorflow:global_step/sec: 51.4581\n",
      "INFO:tensorflow:loss = 0.37881553, step = 2537 (1.944 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.8336\n",
      "INFO:tensorflow:loss = 0.36261207, step = 2637 (1.893 sec)\n",
      "INFO:tensorflow:global_step/sec: 54.0517\n",
      "INFO:tensorflow:loss = 0.360279, step = 2737 (1.850 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 54.7712\n",
      "INFO:tensorflow:loss = 8.453338, step = 2837 (1.825 sec)\n",
      "INFO:tensorflow:global_step/sec: 54.9055\n",
      "INFO:tensorflow:loss = 4.4284105, step = 2937 (1.822 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.0025\n",
      "INFO:tensorflow:loss = 2.4955518, step = 3037 (1.960 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3045 into /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.2663571.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-07-06:51:27\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-3045\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-07-06:51:28\n",
      "INFO:tensorflow:Saving dict for global step 3045: global_step = 3045, loss = 2.3782115, mse_loss = 0.049644258\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3045: /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-3045\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-3045\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3045 into /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt.\n",
      "INFO:tensorflow:loss = 2.6394775, step = 3046\n",
      "INFO:tensorflow:global_step/sec: 51.9219\n",
      "INFO:tensorflow:loss = 2.127466, step = 3146 (1.927 sec)\n",
      "INFO:tensorflow:global_step/sec: 54.0083\n",
      "INFO:tensorflow:loss = 1.1791255, step = 3246 (1.852 sec)\n",
      "INFO:tensorflow:global_step/sec: 54.2992\n",
      "INFO:tensorflow:loss = 0.979966, step = 3346 (1.842 sec)\n",
      "INFO:tensorflow:global_step/sec: 53.8783\n",
      "INFO:tensorflow:loss = 0.85147566, step = 3446 (1.856 sec)\n",
      "INFO:tensorflow:global_step/sec: 54.8457\n",
      "INFO:tensorflow:loss = 0.77434397, step = 3546 (1.823 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.3403\n",
      "INFO:tensorflow:loss = 0.6645179, step = 3646 (1.986 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3654 into /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.50736517.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-07-06:51:42\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-3654\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-07-06:51:42\n",
      "INFO:tensorflow:Saving dict for global step 3654: global_step = 3654, loss = 0.6631407, mse_loss = 0.030285493\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3654: /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-3654\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-3654\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3654 into /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt.\n",
      "INFO:tensorflow:loss = 0.7308578, step = 3655\n",
      "INFO:tensorflow:global_step/sec: 51.1448\n",
      "INFO:tensorflow:loss = 0.66403145, step = 3755 (1.957 sec)\n",
      "INFO:tensorflow:global_step/sec: 53.5777\n",
      "INFO:tensorflow:loss = 0.5468489, step = 3855 (1.866 sec)\n",
      "INFO:tensorflow:global_step/sec: 53.3508\n",
      "INFO:tensorflow:loss = 0.5105466, step = 3955 (1.875 sec)\n",
      "INFO:tensorflow:global_step/sec: 53.0002\n",
      "INFO:tensorflow:loss = 0.50601023, step = 4055 (1.886 sec)\n",
      "INFO:tensorflow:global_step/sec: 53.042\n",
      "INFO:tensorflow:loss = 0.49755973, step = 4155 (1.885 sec)\n",
      "INFO:tensorflow:global_step/sec: 53.0281\n",
      "INFO:tensorflow:loss = 0.47769606, step = 4255 (1.886 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4263 into /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.4180101.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-07-06:51:57\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-4263\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-07-06:51:57\n",
      "INFO:tensorflow:Saving dict for global step 4263: global_step = 4263, loss = 0.49036005, mse_loss = 0.038119067\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4263: /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-4263\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-4263\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 4263 into /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt.\n",
      "INFO:tensorflow:loss = 0.51913136, step = 4264\n",
      "INFO:tensorflow:global_step/sec: 51.4712\n",
      "INFO:tensorflow:loss = 0.48691028, step = 4364 (1.944 sec)\n",
      "INFO:tensorflow:global_step/sec: 54.0517\n",
      "INFO:tensorflow:loss = 0.4323614, step = 4464 (1.850 sec)\n",
      "INFO:tensorflow:global_step/sec: 53.7491\n",
      "INFO:tensorflow:loss = 0.41793263, step = 4564 (1.860 sec)\n",
      "INFO:tensorflow:global_step/sec: 53.3085\n",
      "INFO:tensorflow:loss = 0.42468962, step = 4664 (1.876 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.476\n",
      "INFO:tensorflow:loss = 0.42239878, step = 4764 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7368\n",
      "INFO:tensorflow:loss = 0.40739247, step = 4864 (1.896 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4872 into /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.37267008.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-07-06:52:11\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-4872\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-07-06:52:11\n",
      "INFO:tensorflow:Saving dict for global step 4872: global_step = 4872, loss = 0.41925028, mse_loss = 0.026187759\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4872: /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-4872\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-4872\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 4872 into /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt.\n",
      "INFO:tensorflow:loss = 0.44315273, step = 4873\n",
      "INFO:tensorflow:global_step/sec: 50.0777\n",
      "INFO:tensorflow:loss = 0.41096768, step = 4973 (2.006 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.4449\n",
      "INFO:tensorflow:loss = 0.3804147, step = 5073 (1.936 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.9863\n",
      "INFO:tensorflow:loss = 0.37618273, step = 5173 (1.887 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.8336\n",
      "INFO:tensorflow:loss = 0.38599515, step = 5273 (1.893 sec)\n",
      "INFO:tensorflow:global_step/sec: 53.5065\n",
      "INFO:tensorflow:loss = 0.38074076, step = 5373 (1.868 sec)\n",
      "INFO:tensorflow:global_step/sec: 53.182\n",
      "INFO:tensorflow:loss = 0.3715772, step = 5473 (1.881 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5481 into /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.345162.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-07-06:52:26\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-5481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-07-06:52:26\n",
      "INFO:tensorflow:Saving dict for global step 5481: global_step = 5481, loss = 0.3861667, mse_loss = 0.026242513\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5481: /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-5481\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-5481\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 5481 into /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt.\n",
      "INFO:tensorflow:loss = 0.40486926, step = 5482\n",
      "INFO:tensorflow:global_step/sec: 44.9126\n",
      "INFO:tensorflow:loss = 0.37245393, step = 5582 (2.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.0179\n",
      "INFO:tensorflow:loss = 0.3546372, step = 5682 (2.127 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.4413\n",
      "INFO:tensorflow:loss = 0.3507692, step = 5782 (2.065 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.3906\n",
      "INFO:tensorflow:loss = 0.35277274, step = 5882 (1.984 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7506\n",
      "INFO:tensorflow:loss = 0.36736545, step = 5982 (1.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 54.1533\n",
      "INFO:tensorflow:loss = 0.3462586, step = 6082 (1.847 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6090 into /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.33189708.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-07-06:52:41\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-6090\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-07-06:52:42\n",
      "INFO:tensorflow:Saving dict for global step 6090: global_step = 6090, loss = 0.3748201, mse_loss = 0.03825179\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6090: /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-6090\n"
     ]
    }
   ],
   "source": [
    "# Training params for the input_fn to tf.estimator\n",
    "batch_size = 32\n",
    "train_params = dict(batch_size=batch_size, total_epochs=20, eval_every=2)\n",
    "assert(train_params['total_epochs'] % train_params['eval_every'] == 0)\n",
    "\n",
    "# Do 'eval_every' epochs at once, followed by evaluating on the dev set.\n",
    "# NOTE: use patch_numpy_io.numpy_input_fn instead of tf.estimator.inputs.numpy_input_fn\n",
    "train_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                    x={\"ids\": train_set_essay_ids, \"ns\": train_set_essay_nw}, y=train_set_labels,\n",
    "                    batch_size=train_params['batch_size'], \n",
    "                    num_epochs=train_params['eval_every'], shuffle=True\n",
    "                 )\n",
    "\n",
    "# Input function for dev set batches. As above, but:\n",
    "# - Don't randomize order\n",
    "# - Iterate exactly once (one epoch)\n",
    "dev_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": dev_set_essay_ids, \"ns\": dev_set_essay_nw}, y=dev_set_labels,\n",
    "                    batch_size=batch_size, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "for _ in range(train_params['total_epochs'] // train_params['eval_every']):\n",
    "    # Train for a few epochs, then evaluate on dev\n",
    "    model.train(input_fn=train_input_fn)\n",
    "    eval_metrics = model.evaluate(input_fn=dev_input_fn, name=\"dev\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Quadratic Weighted Kappa, QWK, from the labels and the predictions\n",
    "def get_QWK(float_score_list, predict_dict):\n",
    "\n",
    "    def convert_0_100_scores(float_labels_list):\n",
    "        return [int(i*100) for i in float_labels_list]\n",
    "\n",
    "    def convert_prediction_dict_to_lst(predict_dict):\n",
    "        predict_lst = []\n",
    "        for d in predict_dict:\n",
    "            predict_lst.append(d[\"y_hat\"][0])\n",
    "        return predict_lst\n",
    "    \n",
    "    labels_int = convert_0_100_scores(list(float_score_list))\n",
    "    predict_int = convert_0_100_scores(convert_prediction_dict_to_lst(predict_dict))\n",
    "    \n",
    "    return sklearn.metrics.cohen_kappa_score(labels_int, predict_int, weights='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-6090\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Neural BOW baseline for MSE on dev set: 3.83%\n",
      "Neural BOW baseline for QWK on dev set: 0.7111\n"
     ]
    }
   ],
   "source": [
    "predict_dev = list(model.predict(input_fn=dev_input_fn))\n",
    "print(\"Neural BOW baseline for MSE on dev set: {:.02%}\".format(eval_metrics['mse_loss']))\n",
    "print(\"Neural BOW baseline for QWK on dev set: {:.04}\".format(get_QWK(dev_set_labels, predict_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensorboard charts for the baseline using Neural Bag of Words\n",
    "\n",
    "![Neural Bag-of-Words Training Graphs](images/nbow_train_graphs_mse1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-07-06:53:44\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-6090\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-07-06:53:45\n",
      "INFO:tensorflow:Saving dict for global step 6090: global_step = 6090, loss = 0.37028024, mse_loss = 0.035300553\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6090: /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-6090\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/project/tf_baseline_nbow20190407-1450\\model.ckpt-6090\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "\n",
      "Neural BOW baseline for MSE on test set: 3.53%\n",
      "Neural BOW baseline for QWK on test set: 0.7265\n"
     ]
    }
   ],
   "source": [
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": test_set_essay_ids, \"ns\": test_set_essay_nw}, y=test_set_labels,\n",
    "                    batch_size=batch_size, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "eval_metrics = model.evaluate(input_fn=test_input_fn, name=\"test\")\n",
    "predict_test = list(model.predict(input_fn=test_input_fn))\n",
    "\n",
    "print()\n",
    "print(\"Neural BOW baseline for MSE on test set: {:.02%}\".format(eval_metrics['mse_loss']))\n",
    "print(\"Neural BOW baseline for QWK on test set: {:.04}\".format(get_QWK(test_set_labels, predict_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the LSTM with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM with Attention](images/LSTM_Project.png)\n",
    "\n",
    "- $h_m$ for last hidden state of LSTM (Essay representation)\n",
    "- $h_{m+n}$ for last hidden state of LSTM with Prompt (Essay representation)\n",
    "\n",
    "Semantic Score ($S_e$)\n",
    "\n",
    "- $S_e = sigmod(w_s h_m + b_s)$\n",
    "- $w_s$ for weighted matrix of the dense layer\n",
    "- $b_s$ stands for the bias\n",
    "- $obj(S_e,\\bar{S_e}) = 1/N \\sum_{i=1}^n (S_i - \\bar{S_i})^2 $ \n",
    "- $S_e$ for predict score set of training samples\n",
    "- $\\bar{S_e}$ for the original hand marked score set\n",
    "\n",
    "Coherence Score ($C_e$)\n",
    "\n",
    "- $C_e = sigmod(w_c h_m + b_c)$\n",
    "- $w_c$ for weighted matrix of the dense layer\n",
    "- $b_c$ stands for the bias\n",
    "- $obj(C_e,\\bar{C_e}) = 1/N \\sum_{i=1}^n (C_i - \\bar{C_i})^2 $ \n",
    "- $C_e$ for predict coherence score set of training samples\n",
    "- $\\bar{C_e}$ for the gold coherence score which is equal to the corresponding hand marked scores\n",
    "\n",
    "Prompt-relevant Score ($P_e$)\n",
    "\n",
    "- $P_e = sigmod(w_p h_{m+n} + b_p)$\n",
    "- $w_p$ for weighted matrix of the dense layer\n",
    "- $b_p$ stands for the bias\n",
    "- $obj(P_e,\\bar{P_e}) = 1/N \\sum_{i=1}^n (P_i - \\bar{P_i})^2 $ \n",
    "- $P_e$ for predict Prompt-relevant score set of training samples\n",
    "- $\\bar{P_e}$ for the gold prompt-relevant score which is equal to the corresponding hand marked scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building Model\n",
    "from keras import initializers, regularizers, optimizers\n",
    "from keras.layers import LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from keras.models import Sequential, load_model, model_from_config\n",
    "import layers; reload(layers)\n",
    "from layers import AttentionWithContext, Addition\n",
    "\n",
    "def get_model(hidden_units):\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    num_classes = 1\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_units, dropout=0.4, recurrent_dropout=0.4, input_shape=[512, 768], return_sequences=True))\n",
    "    #add attention\n",
    "    model.add(AttentionWithContext())\n",
    "    model.add(Addition())\n",
    "    model.add(Dense(num_classes, activation='sigmoid'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 512, 300)          1282800   \n",
      "_________________________________________________________________\n",
      "attention_with_context_5 (At (None, 512, 300)          90600     \n",
      "_________________________________________________________________\n",
      "addition_5 (Addition)        (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 1,373,701\n",
      "Trainable params: 1,373,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9344/9732 [===========================>..] - ETA: 35:06 - loss: 0.0969 - mean_absolute_error: 0.26 - ETA: 26:23 - loss: 0.1411 - mean_absolute_error: 0.31 - ETA: 23:27 - loss: 0.1415 - mean_absolute_error: 0.31 - ETA: 22:20 - loss: 0.1189 - mean_absolute_error: 0.27 - ETA: 21:49 - loss: 0.1133 - mean_absolute_error: 0.27 - ETA: 20:54 - loss: 0.1062 - mean_absolute_error: 0.26 - ETA: 19:54 - loss: 0.1008 - mean_absolute_error: 0.25 - ETA: 19:14 - loss: 0.0944 - mean_absolute_error: 0.24 - ETA: 18:39 - loss: 0.0887 - mean_absolute_error: 0.23 - ETA: 18:13 - loss: 0.0834 - mean_absolute_error: 0.22 - ETA: 17:51 - loss: 0.0790 - mean_absolute_error: 0.21 - ETA: 17:30 - loss: 0.0757 - mean_absolute_error: 0.21 - ETA: 17:14 - loss: 0.0728 - mean_absolute_error: 0.20 - ETA: 16:56 - loss: 0.0705 - mean_absolute_error: 0.20 - ETA: 16:39 - loss: 0.0680 - mean_absolute_error: 0.20 - ETA: 16:26 - loss: 0.0670 - mean_absolute_error: 0.20 - ETA: 16:13 - loss: 0.0661 - mean_absolute_error: 0.19 - ETA: 16:03 - loss: 0.0647 - mean_absolute_error: 0.19 - ETA: 15:54 - loss: 0.0634 - mean_absolute_error: 0.19 - ETA: 15:45 - loss: 0.0621 - mean_absolute_error: 0.19 - ETA: 15:39 - loss: 0.0615 - mean_absolute_error: 0.19 - ETA: 15:31 - loss: 0.0600 - mean_absolute_error: 0.18 - ETA: 15:24 - loss: 0.0586 - mean_absolute_error: 0.18 - ETA: 15:16 - loss: 0.0571 - mean_absolute_error: 0.18 - ETA: 15:07 - loss: 0.0557 - mean_absolute_error: 0.18 - ETA: 15:01 - loss: 0.0543 - mean_absolute_error: 0.17 - ETA: 14:55 - loss: 0.0539 - mean_absolute_error: 0.17 - ETA: 14:51 - loss: 0.0531 - mean_absolute_error: 0.17 - ETA: 14:45 - loss: 0.0526 - mean_absolute_error: 0.17 - ETA: 14:38 - loss: 0.0519 - mean_absolute_error: 0.17 - ETA: 14:31 - loss: 0.0516 - mean_absolute_error: 0.17 - ETA: 14:24 - loss: 0.0510 - mean_absolute_error: 0.17 - ETA: 14:16 - loss: 0.0506 - mean_absolute_error: 0.17 - ETA: 14:09 - loss: 0.0504 - mean_absolute_error: 0.17 - ETA: 14:02 - loss: 0.0500 - mean_absolute_error: 0.17 - ETA: 13:55 - loss: 0.0494 - mean_absolute_error: 0.17 - ETA: 13:48 - loss: 0.0487 - mean_absolute_error: 0.16 - ETA: 13:41 - loss: 0.0482 - mean_absolute_error: 0.16 - ETA: 13:35 - loss: 0.0476 - mean_absolute_error: 0.16 - ETA: 13:29 - loss: 0.0474 - mean_absolute_error: 0.16 - ETA: 13:21 - loss: 0.0468 - mean_absolute_error: 0.16 - ETA: 13:14 - loss: 0.0466 - mean_absolute_error: 0.16 - ETA: 13:08 - loss: 0.0462 - mean_absolute_error: 0.16 - ETA: 13:01 - loss: 0.0462 - mean_absolute_error: 0.16 - ETA: 12:55 - loss: 0.0459 - mean_absolute_error: 0.16 - ETA: 12:48 - loss: 0.0454 - mean_absolute_error: 0.16 - ETA: 12:41 - loss: 0.0450 - mean_absolute_error: 0.16 - ETA: 12:36 - loss: 0.0446 - mean_absolute_error: 0.16 - ETA: 12:29 - loss: 0.0443 - mean_absolute_error: 0.16 - ETA: 12:22 - loss: 0.0443 - mean_absolute_error: 0.16 - ETA: 12:15 - loss: 0.0444 - mean_absolute_error: 0.16 - ETA: 12:09 - loss: 0.0444 - mean_absolute_error: 0.16 - ETA: 12:02 - loss: 0.0442 - mean_absolute_error: 0.16 - ETA: 11:55 - loss: 0.0441 - mean_absolute_error: 0.16 - ETA: 11:48 - loss: 0.0441 - mean_absolute_error: 0.16 - ETA: 11:41 - loss: 0.0437 - mean_absolute_error: 0.16 - ETA: 11:34 - loss: 0.0435 - mean_absolute_error: 0.15 - ETA: 11:27 - loss: 0.0432 - mean_absolute_error: 0.15 - ETA: 11:19 - loss: 0.0430 - mean_absolute_error: 0.15 - ETA: 11:12 - loss: 0.0428 - mean_absolute_error: 0.15 - ETA: 11:04 - loss: 0.0426 - mean_absolute_error: 0.15 - ETA: 10:57 - loss: 0.0424 - mean_absolute_error: 0.15 - ETA: 10:50 - loss: 0.0421 - mean_absolute_error: 0.15 - ETA: 10:43 - loss: 0.0420 - mean_absolute_error: 0.15 - ETA: 10:36 - loss: 0.0418 - mean_absolute_error: 0.15 - ETA: 10:29 - loss: 0.0415 - mean_absolute_error: 0.15 - ETA: 10:21 - loss: 0.0413 - mean_absolute_error: 0.15 - ETA: 10:14 - loss: 0.0411 - mean_absolute_error: 0.15 - ETA: 10:07 - loss: 0.0409 - mean_absolute_error: 0.15 - ETA: 10:00 - loss: 0.0407 - mean_absolute_error: 0.15 - ETA: 9:52 - loss: 0.0407 - mean_absolute_error: 0.1544 - ETA: 9:45 - loss: 0.0404 - mean_absolute_error: 0.153 - ETA: 9:38 - loss: 0.0403 - mean_absolute_error: 0.153 - ETA: 9:31 - loss: 0.0400 - mean_absolute_error: 0.152 - ETA: 9:24 - loss: 0.0399 - mean_absolute_error: 0.152 - ETA: 9:16 - loss: 0.0396 - mean_absolute_error: 0.152 - ETA: 9:09 - loss: 0.0394 - mean_absolute_error: 0.151 - ETA: 9:02 - loss: 0.0395 - mean_absolute_error: 0.151 - ETA: 8:55 - loss: 0.0396 - mean_absolute_error: 0.152 - ETA: 8:48 - loss: 0.0394 - mean_absolute_error: 0.151 - ETA: 8:41 - loss: 0.0393 - mean_absolute_error: 0.151 - ETA: 8:33 - loss: 0.0391 - mean_absolute_error: 0.150 - ETA: 8:26 - loss: 0.0389 - mean_absolute_error: 0.150 - ETA: 8:19 - loss: 0.0389 - mean_absolute_error: 0.150 - ETA: 8:12 - loss: 0.0387 - mean_absolute_error: 0.150 - ETA: 8:05 - loss: 0.0386 - mean_absolute_error: 0.149 - ETA: 7:58 - loss: 0.0383 - mean_absolute_error: 0.149 - ETA: 7:51 - loss: 0.0381 - mean_absolute_error: 0.148 - ETA: 7:44 - loss: 0.0379 - mean_absolute_error: 0.148 - ETA: 7:37 - loss: 0.0379 - mean_absolute_error: 0.148 - ETA: 7:30 - loss: 0.0379 - mean_absolute_error: 0.148 - ETA: 7:22 - loss: 0.0378 - mean_absolute_error: 0.148 - ETA: 7:15 - loss: 0.0376 - mean_absolute_error: 0.147 - ETA: 7:08 - loss: 0.0375 - mean_absolute_error: 0.147 - ETA: 7:01 - loss: 0.0374 - mean_absolute_error: 0.147 - ETA: 6:54 - loss: 0.0372 - mean_absolute_error: 0.146 - ETA: 6:47 - loss: 0.0371 - mean_absolute_error: 0.146 - ETA: 6:40 - loss: 0.0369 - mean_absolute_error: 0.146 - ETA: 6:33 - loss: 0.0367 - mean_absolute_error: 0.145 - ETA: 6:25 - loss: 0.0368 - mean_absolute_error: 0.145 - ETA: 6:18 - loss: 0.0366 - mean_absolute_error: 0.145 - ETA: 6:11 - loss: 0.0365 - mean_absolute_error: 0.145 - ETA: 6:04 - loss: 0.0365 - mean_absolute_error: 0.145 - ETA: 5:56 - loss: 0.0366 - mean_absolute_error: 0.145 - ETA: 5:49 - loss: 0.0365 - mean_absolute_error: 0.145 - ETA: 5:42 - loss: 0.0363 - mean_absolute_error: 0.144 - ETA: 5:35 - loss: 0.0362 - mean_absolute_error: 0.144 - ETA: 5:28 - loss: 0.0361 - mean_absolute_error: 0.144 - ETA: 5:20 - loss: 0.0360 - mean_absolute_error: 0.144 - ETA: 5:13 - loss: 0.0360 - mean_absolute_error: 0.144 - ETA: 5:06 - loss: 0.0359 - mean_absolute_error: 0.143 - ETA: 4:59 - loss: 0.0358 - mean_absolute_error: 0.143 - ETA: 4:52 - loss: 0.0356 - mean_absolute_error: 0.143 - ETA: 4:44 - loss: 0.0355 - mean_absolute_error: 0.143 - ETA: 4:37 - loss: 0.0354 - mean_absolute_error: 0.142 - ETA: 4:30 - loss: 0.0353 - mean_absolute_error: 0.142 - ETA: 4:22 - loss: 0.0351 - mean_absolute_error: 0.142 - ETA: 4:15 - loss: 0.0351 - mean_absolute_error: 0.142 - ETA: 4:08 - loss: 0.0351 - mean_absolute_error: 0.142 - ETA: 4:00 - loss: 0.0351 - mean_absolute_error: 0.142 - ETA: 3:53 - loss: 0.0351 - mean_absolute_error: 0.142 - ETA: 3:46 - loss: 0.0349 - mean_absolute_error: 0.141 - ETA: 3:38 - loss: 0.0349 - mean_absolute_error: 0.141 - ETA: 3:31 - loss: 0.0348 - mean_absolute_error: 0.141 - ETA: 3:23 - loss: 0.0347 - mean_absolute_error: 0.141 - ETA: 3:16 - loss: 0.0347 - mean_absolute_error: 0.141 - ETA: 3:09 - loss: 0.0346 - mean_absolute_error: 0.141 - ETA: 3:01 - loss: 0.0345 - mean_absolute_error: 0.141 - ETA: 2:54 - loss: 0.0344 - mean_absolute_error: 0.140 - ETA: 2:46 - loss: 0.0344 - mean_absolute_error: 0.140 - ETA: 2:39 - loss: 0.0343 - mean_absolute_error: 0.140 - ETA: 2:31 - loss: 0.0342 - mean_absolute_error: 0.140 - ETA: 2:24 - loss: 0.0341 - mean_absolute_error: 0.140 - ETA: 2:16 - loss: 0.0341 - mean_absolute_error: 0.140 - ETA: 2:09 - loss: 0.0341 - mean_absolute_error: 0.140 - ETA: 2:01 - loss: 0.0340 - mean_absolute_error: 0.140 - ETA: 1:54 - loss: 0.0339 - mean_absolute_error: 0.139 - ETA: 1:46 - loss: 0.0338 - mean_absolute_error: 0.139 - ETA: 1:39 - loss: 0.0338 - mean_absolute_error: 0.139 - ETA: 1:31 - loss: 0.0337 - mean_absolute_error: 0.139 - ETA: 1:24 - loss: 0.0337 - mean_absolute_error: 0.139 - ETA: 1:16 - loss: 0.0336 - mean_absolute_error: 0.139 - ETA: 1:08 - loss: 0.0335 - mean_absolute_error: 0.139 - ETA: 1:01 - loss: 0.0335 - mean_absolute_error: 0.139 - ETA: 53s - loss: 0.0335 - mean_absolute_error: 0.139 - ETA: 46s - loss: 0.0335 - mean_absolute_error: 0.1393"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9732/9732 [==============================] - ETA: 38s - loss: 0.0335 - mean_absolute_error: 0.13 - ETA: 31s - loss: 0.0334 - mean_absolute_error: 0.13 - ETA: 23s - loss: 0.0334 - mean_absolute_error: 0.13 - ETA: 15s - loss: 0.0333 - mean_absolute_error: 0.13 - ETA: 8s - loss: 0.0333 - mean_absolute_error: 0.1388 - ETA: 0s - loss: 0.0332 - mean_absolute_error: 0.138 - 1166s 120ms/step - loss: 0.0332 - mean_absolute_error: 0.1387\n",
      "Epoch 2/50\n",
      "1984/9732 [=====>........................] - ETA: 19:50 - loss: 0.0244 - mean_absolute_error: 0.13 - ETA: 20:06 - loss: 0.0233 - mean_absolute_error: 0.12 - ETA: 20:09 - loss: 0.0240 - mean_absolute_error: 0.12 - ETA: 20:04 - loss: 0.0259 - mean_absolute_error: 0.12 - ETA: 19:51 - loss: 0.0262 - mean_absolute_error: 0.12 - ETA: 19:47 - loss: 0.0261 - mean_absolute_error: 0.12 - ETA: 19:30 - loss: 0.0255 - mean_absolute_error: 0.12 - ETA: 19:19 - loss: 0.0254 - mean_absolute_error: 0.12 - ETA: 19:07 - loss: 0.0243 - mean_absolute_error: 0.11 - ETA: 18:57 - loss: 0.0236 - mean_absolute_error: 0.11 - ETA: 18:52 - loss: 0.0237 - mean_absolute_error: 0.11 - ETA: 18:47 - loss: 0.0231 - mean_absolute_error: 0.11 - ETA: 18:41 - loss: 0.0229 - mean_absolute_error: 0.11 - ETA: 18:35 - loss: 0.0232 - mean_absolute_error: 0.11 - ETA: 18:28 - loss: 0.0236 - mean_absolute_error: 0.11 - ETA: 18:22 - loss: 0.0237 - mean_absolute_error: 0.11 - ETA: 18:16 - loss: 0.0236 - mean_absolute_error: 0.11 - ETA: 18:10 - loss: 0.0235 - mean_absolute_error: 0.11 - ETA: 18:02 - loss: 0.0234 - mean_absolute_error: 0.11 - ETA: 17:55 - loss: 0.0237 - mean_absolute_error: 0.11 - ETA: 17:48 - loss: 0.0237 - mean_absolute_error: 0.11 - ETA: 17:42 - loss: 0.0234 - mean_absolute_error: 0.11 - ETA: 17:34 - loss: 0.0233 - mean_absolute_error: 0.11 - ETA: 17:27 - loss: 0.0235 - mean_absolute_error: 0.11 - ETA: 17:19 - loss: 0.0233 - mean_absolute_error: 0.11 - ETA: 17:13 - loss: 0.0233 - mean_absolute_error: 0.11 - ETA: 17:05 - loss: 0.0232 - mean_absolute_error: 0.11 - ETA: 16:57 - loss: 0.0230 - mean_absolute_error: 0.11 - ETA: 16:50 - loss: 0.0230 - mean_absolute_error: 0.11 - ETA: 16:43 - loss: 0.0232 - mean_absolute_error: 0.11 - ETA: 16:36 - loss: 0.0233 - mean_absolute_error: 0.1173"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "lstm_model = get_model(300)\n",
    "lstm_model.fit(train_set_essay_bert_ids,train_set_labels, batch_size=64, epochs=50)\n",
    "y_pred = lstm_model.predict(dev_set_essay_bert_ids)\n",
    "lstm_model.save('/tmp/project/rnnlm_trained')\n",
    "\n",
    "#Save Result\n",
    "result = cohen_kappa_score(dev_set_labels,y_pred,weights='quadratic')\n",
    "print(\"Kappa Score: {}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Kappa score after training: \",np.around(np.array(results).mean(),decimals=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict\n",
    "model = get_model()\n",
    "model.load_weights(\"/tmp/project/rnnlm_trained\")\n",
    "preds = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-headed Creativity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Embeddings\n",
    "\n",
    "For the word embeddings, we utilized the pre-trained BERT word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kashgari\n",
    "\n",
    "# Utilizing a package Kashgari to perform the pre-trained BERT embeddings\n",
    "from kashgari.embeddings import BERTEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining two instantiated embeddings with different sequence lengths, one for prompt and one for essay\n",
    "# As defined above max_len_bert = 512, max_len_p = 130\n",
    "W_bert_embed_prompt = BERTEmbedding('bert-base-uncased', sequence_length=max_len_p)\n",
    "W_bert_embed_essay = BERTEmbedding('bert-base-uncased', sequence_length=max_len_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 101, 1045, 2572, 2183, 2188, 2085,  102,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "       [ 101, 1996, 2051, 2038, 2979, 2005,  100,  102,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0]]), array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.]])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 130, 768)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TESTING... to be deleted in future\n",
    "testing = np.array([['i','am','going','home','now'],\n",
    "                    ['the','time','has','passed','for','girrievinjhkhjkjh']\n",
    "                   ])\n",
    "out = W_bert_embed_prompt.embed(testing)\n",
    "out.shape # should be (2,130,768) -- 2 examples, 130 seq length, 768 parameters for the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[  101,  1996,  6888, ...,     0,     0,     0],\n",
      "       [  101,  1045,  2228, ...,     0,     0,     0],\n",
      "       [  101,  1996,  3166, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 15657,  2012, ...,     0,     0,     0],\n",
      "       [  101,  1996,  4292, ...,     0,     0,     0],\n",
      "       [  101,  2004,  1045, ...,     0,     0,     0]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]])]\n",
      "[array([[ 2001,  1999,  1996, ...,  1996, 12558,   102],\n",
      "       [  101, 15657,  1999, ...,  3752,   102,     0],\n",
      "       [ 2114,  1996,  3944, ...,  2115,  4784,   102],\n",
      "       ...,\n",
      "       [  101, 15657,  1999, ...,  3752,   102,     0],\n",
      "       [ 1037,  3242,  2104, ...,  2115,  7091,   102],\n",
      "       [  101,  4339,  2055, ...,     0,     0,     0]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]])]\n",
      "[array([[  101,  1996,  3166, ...,     0,     0,     0],\n",
      "       [  101,  2043,  2017, ...,     0,     0,     0],\n",
      "       [  101,  1996, 16472, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101,  2129,  1996, ...,     0,     0,     0],\n",
      "       [  101,  1996,  6888, ...,     0,     0,     0],\n",
      "       [  101, 11752,  2003, ...,     0,     0,     0]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]])]\n",
      "[array([[ 2114,  1996,  3944, ...,  2115,  4784,   102],\n",
      "       [  101,  2062,  1998, ...,     0,     0,     0],\n",
      "       [ 2612,  1997,  3352, ...,  1996, 28142,   102],\n",
      "       ...,\n",
      "       [ 1037,  3242,  2104, ...,  2115,  7091,   102],\n",
      "       [ 2001,  1999,  1996, ...,  1996, 12558,   102],\n",
      "       [  101,  4339,  2055, ...,     0,     0,     0]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]])]\n",
      "[array([[  101,  2028,  2154, ...,     0,     0,     0],\n",
      "       [  101,  2006,  2026, ...,     0,     0,     0],\n",
      "       [  101,  7592,  2026, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101,  2053,  3043, ...,     0,     0,     0],\n",
      "       [  101,  1996, 16472, ...,     0,     0,     0],\n",
      "       [  101,  1999,  2026, ...,     0,     0,     0]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]])]\n",
      "[array([[  101,  4339,  2055, ...,     0,     0,     0],\n",
      "       [  101,  4339,  2055, ...,     0,     0,     0],\n",
      "       [  101, 15657,  1999, ...,  3752,   102,     0],\n",
      "       ...,\n",
      "       [ 2001,  1999,  1996, ...,  1996, 12558,   102],\n",
      "       [ 2612,  1997,  3352, ...,  1996, 28142,   102],\n",
      "       [  101, 15657,  1999, ...,  3752,   102,     0]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]])]\n",
      "7:17:56.300363\n"
     ]
    }
   ],
   "source": [
    "# Getting all the BERT embeddings for prompts and essays\n",
    "# Warning, this step takes a long time to execute: last run 7:17:56.300363\n",
    "start = timer()\n",
    "\n",
    "train_set_essay_bert_ids = W_bert_embed_essay.embed(train_set_essays_bert)\n",
    "train_set_prompt_bert_ids = W_bert_embed_prompt.embed(train_set_prompts_bert)\n",
    "\n",
    "dev_set_essay_bert_ids = W_bert_embed_essay.embed(dev_set_essays_bert)\n",
    "dev_set_prompt_bert_ids = W_bert_embed_prompt.embed(dev_set_prompts_bert)\n",
    "\n",
    "test_set_essay_bert_ids = W_bert_embed_essay.embed(test_set_essays_bert)\n",
    "test_set_prompt_bert_ids = W_bert_embed_prompt.embed(test_set_prompts_bert)\n",
    "\n",
    "end = timer()\n",
    "print(timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the embeddings to file\n",
    "np.save(\"data/embeddings/train_set_essay_bert_ids\",train_set_essay_bert_ids)\n",
    "np.save(\"data/embeddings/train_set_prompt_bert_ids\",train_set_prompt_bert_ids)\n",
    "np.save(\"data/embeddings/dev_set_essay_bert_ids\",dev_set_essay_bert_ids)\n",
    "np.save(\"data/embeddings/dev_set_prompt_bert_ids\",dev_set_prompt_bert_ids)\n",
    "np.save(\"data/embeddings/test_set_essay_bert_ids\",test_set_essay_bert_ids)\n",
    "np.save(\"data/embeddings/test_set_prompt_bert_ids\",test_set_prompt_bert_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9732, 512, 768)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking shape of the embedding word ids\n",
    "train_set_essay_bert_ids.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
