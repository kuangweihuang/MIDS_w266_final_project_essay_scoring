{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W266 Project - Essay Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "\n",
    "The data is obtained from the Automated Student Assessment Prize (ASAP) AES dataset (https://www.kaggle.com/c/asap-aes/data), which contains essays written by students ranging from Grade 7 to Grade 10. The dataset consists of 8 essay sets, each with a different topic or prompt, with a total of 12,978 essays with scores.\n",
    "\n",
    "Each of the sets of essays was generated from a single prompt. Selected essays range from an average length of 150 to 550 words per response. Some of the essays are dependent upon source information and others are not. All responses were written by students ranging in grade levels from Grade 7 to Grade 10. All essays were hand graded and were double-scored. Each of the eight data sets has its own unique characteristics. The variability is intended to test the limits of your scoring engine's capabilities.\n",
    "\n",
    "The training data is provided in three formats: a tab-separated value (TSV) file, a Microsoft Excel 2010 spreadsheet, and a Microsoft Excel 2003 spreadsheet.  The current release of the training data contains essay sets 1-6.  Sets 7-8 will be released on February 10, 2012.  Each of these files contains 28 columns:\n",
    "\n",
    "    essay_id: A unique identifier for each individual student essay\n",
    "    essay_set: 1-8, an id for each set of essays\n",
    "    essay: The ascii text of a student's response\n",
    "    rater1_domain1: Rater 1's domain 1 score; all essays have this\n",
    "    rater2_domain1: Rater 2's domain 1 score; all essays have this\n",
    "    rater3_domain1: Rater 3's domain 1 score; only some essays in set 8 have this.\n",
    "    domain1_score: Resolved score between the raters; all essays have this\n",
    "    rater1_domain2: Rater 1's domain 2 score; only essays in set 2 have this\n",
    "    rater2_domain2: Rater 2's domain 2 score; only essays in set 2 have this\n",
    "    domain2_score: Resolved score between the raters; only essays in set 2 have this\n",
    "    rater1_trait1 score - rater3_trait6 score: trait scores for sets 7-8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up ML libraries\n",
    "\n",
    "Importing the relevant NLP and tensorflow libraries for our use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kw.UNLOVEDPC\\Anaconda3\\envs\\py3-Env\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Pandas and SKLearn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Helper libraries\n",
    "#from w266_common import utils, vocabulary, tf_embed_viz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the data\n",
    "\n",
    "Data from AES dataset is stored in the `/data/` folder.  We will begin by loading the training dataset `training_set_rel3.tsv` and partitioning it into train, test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of rows in full data set: 12978\n"
     ]
    }
   ],
   "source": [
    "training_set_rel3_df = pd.read_csv(\"data/training_set_rel3.csv\")\n",
    "#training_set_rel3_df.head()\n",
    "print(\"No. of rows in full data set:\", len(training_set_rel3_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train, dev and test sets\n",
    "train_set, test_set = train_test_split(training_set_rel3_df, test_size=0.1, random_state=0)\n",
    "train_set, dev_set = train_test_split(train_set, test_size=15/90, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: 9733\n",
      "Dev Set: 1947\n",
      "Test Set: 1298\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Set:\", len(train_set))\n",
    "print(\"Dev Set:\", len(dev_set))\n",
    "print(\"Test Set:\", len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_essays = np.array(train_set[\"essay\"])\n",
    "train_set_labels = np.array(train_set[\"domain1_score\"])\n",
    "dev_set_essays = np.array(dev_set[\"essay\"])\n",
    "dev_set_labels = np.array(dev_set[\"domain1_score\"])\n",
    "test_set_essays = np.array(test_set[\"essay\"])\n",
    "test_set_labels = np.array(test_set[\"domain1_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the LSTM with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-headed Creativity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Embeddings\n",
    "\n",
    "For the word embeddings, we utilized the pre-trained BERT word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
